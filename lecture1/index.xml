<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lecture 1 on MLOps: Operationalizing Machine Learning</title>
    <link>https://chicagodatascience.github.io/MLOps/lecture1/</link>
    <description>Recent content in Lecture 1 on MLOps: Operationalizing Machine Learning</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    
	<atom:link href="https://chicagodatascience.github.io/MLOps/lecture1/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Basics</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/intro/</guid>
      <description>Python  We will be predominantly concerned with the Python ecosystem A big advanage is that local system development can be easily moved to cloud and or a scalable on-prem solution. Many companies use python to start data science projects in-house (via fresh recruits, interns etc) Python has some relatively easy ways to access databases Big data platforms such as Spark have great python bindings  E.g., Pandas dataframe and Spark dataframe  Latest models (deep learning, pre-trained) are built in the python ecosystem Many many useful libraries: pandas, matplotlib, flask,&amp;hellip;  Our Objective  Learn the patterns, not the specific tools  Deployment Targets  Local machines On-prem or self-hosted machines (needs DevOps skills) Managed cloud  Heroku (PAAS) Azure GCP AWS (IAAS)  The decision to deply on one versus the other depends on  skills business need internal vs external scale, reliability, security costs ease of deployment   Local Deployments are Hard  Need to learn linux security Need to learn how to manage access Need for learn backups Need to learn hot switching / reliability  Cloud Deployments are not Easy  Also need to learn a complex ecosystem Vendor lock-in (for successful businesses, this is not an issue)  Aside: Software Tools Python development can happen:</description>
    </item>
    
    <item>
      <title>Setting up Python</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/conda/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/conda/</guid>
      <description>Here are a few notes on installing a user specific python distribution:
Get Miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh conda install pip #better to use the pip in the base conda env than system pip   The difference between conda and pip: pip is a package manager specifically for python, whereas conda is a package manager for multiple languages as well as is an environment manager. Python module venv is python specific environment manager.</description>
    </item>
    
    <item>
      <title>Remote Jupyter Server</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/jupyter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/jupyter/</guid>
      <description>The following sets a simple password based login, which is handy:
jupyter notebook --generate-config jupyter notebook password  Unfortuantely, hashed password is sent unencrypted by your browser here. So read up here to do this in a better way.
Starting jupyter on the server can be done inside a screen session:
screen -S jupyter-session #can also use nohup or tmux here jupyter notebook --no-browser --port=8888  SSH tunnel can be setup by running the following on your local machine, and then opening the browser (http://localhost:8889)</description>
    </item>
    
    <item>
      <title>Serving ML Models Using Web Servers</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/deploy_webserver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/deploy_webserver/</guid>
      <description>Model Serving  Sharing results with others (humans, web services, applications) Batch approach: dump predictions to a database (quite popular) Real-time approach: send a test feature vector, get back the prediction instantly and the computation happens now  How to consume from prediction services?  Using web requests (e.g., using a JSON payload)  How to output predictions?  We will plan to set up a server to serve predictions  It will respond to web requests (GET, POST) We pass some inputs (image, text, vector of numbers), and get some outputs (just like a function) The environment from which we pass inputs may be very different from the environment where the prediction happens (e.</description>
    </item>
    
    <item>
      <title>MLFlow</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/mlflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/mlflow/</guid>
      <description>See https://pypi.org/project/mlflow/</description>
    </item>
    
  </channel>
</rss>