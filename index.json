[
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/",
	"title": "Course Logistics",
	"tags": [],
	"description": "",
	"content": " Course Logistics  Semester: Fall 2020 Lectures: Thursdays 6.30 PM to 9.00 PM Mode: Online synchronous (i.e., location is online). The course will be delivered over Zoom (an invite will be sent before the first day of class). See the online learning page for basic technology requirements. Staff  Instructor: Dr. Theja Tulabandhula (netid: theja) Teaching Assistant: Tengteng Ma (netid: tma24)  Communication: via slack, zoom and one note class notebook. Office hours: online via slack and zoom.  Textbook and Materials  Data Science in Production by Ben Weber (2020, $5 for the ebook/pdf). A sample of the first three chapters is available at the publishers page linked here.  Software  Any OS should be okay. If in doubt, run a virtual machine running linux (this will be discussed in the class). Some of the software we will work with are: Containers  Docker for Desktop kubectl and minikube  Python (Anaconda)  flask requests pandas pytorch scikit-learn matplotlib \u0026hellip;  Command Line Utilities (CLIs) from AWS, Google Cloud etc. \u0026hellip;  Hardware  There will be varied computing resources needed for this course. Try using a virtual machine with linux on your own computer if possible. A Windows virtual desktop is available at desktop.uic.edu if needed. You can refer to these two help pages to get started.  Assignments  There are no graded assignments or exams for this course. Students are expected to go over the lectures and practice the use of technologies discussed each week.  Project  Students are expected to apply what they learn in the course and demonstrate a deployment of an existing machine learning model they have access to. A suitable documentation of this process along with the scripts/codes/commands used is to be submitted on October 14th (with no exceptions). The evaluation criteria and other details will be released shortly. Submission deadline is BEFORE 11.59 PM on the concerned day. Late submissions will have an automatic 20% penalty per day. Use Blackboard for uploading your work as a single zip file.  Grade  Grades will be assigned based on the project (see project evaluation criteria above) (80%) and course participation (20%).  Miscellaneous Information  This is a 2 credit graduate level course offered by the Information and Decision Sciences department at UIC. See the academic calendar for the semester timeline. Students who wish to observe their religious holidays (http://oae.uic.edu/religious-calendar/) should notify the instructor within one week of the first lecture date. Contact the instructor at the earliest, if you require any accommodations for access to and/or participation in this course. Refer to the academic integrity guidelines set by the university.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/",
	"title": "Lecture 1",
	"tags": [],
	"description": "",
	"content": " Web Servers - SSH and Firewall - Conda Environments - Jupyter - Making requests and processing responses - Model Persistence using MLFlow - Serving a model using Flask "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/online_technology_requirements/",
	"title": "Online Learning Details",
	"tags": [],
	"description": "",
	"content": " Online Learning Details To maximize the learning experience, it will be good if students can meet the following basic technology requirements:\n At a minimum, students should have a device and an internet connection. A microphone, and a webcam would be highly recommended. See the Basic Technology Requirements link for more details.\n Laptop, Chromebook or Desktop Computer: Note that Chromebooks are used to perform a variety of browser-based tasks with most data and applications, such as Blackboard Learn, Blackboard Collaborate, Google Docs, and Office 365, residing in the cloud rather than on the machine itself. This can result in somewhat reduced functionality, depending on your needs. If you do not have reliable access to a computer at home, ACCC may have a laptop to lend to you. Please fill out our request form at accc.uic.edu/forms/laptop-request\n Internet: Many service providers are offering connectivity solutions for students without access to Wi-Fi or the internet. The Illinois Citizens Utility Board is maintaining a comprehensive list of the available options here: citizensutilityboard.org/blog/2020/03/19/cubs-guide-utility-services-during-the-covid-19-public-health-emergency.\n The State of Illinois is maintaining a map of publicly available internet hotspots across the state that can be used for academic-related needs. These hotspots are available from within a parked vehicle. The map, and additional information, can be viewed at www.ildceo.net/wifi.\n Additionally, the ACCC has a very limited supply of cellular hotspots available for those students who are unable to take advantage of the above offers. Please fill out our request form at accc.uic.edu/forms/laptop-request/.\n  Microphone: While this may be built into your computer, we recommend using an external device such as a USB microphone or headset.\n Webcam: A built-in camera may be installed on your laptop; if not, you can use an external USB camera for video conferencing.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/schedule/",
	"title": "Schedule",
	"tags": [],
	"description": "",
	"content": " Textbook  Data Science in Production by Ben Weber (2020, $5 for the ebook/pdf). A sample of the first three chapters is available at the publishers page linked here.  Lecture Schedule Lecture 1: Serving ML Models Using Web Servers Reference: Chapter 2  Learning Goals:  Be able to set up a Python environment Be able to set up a jupyter session with SSH tunneling Be able to secure a web server Be able to use Flask to serve a ML model   Lecture 2: Serving ML Models Using Serverless Infrastructure Reference: Chapter 3  Learning Goals:  Be able to differentiate hosted vs managed solutions Assess deops effort for web server vs serverless deployments Be able to deploy a ML model using Google Cloud Functions and AWS Lambda Functions   Lecture 3: Serving ML Models Using Docker Reference: Chapter 4, upto 4.2  Learning Goals:  Be able to reason the pros and cons of container technologies Be able to differentiate containers from virtual machines Be able to create a new Docker image using Dockerfile Be able to upload the image to a remote registry   Lecture 4: Kubernetes for Orchestrating ML Deployments Reference: Chapter 4, 4.3 onwards  Learning Goals:  Understand the uses of Kubernetes Be able to set up a single node Kubernetes cluster using kubectl and minicube Be able to serve a prediction model on a container in the Kuebernetes cluster Be able to deploy a prediction model on Google Kubernetes Engine (GKE)   Lecture 5: ML Model Pipelines Reference: Chapter 5  Learning Goals:  TBD   Lecture 6: Reference: Chapter 6  Learning Goals:  TBD   Lecture 7: Reference: Chapter 7  Learning Goals:  TBD   Lecture 8: Online Experimentation Reference: 1: https://help.optimizely.com/Get_Started/Get_started_with_Optimizely_Web_Recommendations Reference: 2: https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html  Learning Goals:  TBD   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/project_instructions/",
	"title": "Project",
	"tags": [],
	"description": "",
	"content": " Aim  The aim of the project is to simulate the real-world process of deploying machine learning models. More specifically, the project component of this course allows you to explore a technology that assists in model deployment, either directly or indirectly, and asks you to report your experience working with that technology (or multiple technologies) to achieve some overall deployment goal.  Group  You should form groups of 4 students for this project component (this is a strict requirement). Reach out to your classmates early. Because this is a group project, a commensurate effort is expected, and each members contributions needs to be reported in the final submission.  Project Outcomes  There is one due-date for the project deliverables. See the course logistics page for the exact date. The deliverables are as follows. Project Report: In at most 8 pages (12 point, single column; you can have an appendix for supplementary material that may or may not be checked), you should explain your contributions in the project. Code and data: Code associated with the project (e.g., Jupyter notebooks), a small sample of the data/inputs/outputs if needed, and all steps necessary to replicate your project should be provided along with/in the report. A link to your github/gitlab/bitbucket/other repository is acceptable here (provide it at the front page of the report). A video presentation: You should provide a 10 minute video walk-through (discussing highlights) of your project and provide the link (say from Youtube where the video can be in unlisted mode) on the front page of the report.  Each team should upload the report (and code and video link) to Blackboard before the deadline.\nExample Report Components  For example, here are some aspects to focus on in your project report:  what was the goal what were the possible solutions what were the specific pros and cons from a business point of view a cost benefit analysis actual handling of the technology and demonstration in a dev environment documenting the experience lessons learned code artifacts and/or Jupyter notebooks \u0026hellip;  Here is an example project idea: try out a technology (or a specific aspect of it) and its competitors by following their documentation in a very extensive and well thought out manner (e.g., MLFlow vs bentoml vs cortex).  Grading Rubric  Projects will be graded based on the creativity shown in handling the technology and the insights drawn. The reports should be very clearly written and presented, and will be evaluated based on the correctness, content, creativity and clarity:  Correctness will be assessed based on the correct application of a technology, valid software setup and discussion of choices, technical correctness and the assumptions laid out. Content will be assessed based on the contributions made in the project (given group size) and project depth (e.g., why this aspect of ML deployment, why this problem, what did you do, visualization and interesting conclusions, insights, discussion of methodology followed). You should try to demonstrate your understanding of the relevant topics and their use in your non-trivial project. Creativity will be assessed based on how no-obvious your solution or contribution is and how different choices were thoughtfully made in the execution of the project. Clarity will be assessed based on the language quality, layout and structure of the report, the adequacy of the references cited, the capability of the team in explaining ideas in a clear and professional manner, and the clarity demonstrated in your discussions etc.  All external material/sources (code/idea/theory/insights) used should be cited prominently without failure. Use of pre-trained models, databases, web servers, front-end frameworks, visualization tools etc for your project is allowed and encouraged. This project cannot be used as part of any other course or requirement.  Additional Pointers  Keep track of costs especially if you are using services that require having a payment mode on file. Also, try to use free resources as much as possible. Do not train deep networks from scratch if it can be avoided. The project should not be centered around model accuracy. It is importantly to make a project plan that allocates sufficient tasks for each team member. It will be great if you can submit the project plan (a Gantt chart for example).  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/intro/",
	"title": "Basics",
	"tags": [],
	"description": "",
	"content": " Python  We will be predominantly concerned with the Python ecosystem A big advanage is that local system development can be easily moved to cloud and or a scalable on-prem solution. Many companies use python to start data science projects in-house (via fresh recruits, interns etc) Python has some relatively easy ways to access databases Big data platforms such as Spark have great python bindings  E.g., Pandas dataframe and Spark dataframe  Latest models (deep learning, pre-trained) are built in the python ecosystem Many many useful libraries: pandas, matplotlib, flask,\u0026hellip;  Our Objective  Learn the patterns, not the specific tools  Deployment Targets  Local machines On-prem or self-hosted machines (needs DevOps skills) Managed cloud  Heroku (PAAS) Azure GCP AWS (IAAS)  The decision to deply on one versus the other depends on  skills business need internal vs external scale, reliability, security costs ease of deployment   Local Deployments are Hard  Need to learn linux security Need to learn how to manage access Need for learn backups Need to learn hot switching / reliability  Cloud Deployments are not Easy  Also need to learn a complex ecosystem Vendor lock-in (for successful businesses, this is not an issue)  Aside: Software Tools Python development can happen:\n In text editors (e.g., sublime-text) In IDEs (e.g., Pycharm or VSCode) In Jupyter notebooks and variants (Google Colab, Databricks notebooks)  vanilla notebook does not allow collaboration as such   Part 1: Setting up Jupyter access on a VPS  We will use Vultr, but all steps are vendor agnostic. Alternatives include: Digitalocean, AWS EC2, Google Cloud; using Google Colab and other vendors. SSH passwordless access is set up. Next, we set up a basic firewall for security. This is followed by installing conda. (Optional) To run the jupyter server uninterrupted, we will run it within a screen session. We will access the server and notebooks on our local browser using SSH tunneling.  Part 2: Preparing an ML Model  We will show how data is accessed, and how the model is trained (this should be familiar to you).\n In particular, we will look at the moive recommendation problem.  There are aspects of saving and loading models that become important in production. For instance, we would like the models to be able to live across dev/staging/prod environments. For this, we think of the notion of model persistence\n Natively:\n For example, pytorch has native save and load methods. Same is the case for scikit-learn and a variety of other packages.  Using MLFlow:\n MLFlow addresses the problem of moving models across different environments without issues of incompatibility (minor version numbers, OS etc) among other things. See these links for more information: https://pypi.org/project/mlflow/ and mlflow.org    "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/ssh_and_firewall/",
	"title": "SSH and Firewall",
	"tags": [],
	"description": "",
	"content": " It is important to secure your connection to the machine. In order to do so, we will configure the ssh access pattern as well as set up a firewall that blocks all incoming requests except ssh port and web server ports.\nWe will assume that we have a non-root account that is in the sudoers group.\nSSH  When you first create the server instance, you may or may not have the ssh server running. If it is not running, you can install it first. On Ubuntu/Debian, you can use the following command:\nsudo apt install openssh-server  Next, we create a ssh key pair on our local machine with which we will access the server. From your local user home directory:\nmkdir .ssh ssh-keygen cd .ssh less id_rsa.pub  Copy this content to the following file authorized_keys in the webserver:\nmkdir .ssh vim authorized_keys #if vim is not present, you can use other editors or install it using `sudo apt install vim` #copy the content and quit (shift+colon\u0026gt; wq -\u0026gt; enter) chmod 600 authorized_keys  We need to edit the following fields in the file /etc/ssh/sshd_config on the server (say using vim):\n Port choose something other than 22 (opttional) PermitRootLogin no (changed from prohibit-password) PubkeyAuthentication yes (already defaults to this) PasswordAuthentication no (disable it for security)  Restart the ssh server. In Ubuntu/Debian this is achieved by sudo systemctl restart ssh\n  Firewall  A basic firewall such as ufw can help provide a layer of security. Install and run it using the following commands (Ubuntu/Debian):\nsudo apt install ufw sudo ufw allow [PortNumber] #here it is 22 or another port that you chose for ssh sudo ufw enable sudo ufw status verbose #this should show what the firewall is doing   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/conda/",
	"title": "Setting up Python",
	"tags": [],
	"description": "",
	"content": " Here are a few notes on installing a user specific python distribution:\nGet Miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh conda install pip #better to use the pip in the base conda env than system pip   The difference between conda and pip: pip is a package manager specifically for python, whereas conda is a package manager for multiple languages as well as is an environment manager. Python module venv is python specific environment manager.  Set up a conda environment and activate it conda create --name datasci-env python #or conda create -n dataeng-env python jupyter pandas numpy matplotlib #or conda create -n datasci-env scipy=0.15.0 #or conda env create -f environment.yml conda activate datasci-env   You don\u0026rsquo;t have to give names, can give prefixes where the env is saved, can create based on specific pages, can use explicit previous conda environments, yaml files, clone/update an existing one, etc. Use this link to get more information.\n Specifying a path to a subdirectory of your project directory when creating an environment can keep everything 100% self contained.\n To deactivate this environment, use conda deactivate datasci-env.\n  Install jupyter and pytorch (and tensorflow, keras, scikit-learn similarly) in a specific environment conda install jupyter conda install pytorch torchvision cpuonly -c pytorch # https://pytorch.org/   Change the command for pytorch installation if you do intend to use GPUs. In particular, install CUDA from conda after installing the latest NVidia drivers on the instance.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/jupyter/",
	"title": "Remote Jupyter Server",
	"tags": [],
	"description": "",
	"content": "The following sets a simple password based login, which is handy:\njupyter notebook --generate-config jupyter notebook password  Unfortuantely, hashed password is sent unencrypted by your browser here. So read up here to do this in a better way.\nStarting jupyter on the server can be done inside a screen session:\nscreen -S jupyter-session #can also use nohup or tmux here jupyter notebook --no-browser --port=8888  SSH tunnel can be setup by running the following on your local machine, and then opening the browser (http://localhost:8889)\nssh -N -f -L localhost:8889:localhost:8888 -p 22 theja@192.168.0.105  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/",
	"title": "Recommendation Models",
	"tags": [],
	"description": "",
	"content": "We will look at two models for recommending movies to existing users.\n Matrix factorization based on the surprise package. Matrix factorization based on Pytorch.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/surprise_training/",
	"title": "Recommendation (SVD) Training",
	"tags": [],
	"description": "",
	"content": "# https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.accuracy import rmse from surprise.dump import dump  # Load the movielens-100k dataset (download it if needed). data = Dataset.load_builtin('ml-100k') trainset = data.build_full_trainset() # Use an example algorithm: SVD. algo = SVD() algo.fit(trainset) # predict ratings for all pairs (u, i) that are in the training set. testset = trainset.build_testset() predictions = algo.test(testset) rmse(predictions) #actual predictions as thse items have not been seen by the users. there is no ground truth. # We predict ratings for all pairs (u, i) that are NOT in the training set. testset = trainset.build_anti_testset() predictions = algo.test(testset)  RMSE: 0.6774  dump('./surprise_model', predictions, algo)  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/deploy_webserver/",
	"title": "Serving ML Models Using Web Servers",
	"tags": [],
	"description": "",
	"content": " Model Serving  Sharing results with others (humans, web services, applications) Batch approach: dump predictions to a database (quite popular) Real-time approach: send a test feature vector, get back the prediction instantly and the computation happens now  How to consume from prediction services?  Using web requests (e.g., using a JSON payload)  How to output predictions?  We will plan to set up a server to serve predictions  It will respond to web requests (GET, POST) We pass some inputs (image, text, vector of numbers), and get some outputs (just like a function) The environment from which we pass inputs may be very different from the environment where the prediction happens (e.g., different hardware)   Our Objective  Use sklearn/keras with flask, gunicorn and heroku to set up a prediction server  Part 1: Making API Calls  Using the requests module from a jupyter notebook (this is an example of a programmatic way) Alternatively, using curl or postman (these are more versatile)  Part 2: Simple Flask App  Function decorators are used in Flask to achive routes to functions mapping. Integrating the model with the app is relatively easy if the model can be read from disk.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/surprise_inference/",
	"title": "Recommendation (SVD) Inference",
	"tags": [],
	"description": "",
	"content": "# https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.dump import load from collections import defaultdict import pandas as pd def get_top_n(predictions, n=10): \u0026quot;\u0026quot;\u0026quot;Return the top-N recommendation for each user from a set of predictions. Args: predictions(list of Prediction objects): The list of predictions, as returned by the test method of an algorithm. n(int): The number of recommendation to output for each user. Default is 10. Returns: A dict where keys are user (raw) ids and values are lists of tuples: [(raw item id, rating estimation), ...] of size n. \u0026quot;\u0026quot;\u0026quot; # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n  df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('./surprise_model')  Output:\n 0 1 2 0 1 Toy Story (1995) Animation|Children's|Comedy 1 2 Jumanji (1995) Adventure|Children's|Fantasy 2 3 Grumpier Old Men (1995) Comedy|Romance 3 4 Waiting to Exhale (1995) Comedy|Drama 4 5 Father of the Bride Part II (1995) Comedy algo: SVD  top_n = get_top_n(predictions, n=5) # Print the recommended items for each user limit = 0 for uid, user_ratings in top_n.items(): print('\\nUser:',uid) seen = [df.loc[int(iid),'name'] for (iid, _) in algo.trainset.ur[int(uid)]] if len(seen) \u0026gt; 10: seen = seen[:10] print('\\tSeen:',seen) print('\\tRecommendations:',[df.loc[int(iid),'name'] for (iid, _) in user_ratings]) limit+=1 if limit\u0026gt;3: break  Output:\nUser: 196 Seen: ['Richie Rich (1994)', 'Getaway, The (1994)', 'Batman Forever (1995)', 'Feast of July (1995)', 'Heidi Fleiss: Hollywood Madam (1995)', 'Shadows (Cienie) (1988)', 'Terminator 2: Judgment Day (1991)', \u0026quot;Nobody's Fool (1994)\u0026quot;, \u0026quot;Breakfast at Tiffany's (1961)\u0026quot;, 'Basic Instinct (1992)'] Recommendations: ['Age of Innocence, The (1993)', 'Bio-Dome (1996)', 'Strawberry and Chocolate (Fresa y chocolate) (1993)', 'Guardian Angel (1994)', \u0026quot;Carlito's Way (1993)\u0026quot;] User: 186 Seen: ['Double Happiness (1994)', 'Mr. Jones (1993)', 'War Room, The (1993)', 'Bloodsport 2 (1995)', 'Usual Suspects, The (1995)', 'Big Green, The (1995)', 'Mighty Morphin Power Rangers: The Movie (1995)', 'Boys on the Side (1995)', 'Cold Fever (� k�ldum klaka) (1994)', 'Sum of Us, The (1994)'] Recommendations: ['Lightning Jack (1994)', 'Robocop 3 (1993)', 'Walk in the Clouds, A (1995)', 'Living in Oblivion (1995)', 'Strawberry and Chocolate (Fresa y chocolate) (1993)'] User: 22 Seen: ['Assassins (1995)', 'Nico Icon (1995)', 'From the Journals of Jean Seberg (1995)', 'Last Summer in the Hamptons (1995)', 'Down Periscope (1996)', 'Bushwhacked (1995)', 'Beyond Bedlam (1993)', 'Client, The (1994)', 'Hoop Dreams (1994)', 'Ladybird Ladybird (1994)'] Recommendations: ['Home for the Holidays (1995)', 'Age of Innocence, The (1993)', 'Balto (1995)', 'City Hall (1996)', 'Ready to Wear (Pret-A-Porter) (1994)'] User: 244 Seen: ['When Night Is Falling (1995)', 'Birdcage, The (1996)', '8 Seconds (1994)', 'Foreign Student (1994)', 'Mighty Aphrodite (1995)', 'Before Sunrise (1995)', 'Lion King, The (1994)', 'Clockers (1995)', 'Underneath, The (1995)', 'Manny \u0026amp; Lo (1996)'] Recommendations: ['Century (1993)', 'Balto (1995)', 'Age of Innocence, The (1993)', 'Remains of the Day, The (1993)', 'Jimmy Hollywood (1994)']  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/flask/",
	"title": "Flask App",
	"tags": [],
	"description": "",
	"content": " Flask is a micro web framework written in Python. We first show how a simple service works, and then show how to load a model (e.g., based on pytorch) and serve it as well.\nWeather Reporting Service The key thing to see here are that the HTTP route / is mapped directly to a function weather. For instance, when someone hits localhost:5000 (5000 is the default unless specified in app.run() below) the function weather starts execution based on any received inputs.\n# load Flask import flask from flask import jsonify from geopy.geocoders import Nominatim import requests app = flask.Flask(__name__) # define a predict function as an endpoint @app.route(\u0026quot;/\u0026quot;, methods=[\u0026quot;GET\u0026quot;,\u0026quot;POST\u0026quot;]) def weather(): data = {\u0026quot;success\u0026quot;: False} #https://pypi.org/project/geopy/ geolocator = Nominatim(user_agent=\u0026quot;cloud_function_weather_app\u0026quot;) params = flask.request.json if params is None: params = flask.request.args # params = request.get_json() if \u0026quot;msg\u0026quot; in params: location = geolocator.geocode(str(params['msg'])) # https://www.weather.gov/documentation/services-web-api result1 = requests.get(f\u0026quot;https://api.weather.gov/points/{location.latitude},{location.longitude}\u0026quot;) result2 = requests.get(f\u0026quot;{result1.json()['properties']['forecast']}\u0026quot;) data[\u0026quot;response\u0026quot;] = result2.json() data[\u0026quot;success\u0026quot;] = True return jsonify(data) # start the flask app, allow remote connections if __name__ == '__main__': app.run(host='0.0.0.0')  This service can be run by using the command python weather.py (assuming that is the filename for the above script) locally. If the port 5000 is open, then this server will be accessible to the world if the server has an external IP address.\nModel Serving We can modify the above to serve the recommendation models we built earlier as follows:\nfrom surprise import Dataset from surprise.dump import load from collections import defaultdict import pandas as pd import flask def get_top_n(predictions, n=10): # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('./surprise_model') top_n = get_top_n(predictions, n=5) app = flask.Flask(__name__) # define a predict function as an endpoint @app.route(\u0026quot;/\u0026quot;, methods=[\u0026quot;GET\u0026quot;]) def predict(): data = {\u0026quot;success\u0026quot;: False} # check for passed in parameters params = flask.request.json if params is None: params = flask.request.args if \u0026quot;uid\u0026quot; in params.keys(): data[\u0026quot;response\u0026quot;] = str([df.loc[int(iid),'name'] for (iid, _) in top_n[params.get(\u0026quot;uid\u0026quot;)]]) data[\u0026quot;success\u0026quot;] = True # return a response in json format return flask.jsonify(data) # start the flask app, allow remote connections app.run(host='0.0.0.0')  You can use the following request in the browser http://0.0.0.0:5000/?uid=196 or use the requests module.\n"
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/",
	"title": "Lecture 2",
	"tags": [],
	"description": "",
	"content": " Serverless Deployments - Managed Solutions - Cloud Functions (GCP) - Lambda Functions (AWS) "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/serverless/",
	"title": "Serverless Deployments",
	"tags": [],
	"description": "",
	"content": " A. TLDR  Models do not need to be complex, but it can be complex to deploy models. - Ben Weber (2020)\n Problem  We have to take care of provisioning and server maintenance while deploying our models. We have to worry about scale: would 1 server be enough? How to minimize the time to deploy (at an acceptable increase in cost)? How can a single developer or data science/analytics professional manage a complex service?  Solution  Software that abstracts away server details and lets you serve your model (any function actually) with few lines of code/UI.\n The software automates\n prrovising servers scaling machines up and down  load balancing\n code versioning\n Our task is then to specify the requirements (pandas, pytorch).\n   B. Our Objective  Write serverless functions that generate predictions when they get triggered by HTTP requests. We will work with:  AWS GCP  We will deploy  a keras model, and a sklearn model.   C. Managed Services  Cloud is responsible for abstracting away various computing components: compute, storage, networking etc Minimizes thinking about dev/staging vs production. Note: what we did last class, ssh\u0026rsquo;ing into a virtual private server (VPS) would be considered as a hosted deployment, which is the opposite of managed deployment For example, serverless technology was introduced in 2015\u0026frasl;2016 by AWS (Amazon web) and GCP (Google cloud). It contrasts with VPS based deployment. Similarly, AWS ECS (Elastic Container Service) managed solution contrasts with the hosted/manual Docker on VPS setup.  When is a managed solution a bad idea?  No need for quick iteration (company cares about processes and protocols) Need a high speed service No need to scale system arbirarily Cost conscious or have an in-house developer.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions/",
	"title": "Cloud Functions",
	"tags": [],
	"description": "",
	"content": " Intro  Cloud Functions (CFs) are a solution from GCP for serverless deployments. Very little boilerplate beyond what we will write for simple offline model inference. In any such deployment, we need to be concerned about:  where the model is stored (recall pickle and mlflow), and what python packages are available.   Empty Deployment  We will set up triggers that will trigger our serving function (in particular, a HTTP request). We will specify the requirements needed for our python function to work The function we deploy here, similar to lecture 1, produces weather forecasts given a location.  Setting up using UI  Sign up with GCP if you haven\u0026rsquo;t already (typically you get a 300$ credit)\n Get to the console and find the Cloud Function page.\n   Go through the UI for creating a function.   We will choose the HTTP trigger and unauthenticated access option.   We may have to enable Cloud Build API   Finally, we choose the Python environment. You can see two default example files (main.py and requirements.txt). We will be modifying these two.  Python Files and Requirements  We will specify the following requirements:\nflask geopy requests  Our main file is the following:\ndef weather(request): from flask import jsonify from geopy.geocoders import Nominatim import requests data = {\u0026quot;success\u0026quot;: False} #https://pypi.org/project/geopy/ geolocator = Nominatim(user_agent=\u0026quot;cloud_function_weather_app\u0026quot;) params = request.get_json() if \u0026quot;msg\u0026quot; in params: location = geolocator.geocode(str(params['msg'])) # https://www.weather.gov/documentation/services-web-api # Example query: https://api.weather.gov/points/39.7456,-97.0892 result1 = requests.get(f\u0026quot;https://api.weather.gov/points/{location.latitude},{location.longitude}\u0026quot;) # Example query: https://api.weather.gov/gridpoints/TOP/31,80 result2 = requests.get(f\u0026quot;{result1.json()['properties']['forecast']}\u0026quot;) data[\u0026quot;response\u0026quot;] = result2.json() data[\u0026quot;success\u0026quot;] = True return jsonify(data)  Once the function is deployed, we can test the function (click actions on the far right in the dashboard)\n   We can pass the JSON string {\u0026quot;msg\u0026quot;:\u0026quot;Chicago\u0026quot;} and see that we indeed get the JSON output for the weather of Chicago.   We can also access the function from the web endpoint https://us-central1-authentic-realm-276822.cloudfunctions.net/function-1 (you will have a different endpoint). Note that unlike previous times, the request to this endpoint is a JSON payload.\n Below is the screen-shot of querying the weather of Chicago using the Postman tool. The way to use it is as follows:\n Insert the URL of the API Se the method type to POST Navigate to body and choose raw and then choose JSON from the dropdown menu. Now add the relevant parameters as a JSON string.    Finally, here is a query you can use from a Jupyter notebook.\nimport requests result = requests.post( \u0026quot;https://us-central1-authentic-realm-276822.cloudfunctions.net/function-1\u0026quot; ,json = { 'msg': 'Chicago' }) print(result.json()) #should match with https://forecast.weather.gov/MapClick.php?textField1=41.98\u0026amp;textField2=-87.9   Saving Model on the Cloud  For our original task of deploying a trained ML model, we need a way to read it from somewhere when the function is triggered.\n One way is to dump the model onto Google Cloud Storage (GCS)\n GCS is similar to the S3 (simple storage service) by AWS.\n We will use the command line to dump our model onto the cloud.\n  GCP access via the commandline  First we need to install the Google Cloud SDK from https://cloud.google.com/sdk/docs/downloads-interactive\ncurl https://sdk.cloud.google.com | bash gcloud init  There are two types of accounts you can work with: a user account or a service account (see https://cloud.google.com/sdk/docs/authorizing?authuser=2).\n Among others, [this page] gives a brief idea of why such an account is needed. In particular, we will create a service account (so that it can be used by an application programmatically anywhere) and store the encrypted credentials on disk for programmatic access through python. To do so, we run the following commands:\n We create the service account and check that it is active by using this command: gcloud iam service-accounts list.\ngcloud iam service-accounts create idsservice \\ --description=\u0026quot;IDS service account\u0026quot; \\ --display-name=\u0026quot;idsservice-displayed\u0026quot;  We then assign a new role for this service account in the project. The account can be disabled using the command gcloud iam service-accounts disable idsservice@authentic-realm-276822.iam.gserviceaccount.com (change idsservice and authentic-realm-276822 to your specific names).\ngcloud projects add-iam-policy-binding authentic-realm-276822 --member=serviceAccount:idsservice@authentic-realm-276822.iam.gserviceaccount.com --role=roles/owner  Finally, we can download the credentials\ngcloud iam service-accounts keys create ~/idsservice.json \\ --iam-account idsservice@authentic-realm-276822.iam.gserviceaccount.com  Once the credentials are downloaded, they can be programmatically accessed using python running on that machine. We just have to explore the location of the file:\nexport GOOGLE_APPLICATION_CREDENTIALS=/Users/theja/idsservice.json   Next we will install a python module to access GCS, so that we can write our model to the cloud:\npip install google-cloud-storage  The following code creates a bucket called theja_model_store\nfrom google.cloud import storage bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() storage_client.create_bucket(bucket_name) for bucket in storage_client.list_buckets(): print(bucket.name)  We can dump the model we used previous here using the following snippet\nfrom google.cloud import storage bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1a\u0026quot;) blob.upload_from_filename(\u0026quot;surprise_model\u0026quot;) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1b\u0026quot;) blob.upload_from_filename(\u0026quot;movies.dat\u0026quot;)   After running the above, the surprise package based recommendation model and the helper data file will be available at gs://theja_model_store/serverless/surprise_model/v1a and gs://theja_model_store/serverless/surprise_model/v1b as seen below.\nWe can either use the URIs above or use a programmatic way with the storage class. For example, here is the way to download the file v1b:\nfrom google.cloud import storage bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1b\u0026quot;) blob.download_to_filename(\u0026quot;movies.dat.from_gcp\u0026quot;)  We can diff it in Jupyter notebook itself using the expression !diff movies.dat movies.dat.from_gcp.\nWe will use this programmatic way of reading external data/model in the cloud function next.\n"
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions_model/",
	"title": "GCP Serverless Model Serving",
	"tags": [],
	"description": "",
	"content": " We modify the flask app that we had before, by again specifying the requirements.txt and the main python file appropriately. We will also increase the memory to 2GB and the timeout to 180 seconds. You will see that the following deployment has a lot of inefficiencies (can you spot the redundacy in loading the model and the predictions below?).\nThe requirements file will have the following entries:\nnumpy flask pandas google-cloud-storage scikit-surprise pickle5  The main file is also modified accordingly. Note that if we reload the model and the metadata on every request, it will be extremely inefficient. To fix that we can use global variables. This is not a good choice in much of python programming, but quite useful here. Essentially, global variables allow us to cache some of the objects, for faster response times.\ntop_n = None def recommend(request): global top_n from surprise import Dataset import pandas as pd import flask from google.cloud import storage import pickle5 def load(file_name): dump_obj = pickle5.load(open(file_name, 'rb')) return dump_obj['predictions'], dump_obj['algo'] def get_top_n(predictions, n=10): def defaultdict(default_type): class DefaultDict(dict): def __getitem__(self, key): if key not in self: dict.__setitem__(self, key, default_type()) return dict.__getitem__(self, key) return DefaultDict() # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n data = {\u0026quot;success\u0026quot;: False} params = request.get_json() if \u0026quot;uid\u0026quot; in params: if not top_n: bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1a\u0026quot;) blob.download_to_filename(\u0026quot;/tmp/surprise_model\u0026quot;) #ideally we should be reading things into memory blob = bucket.blob(\u0026quot;serverless/surprise_model/v1b\u0026quot;) blob.download_to_filename(\u0026quot;/tmp/movies.dat\u0026quot;) df = pd.read_csv('/tmp/movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('/tmp/surprise_model') top_n = get_top_n(predictions, n=5) data[\u0026quot;response\u0026quot;] = str([df.loc[int(iid),'name'] for (iid, _) in top_n[params.get(\u0026quot;uid\u0026quot;)]]) data[\u0026quot;success\u0026quot;] = True # return a response in json format return flask.jsonify(data)  We can test the function on the GCP console with a request JSON {\u0026quot;uid\u0026quot;:\u0026quot;206\u0026quot;}.\nAs an exercise, think of ways to make the whole setup above lightweight in terms of model and data size.\nDeploying the pytorch model after removing the dependecy on surprise is also a good challenge to tackle.\nAccess Management  It is a bad idea to allow unauthenticated access There are ways of restricting who can reach this model serving endpoint.  One way is to disable unauthenticated access while creating the function. Once we do that, we can create a permission structure based on GCP best practices.  We will omit the details here.  Updating Models and Monitoring  We can easily update our endpoint by rewriting the files we uploaded to GCS. A cleaner way is to create another cloud function. The function itself can have logic to reload files using the google-cloud-storage module based on time elapsed (say using datetime). Monitoring is very important, not just to know if our model performance has changed over time, but also to measure things such as latency, bugs and API access patterns. We will revisit this topic in the future.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions/",
	"title": "Lambda Functions",
	"tags": [],
	"description": "",
	"content": "  Lambda by Amazon Web Services (AWS) is an analogous serverless solution. Lambda can be used internall as well as for model deployments (we are focusing on the latter). We will repeat setting up the weather app and the recommender model, using the CLI (command line interface tools)  Aside: Setting up an IAM user  TBD  Hello World in Lambda  Select the lambda service.   Pick the python 3.7 runtime.   You will see the green bar indicating a successful creation.   We will initially not changed the code.   And create a dummy test.   The result looks like this.  Setting up an API Gateway  We will slightly change the function so that it forms a proper HTTP response.   Under the Designer section, we will \u0026lsquo;Add Trigger\u0026rsquo; and select \u0026lsquo;API Gateway\u0026rsquo;.   We choose HTTP API and security to be open for the time being.   A block will appear on the designer tab in the Lambda function page.   Once you click on the API Gateway link provided, you will see the response as expected:  See this documentation for more details.\nSet up a Programmatic User to Access S3  What is IAM? IAM (Identity and Access Management) is a very useful tool for an organization to administer and control access to various resources.\n We will create a programmatic user, similar to the service account in GCP.  What is S3? S3 (Simple Storage Service) is a very popular service used by many large and small companies and individuals.\n For example, a typical data science work flow may involve ETL work on databases (such as Amazon Redshift) and then storing outputs on S3.  Start with finding the IAM service.\n   Click on create a new user.   We will make this user have programmatic access (for example via Python later on).   For now, we let the user have full read and write access to S3.   Lets review the user setup.   In this step, copy the access key and secret key that will be needed for the programmatic access. They will not be shown again, so store them safely and securely.   Here is the description of the user we just created.  Set up AWS CLI  The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.\n  Head over to https://aws.amazon.com/cli/ and download a suitable installer for your OS. Install it and open your terminal. If it is in the PATH, you should be able to verify the following:\nwhich aws #/usr/local/bin/aws #your location could be different. aws --version #aws-cli/2.0.45 Python/3.7.4 Darwin/18.6.0 exe/x86_64  Using the access and secret keys to configure aws using the aws configure command.\n   Test S3 access by issing the following commmand: aws s3 ls\n$ aws s3 ls 2020-09-03 12:27:48 chicagodatascience-com 2020-09-03 12:02:25 theja-model-store  Instead of programmatically creating a bucket (with the aws s3 mb s3://bucket-name) we will do it using the GUI below.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions_model/",
	"title": "AWS Serverless Model Serving",
	"tags": [],
	"description": "",
	"content": " Storing the Model on S3  To set up S3 for model serving, we have to perform a number of steps. We start with the s3 page.  - Create a bucket with an informative name.\n We don\u0026rsquo;t have to touch any of these for now.   Here the summary to review.   And we can see the bucket in the list of buckets.  Zip of Local Environment  We need a zip of local environment that includes all dependent libraries. This is because there is no way to specify requirements.txt like in Cloud Functions. This zip file can be uploaded to the bucket we created above. We start with creating a directory of all dependencies.\nmkdir lambda_model cd lambda_model # pip install pickle5 -t . #if we had a specific requirement, we would execute this line  First we will read the model and the movie.dat files (presumably in the parent directory) to create two new dictionaries:\n#Dumping the recommendations and movie info as dictionaries. from surprise import Dataset import pandas as pd import pickle5 import pickle import json def load(file_name): dump_obj = pickle5.load(open(file_name, 'rb')) return dump_obj['predictions'], dump_obj['algo'] def get_top_n(predictions, n=10): # First map the predictions to each user. top_n = {} for uid, iid, true_r, est, _ in predictions: if uid not in top_n: top_n[uid] = [] top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n def defaultdict(default_type): class DefaultDict(dict): def __getitem__(self, key): if key not in self: dict.__setitem__(self, key, default_type()) return dict.__getitem__(self, key) return DefaultDict() df = pd.read_csv('../movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('../surprise_model') top_n = get_top_n(predictions, n=5) df = df.drop(['genre'],axis=1) movie_dict = df.T.to_dict() pickle.dump(movie_dict,open('movie_dict.pkl','wb')) pickle.dump(top_n,open('top_n.pkl','wb'))  Next we will create the following file called lambda_function.py in this directory:\nimport json # top_n = {'196':[(1,3),(2,4)]} # movie_dict = {1:{'name':'a'},2:{'name':'b'}} def lambda_handler(event,context): data = {\u0026quot;success\u0026quot;: False} with open(\u0026quot;top_n.json\u0026quot;, \u0026quot;r\u0026quot;) as read_file: top_n = json.load(read_file) with open(\u0026quot;movie_dict.json\u0026quot;, \u0026quot;r\u0026quot;) as read_file: movie_dict = json.load(read_file) print(event) #debug if \u0026quot;body\u0026quot; in event: event = event[\u0026quot;body\u0026quot;] if event is not None: event = json.loads(event) else: event = {} if \u0026quot;uid\u0026quot; in event: data[\u0026quot;response\u0026quot;] = str([movie_dict.get(iid,{'name':None})['name'] for (iid, _) in top_n[event.get(\u0026quot;uid\u0026quot;)]]) data[\u0026quot;success\u0026quot;] = True return { 'statusCode': 200, 'headers':{'Content-Type':'application/json'}, 'body': json.dumps(data) }  Here instead of a request object in GCP, a pair (event,context) are taken as input. The event object will have the query values. See this for more details.\n Next we zip the model and its dependencies and upload to S3.\nzip -r recommend.zip . aws s3 cp recommend.zip s3://theja-model-store/recommend.zip aws s3 ls s3://theja-model-store/  Add the API Gateway as before and see the predictions in action!\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/pytorch_training/",
	"title": "Recommendation (Pytorch) Training",
	"tags": [],
	"description": "",
	"content": "Please install the package using the command conda install -c conda-forge scikit-surprise in the ight environment.\n# https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.accuracy import rmse from surprise.dump import dump import numpy as np import torch from torch import nn import torch.nn.functional as F from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator from ignite.metrics import Loss, MeanSquaredError from datetime import datetime from sklearn.utils import shuffle class Loader(): current = 0 def __init__(self, x, y, batchsize=1024, do_shuffle=True): self.shuffle = shuffle self.x = x self.y = y self.batchsize = batchsize self.batches = range(0, len(self.y), batchsize) if do_shuffle: # Every epoch re-shuffle the dataset self.x, self.y = shuffle(self.x, self.y) def __iter__(self): # Reset \u0026amp; return a new iterator self.x, self.y = shuffle(self.x, self.y, random_state=0) self.current = 0 return self def __len__(self): # Return the number of batches return int(len(self.x) / self.batchsize) def __next__(self): n = self.batchsize if self.current + n \u0026gt;= len(self.y): raise StopIteration i = self.current xs = torch.from_numpy(self.x[i:i + n]) ys = torch.from_numpy(self.y[i:i + n]) self.current += n return (xs, ys) def l2_regularize(array): loss = torch.sum(array ** 2.0) return loss class MF(nn.Module): itr = 0 def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0): super(MF, self).__init__() self.k = k self.n_user = n_user self.n_item = n_item self.c_bias = c_bias self.c_vector = c_vector self.user = nn.Embedding(n_user, k) self.item = nn.Embedding(n_item, k) # We've added new terms here: self.bias_user = nn.Embedding(n_user, 1) self.bias_item = nn.Embedding(n_item, 1) self.bias = nn.Parameter(torch.ones(1)) def __call__(self, train_x): user_id = train_x[:, 0] item_id = train_x[:, 1] vector_user = self.user(user_id) vector_item = self.item(item_id) # Pull out biases bias_user = self.bias_user(user_id).squeeze() bias_item = self.bias_item(item_id).squeeze() biases = (self.bias + bias_user + bias_item) ui_interaction = torch.sum(vector_user * vector_item, dim=1) # Add bias prediction to the interaction prediction prediction = ui_interaction + biases return prediction def loss(self, prediction, target): loss_mse = F.mse_loss(prediction, target.squeeze()) # Add new regularization to the biases prior_bias_user = l2_regularize(self.bias_user.weight) * self.c_bias prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias prior_user = l2_regularize(self.user.weight) * self.c_vector prior_item = l2_regularize(self.item.weight) * self.c_vector total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item return total def log_training_loss(engine, log_interval=400): epoch = engine.state.epoch itr = engine.state.iteration fmt = \u0026quot;Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\u0026quot; msg = fmt.format(epoch, itr, len(train_loader), engine.state.output) model.itr = itr if itr % log_interval == 0: print(msg) def log_validation_results(engine): evaluat.run(test_loader) metrics = evaluat.state.metrics avg_accuracy = metrics['accuracy'] print(\u0026quot;Epoch[{}] Validation MSE: {:.2f} \u0026quot; .format(engine.state.epoch, avg_accuracy))  #Data data = Dataset.load_builtin('ml-100k') trainset = data.build_full_trainset() uir = np.array([x for x in trainset.all_ratings()]) train_x = test_x = uir[:,:2].astype(np.int64) train_y = test_y = uir[:,2].astype(np.float32)  #Parameters lr = 1e-2 k = 10 #latent dimension c_bias = 1e-6 c_vector = 1e-6 batchsize = 1024 model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector) optimizer = torch.optim.Adam(model.parameters()) trainer = create_supervised_trainer(model, optimizer, model.loss) metrics = {'accuracy': MeanSquaredError()} evaluat = create_supervised_evaluator(model, metrics=metrics) train_loader = Loader(train_x, train_y, batchsize=batchsize) test_loader = Loader(test_x, test_y, batchsize=batchsize) trainer.add_event_handler(event_name=Events.ITERATION_COMPLETED, handler=log_training_loss) trainer.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=log_validation_results)  \u0026lt;ignite.engine.events.RemovableEventHandle at 0x7f011e8a67c0\u0026gt;  model  MF( (user): Embedding(943, 10) (item): Embedding(1682, 10) (bias_user): Embedding(943, 1) (bias_item): Embedding(1682, 1) )  trainer.run(train_loader, max_epochs=50)  Epoch[1] Validation MSE: 17.82 Epoch[2] Validation MSE: 16.02 Epoch[3] Validation MSE: 14.43 Epoch[4] Validation MSE: 13.04 Epoch[5] Iteration[400/97] Loss: 12.52 Epoch[5] Validation MSE: 11.79 Epoch[6] Validation MSE: 10.70 Epoch[7] Validation MSE: 9.71 Epoch[8] Validation MSE: 8.85 Epoch[9] Iteration[800/97] Loss: 8.95 Epoch[9] Validation MSE: 8.08 Epoch[10] Validation MSE: 7.40 Epoch[11] Validation MSE: 6.80 Epoch[12] Validation MSE: 6.27 Epoch[13] Iteration[1200/97] Loss: 6.50 Epoch[13] Validation MSE: 5.79 Epoch[14] Validation MSE: 5.36 Epoch[15] Validation MSE: 4.98 Epoch[16] Validation MSE: 4.63 Epoch[17] Iteration[1600/97] Loss: 4.80 Epoch[17] Validation MSE: 4.32 Epoch[18] Validation MSE: 4.04 Epoch[19] Validation MSE: 3.79 Epoch[20] Validation MSE: 3.56 Epoch[21] Iteration[2000/97] Loss: 3.35 Epoch[21] Validation MSE: 3.35 Epoch[22] Validation MSE: 3.15 Epoch[23] Validation MSE: 2.97 Epoch[24] Validation MSE: 2.81 Epoch[25] Iteration[2400/97] Loss: 2.73 Epoch[25] Validation MSE: 2.66 Epoch[26] Validation MSE: 2.53 Epoch[27] Validation MSE: 2.40 Epoch[28] Validation MSE: 2.28 Epoch[29] Iteration[2800/97] Loss: 2.31 Epoch[29] Validation MSE: 2.17 Epoch[30] Validation MSE: 2.07 Epoch[31] Validation MSE: 1.97 Epoch[32] Validation MSE: 1.89 Epoch[33] Iteration[3200/97] Loss: 1.82 Epoch[33] Validation MSE: 1.81 Epoch[34] Validation MSE: 1.73 Epoch[35] Validation MSE: 1.66 Epoch[36] Validation MSE: 1.60 Epoch[37] Validation MSE: 1.54 Epoch[38] Iteration[3600/97] Loss: 1.60 Epoch[38] Validation MSE: 1.48 Epoch[39] Validation MSE: 1.43 Epoch[40] Validation MSE: 1.38 Epoch[41] Validation MSE: 1.34 Epoch[42] Iteration[4000/97] Loss: 1.27 Epoch[42] Validation MSE: 1.29 Epoch[43] Validation MSE: 1.25 Epoch[44] Validation MSE: 1.22 Epoch[45] Validation MSE: 1.19 Epoch[46] Iteration[4400/97] Loss: 1.11 Epoch[46] Validation MSE: 1.15 Epoch[47] Validation MSE: 1.13 Epoch[48] Validation MSE: 1.10 Epoch[49] Validation MSE: 1.07 Epoch[50] Iteration[4800/97] Loss: 1.11 Epoch[50] Validation MSE: 1.05 State: iteration: 4850 epoch: 50 epoch_length: 97 max_epochs: 50 output: 1.0818936824798584 batch: \u0026lt;class 'tuple'\u0026gt; metrics: \u0026lt;class 'dict'\u0026gt; dataloader: \u0026lt;class '__main__.Loader'\u0026gt; seed: \u0026lt;class 'NoneType'\u0026gt; times: \u0026lt;class 'dict'\u0026gt;  torch.save(model.state_dict(), \u0026quot;./pytorch_model\u0026quot;)   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/pytorch_inference/",
	"title": "Recommendation (Pytorch) Inference",
	"tags": [],
	"description": "",
	"content": "from surprise import Dataset import numpy as np import torch from torch import nn import pandas as pd class MF(nn.Module): itr = 0 def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0): super(MF, self).__init__() self.k = k self.n_user = n_user self.n_item = n_item self.c_bias = c_bias self.c_vector = c_vector self.user = nn.Embedding(n_user, k) self.item = nn.Embedding(n_item, k) # We've added new terms here: self.bias_user = nn.Embedding(n_user, 1) self.bias_item = nn.Embedding(n_item, 1) self.bias = nn.Parameter(torch.ones(1)) def __call__(self, train_x): user_id = train_x[:, 0] item_id = train_x[:, 1] vector_user = self.user(user_id) vector_item = self.item(item_id) # Pull out biases bias_user = self.bias_user(user_id).squeeze() bias_item = self.bias_item(item_id).squeeze() biases = (self.bias + bias_user + bias_item) ui_interaction = torch.sum(vector_user * vector_item, dim=1) # Add bias prediction to the interaction prediction prediction = ui_interaction + biases return prediction def loss(self, prediction, target): loss_mse = F.mse_loss(prediction, target.squeeze()) # Add new regularization to the biases prior_bias_user = l2_regularize(self.bias_user.weight) * self.c_bias prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias prior_user = l2_regularize(self.user.weight) * self.c_vector prior_item = l2_regularize(self.item.weight) * self.c_vector total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item return total def get_top_n(model,testset,trainset,uid_input,n=10): preds = [] try: uid_input = int(trainset.to_inner_uid(uid_input)) except KeyError: return preds # First map the predictions to each user. for uid, iid, _ in testset: #inefficient try: uid_internal = int(trainset.to_inner_uid(uid)) except KeyError: continue if uid_internal==uid_input: try: iid_internal = int(trainset.to_inner_iid(iid)) movie_name = df.loc[int(iid),'name'] preds.append((iid,movie_name,float(model(torch.tensor([[uid_input,iid_internal]]))))) except KeyError: pass # Then sort the predictions for each user and retrieve the k highest ones if preds is not None: preds.sort(key=lambda x: x[1], reverse=True) if len(preds) \u0026gt; n: preds = preds[:n] return preds  #Data df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) print(df.head()) data = Dataset.load_builtin('ml-100k') trainset = data.build_full_trainset() testset = trainset.build_anti_testset()   name genre iid 1 Toy Story (1995) Animation|Children's|Comedy 2 Jumanji (1995) Adventure|Children's|Fantasy 3 Grumpier Old Men (1995) Comedy|Romance 4 Waiting to Exhale (1995) Comedy|Drama 5 Father of the Bride Part II (1995) Comedy  #Parameters lr = 1e-2 k = 10 #latent dimension c_bias = 1e-6 c_vector = 1e-6 model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector) model.load_state_dict(torch.load('./pytorch_model')) model.eval()  MF( (user): Embedding(943, 10) (item): Embedding(1682, 10) (bias_user): Embedding(943, 1) (bias_item): Embedding(1682, 1) )  # Print the recommended items for each user limit = 0 for uid,_,_ in testset: print('\\nUser:',uid) seen = [df.loc[int(iid),'name'] for (iid, _) in trainset.ur[int(uid)]] if len(seen) \u0026gt; 10: seen = seen[:10] print('\\tSeen:',seen) print('\\tRecommendations:',get_top_n(model,testset,trainset,uid,n=10)) limit+=1 if limit\u0026gt;3: break  User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)'] User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)'] User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)'] User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)']  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/intro/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " TLDR Problem  Environmental compatibility is a headache, in addition to scaling, security, maintenance and upgrade of software and hardware.  For instance, in the serverless examples, the need for pickle5 instead of pickle was due to such incompatibility.  For hosted environments, we have to work hard on the devops to ensure the environments are the same. For serverless, we did this via the requirements file (Cloud Functions) and locally installing python packages (Lambda functions)  Solution  Containers\n they allow you to run arbitrary programs on new environments as long as they have a common piece of software (e.g., docker).  Their use cases include:\n Webapp deployments (real time dashboards) Model deployments (our objective) Packaging up projects (like a snapshot) and archiving so you can return to the project with the runtime/environment all set to go.   Containers  Containers are tools to produce reproducible environments by isolating and packaging everything that your program needs, independent of the target/host environment where you intended to run the program. Many tasks can be isolated and served in such a way, including model deployments. A Container is a process with features to keep it isolated from the host system. Because of their architecture, they are much lighter on resources compared to a full blown VM. You could think of them as stripped down virtual machines (VMs). Obviously this comes with some trade-offs as well. Containers with our code/program/model in it can be distributed to team members and can be publicly released with minimal environment incompatibility issues. One key variant of this solution is Docker. There are others, but we will stick with this one. In summary, they are useful because:  consistency: in app development as well as its delivery portable: local machine or cloud lightweight: multiple containers on single host   Elastic Container Service by AWS  A proprietary solution by AWS Elastic Container Service (ECS) similar to Lambda functions, and allows one to be agnostic to devops details (which server, server configs, security etc). Lifts certain restrictions of Lambda functions:  Not specific to the runtimes available in Lambda functions No memory limit (e.g., 256MB)  In ECS, we have to define the container explicitly, and then the rest (scaling, fault-tolerance) is behind the scenes.  Note: While exploring ECS, keep a tab on billing (check every so often)!\nWhat we will study  Docker local experimentation Run docker on EC2/AWS Use ECS (and Elastic Container Registry/ECR) on AWS The end goal is a functionality that is almost serverless, but has more work, potentially costing less and also allowing for more flexibility.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/",
	"title": "Lecture 3",
	"tags": [],
	"description": "",
	"content": " Serving ML Models Using Containers on AWS - Docker - AWS Elastic Container Service with Fargate "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/docker/",
	"title": "Docker",
	"tags": [],
	"description": "",
	"content": " We will first get learn a bit more about docker. From their website:\n Docker is an open platform (written in Go) for developing, shipping, and running applications.\nDocker enables you to separate your applications from your infrastructure so you can deliver software quickly.\nWith Docker, you can manage your infrastructure in the same ways you manage your applications.\nBy taking advantage of Docker’s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.\n Docker Overview  A loosely isolated environment (a container) can be created using Docker. We can run multiple containers ona host (like our operating system: Mac/Win/Linux) Containers are light on resources compared to VMs because they run directly with the host machine\u0026rsquo;s kernel.  Example, you may have heard of the linux OS kernel. A kernel is a computer program at the core of a computer\u0026rsquo;s operating system with complete control over everything in the system   Source: https://en.wikipedia.org/wiki/Kernel_(operating_system) \nDocker Engine  Is an app that is comprised of\n a server (dockerd) that handles creating containers an API to talk to the server a CLI (docker) that uses the API\n The client and the daemon they interact via the API to handle creation and deployment of containers.\n   Source: https://docs.docker.com/get-started/overview/ \nDocker Architecture Source: https://docs.docker.com/get-started/overview/ \n Docker registry: stores Docker images. E.g., Docker hub is a public registry. Docker objects:  Images Networks Containers Volumnes Plugins  Image:\n Read only template of instructions to create a container. It is often based on other images. Dockerfile creates images: slightly changing it does not mean rebuilding from scratch (lightweight)  When we run the following command:\n$ docker run -i -t ubuntu /bin/bash   Docker pulls the image from the registry Creates a container Allocates a private filesystem Creates a network inteface Executes /bin/bash because of the -i -t flag  Services: Docker (\u0026gt; 1.12) has the capability to make multiple daemons work together (as a swarm) with load balancing automatically handled.\n Container:\n A runnable instance of an image Several operations: create, start, stop, move, delete. When deleted, it does NOT store state! In contrast, a VM is a guest operating system that accesses host resources through a hypervisor.   Source: https://docs.docker.com/get-started/\nRunning a Container  Download Docker for Desktop from here. You will have to create a free account with them.\n Check version docker --version\n Run a hello world container: docker run hello-world\n   In the above, I changed to the appropriate conda env, although it is not necessary to do so. Its just a good practice.\n We can check the list of images we have locally via:\n(datasci-dev) ttmac:~ theja$ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest bf756fb1ae65 8 months ago 13.3kB  We can check the active/recently run containers via the following command:\n(datasci-dev) ttmac:~ theja$ docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 76cf92eaec76 hello-world \u0026quot;/hello\u0026quot; 4 minutes ago Exited (0) 4 minutes ago great_sinoussi  create a folder and copy the flask weather service app (weather.py) from before.\n(datasci-dev) ttmac:code theja$ mkdir docker-weather-service (datasci-dev) ttmac:code theja$ cd docker-weather-service/ (datasci-dev) ttmac:code theja$ cp ../lecture1/weather.py docker-weather-service/  Recall that running python weather.py and accessing it from the browser using the url http://localhost:5000/?msg=Chicago should give you a JSON response back.\n We will write a Dockerfile as below to build a custom image.\nFROM debian:buster-slim MAINTAINER Your Name RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y python3-pip python3-dev \\ \u0026amp;\u0026amp; cd /usr/local/bin \\ \u0026amp;\u0026amp; ln -s /usr/bin/python3 python \\ \u0026amp;\u0026amp; pip3 install flask geopy requests COPY weather.py weather.py ENTRYPOINT [\u0026quot;python3\u0026quot;,\u0026quot;weather.py\u0026quot;]  Building the custom image is achieved by the following commands:\ndocker image build -t \u0026quot;weather_service\u0026quot; . docker images #to check the recently built image  You should see the following output (truncated, only showing the last)\nStep 4/5 : COPY weather.py weather.py ---\u0026gt; 6ad721a3d5ef Step 5/5 : ENTRYPOINT [\u0026quot;python3\u0026quot;,\u0026quot;weather.py\u0026quot;] ---\u0026gt; Running in 70347eb72094 Removing intermediate container 70347eb72094 ---\u0026gt; ffd1e8b9172f Successfully built ffd1e8b9172f Successfully tagged weather_service:latest (datasci-dev) ttmac:docker-weather-service theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE weather_service latest ffd1e8b9172f 9 seconds ago 492MB debian buster-slim c7346dd7f20e 4 weeks ago 69.2MB hello-world latest bf756fb1ae65 8 months ago 13.3kB  We will run the newly created weather_service image as a container locally suing the docker run command (recall how we did the hello world example).\ndocker run -d -p 5000:5000 weather_service docker ps  Here, the -d -p flags are to make the container run as a daemon (i.e., not interactive with the user) and to do port forwarding.\n(datasci-dev) ttmac:docker-weather-service theja$ docker run -d -p 5000:5000 weather_service 50dbba727a24c9534f0a88165a0601f9447ec5e2e3a6c21fe8443c12cfac63e4 (datasci-dev) ttmac:docker-weather-service theja$ docker ps -all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 50dbba727a24 weather_service \u0026quot;python3 weather.py\u0026quot; 4 seconds ago Up 3 seconds 0.0.0.0:5000-\u0026gt;5000/tcp tender_shtern  Going to your browser and accessing http://localhost:5000/?msg=Chicago should give the appropriate weather response JSON object.\n The model serving python files (both pytorch and surprise based) can be deployed locally or on a VPS (EC2/Vultr/Digitalocean) similarly.\n Since hosting a container does not by itself give us additional features such as fault tolerance and scalability (being able to respond to many requests simultaneously), we will make use of ECS and GKE to address thee.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/ecr/",
	"title": "Orchestration using ECS and ECR - Part I",
	"tags": [],
	"description": "",
	"content": " Intro  Orchestration means managing container life cycle from building them to deploying (which requires provisioning of appropriate compute resources, storage resources, networking resources), scaling, load-balancing and other tasks, while accounting for failures throughout.\n While there are many orchestration solutions, we will focus on a couple of them: ECS by AWS and Kubernetes (local hosted solution and managed by GCP). While there is Elastic Kubernetes Service (EKS) by AWS as well, we will omit it here, as the ideas are the same.\nWhy should data science professions know such orchestration solutions?\n Pro: Get key devops features (fault tolerance, scalability etc) with low on-going effort. The deployed service will not likely break down. Con: There is a non-trivial setup cost/complexity.  Next, we will (a) deploy our model serving docker image to the AWS container registry ECR, (b) use ECS to deploy a container based on that image, and \u0026copy; set up a load balancer that mediates requests to the prediction model.\nElastic Container Registry (ECR)  Sending the image to a model registry such as ECR is needed to access it at other places to create containers. ECR allows for better integration with the AWS platform, and works with EKS as well. We will create a docker image locally (can also be done on EC2) and push it to an ECR repository that we will create.\n Navigate to the ECR link on the AWS console.\n   Click create a repository \u0026lsquo;Get Started\u0026rsquo; button. ECR can have multiple repositories and each repository can hold multiple images.   Give a name to the repository. It will contain multiple Docker images.   Review the current repository list.   Next we will allow the programmatic user we created for accessing S3 (we gave the user name model-user) to also manage the ECR repositories. Navigate to IAM as shown below.   Click on the user who we want to edit access controls for.   Currently this user had S3 access (not relevant for us).   Click on attaching existing policies and search for AmazonEC2ContainerRegistryFullAccess.   Review your policy choice and proceed.   You can see the summary of permissions that this programmatic user has.  Creating the Model Prediction Docker Image  We will use the following flask app that uses the pytorch model to serve recommendations:\nfrom surprise import Dataset import torch import pandas as pd import flask class MF(torch.nn.Module): def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0): super(MF, self).__init__() self.k = k self.n_user = n_user self.n_item = n_item self.c_bias = c_bias self.c_vector = c_vector self.user = torch.nn.Embedding(n_user, k) self.item = torch.nn.Embedding(n_item, k) # We've added new terms here: self.bias_user = torch.nn.Embedding(n_user, 1) self.bias_item = torch.nn.Embedding(n_item, 1) self.bias = torch.nn.Parameter(torch.ones(1)) def __call__(self, train_x): user_id = train_x[:, 0] item_id = train_x[:, 1] vector_user = self.user(user_id) vector_item = self.item(item_id) # Pull out biases bias_user = self.bias_user(user_id).squeeze() bias_item = self.bias_item(item_id).squeeze() biases = (self.bias + bias_user + bias_item) ui_interaction = torch.sum(vector_user * vector_item, dim=1) # Add bias prediction to the interaction prediction prediction = ui_interaction + biases return prediction def loss(self, prediction, target): loss_mse = F.mse_loss(prediction, target.squeeze()) # Add new regularization to the biases prior_bias_user = l2_regularize(self.bias_user.weight) * self.c_bias prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias prior_user = l2_regularize(self.user.weight) * self.c_vector prior_item = l2_regularize(self.item.weight) * self.c_vector total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item return total def get_top_n(model,testset,trainset,uid_input,n=10): preds = [] try: uid_input = int(trainset.to_inner_uid(uid_input)) except KeyError: return preds # First map the predictions to each user. for uid, iid, _ in testset: #inefficient try: uid_internal = int(trainset.to_inner_uid(uid)) except KeyError: continue if uid_internal==uid_input: try: iid_internal = int(trainset.to_inner_iid(iid)) movie_name = df.loc[int(iid),'name'] preds.append((iid,movie_name,float(model(torch.tensor([[uid_input,iid_internal]]))))) except KeyError: pass # Then sort the predictions for each user and retrieve the k highest ones if preds is not None: preds.sort(key=lambda x: x[1], reverse=True) if len(preds) \u0026gt; n: preds = preds[:n] return preds app = flask.Flask(__name__) #Data df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) data = Dataset.load_builtin('ml-100k',prompt=False) ''' Exercise: remove the above dependency. Currently it downloads data from grouplens website and stores in .surprise folder in $HOME ''' trainset = data.build_full_trainset() testset = trainset.build_anti_testset() #Parameters that are needed to reload the model from disk k = 10 #latent dimension c_bias = 1e-6 c_vector = 1e-6 model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector) model.load_state_dict(torch.load('./pytorch_model')) model.eval() #no need for gradient computations in this setting # define a predict function as an endpoint @app.route(\u0026quot;/\u0026quot;, methods=[\u0026quot;GET\u0026quot;]) def predict(): data = {\u0026quot;success\u0026quot;: False} # check for passed in parameters params = flask.request.json if params is None: params = flask.request.args if \u0026quot;uid\u0026quot; in params.keys(): data[\u0026quot;response\u0026quot;] = get_top_n(model,testset,trainset,params['uid'],n=10) data[\u0026quot;success\u0026quot;] = True # return a response in json format return flask.jsonify(data) # start the flask app, allow remote connections app.run(host='0.0.0.0', port=80)  The corresponding Dockerfile is below. The key additional files in addition to recommend.py above are:\n movies.dat pytorch_model\nFROM continuumio/miniconda3:latest RUN conda install -y flask pandas \\ \u0026amp;\u0026amp; conda install -c conda-forge scikit-surprise \\ \u0026amp;\u0026amp; conda install pytorch torchvision cpuonly -c pytorch COPY recommend.py recommend.py COPY movies.dat movies.dat COPY pytorch_model pytorch_model ENTRYPOINT [\u0026quot;python\u0026quot;,\u0026quot;recommend.py\u0026quot;]   The miniconda image above is from https://hub.docker.com/r/continuumio/miniconda3.\n Building an image based on the above file and running our prediction locally can be done using the following commands:\ndocker image build -t \u0026quot;prediction_service\u0026quot; . docker run -d -p 5000:5000 prediction_service docker ps -a #check what all containers were/are running docker kill container_id #after checking that the service runs, we can safely stop and delete the container. docker rm container_id  If we run a container based on this image, the python file and others will be in the root (/) folder and will be run by the root user. While we will not improve this here, it is better to run services as non-root users.\n  Sending our Docker Image to ECR  We will follow the instruction here to push our image to the repository we just created.\n Assuming you have the aws CLI configured with the secret keys, run the following command:\naws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com  Substitute region with us-east-1 etc (check the URL on the ECR page) as well as aws_account_id with the actual account id. We should get a prompt saying \u0026lsquo;Login Succeeded\u0026rsquo;.\n Lets tag our image before sending it to ECR (replace account id and region below as well):\n(datasci-dev) ttmac:docker-prediction-service theja$ docker tag prediction_service aws_account_id.dkr.ecr.region.amazonaws.com/models:recommendations (datasci-dev) ttmac:docker-prediction-service theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE aws_account_id.dkr.ecr.region.amazonaws.com/recommendations pytorch_model 364179b27eb1 21 minutes ago 2.06GB weather_service latest 20d340f941c0 2 days ago 496MB debian buster-slim c7346dd7f20e 5 weeks ago 69.2MB continuumio/miniconda3 latest b4adc22212f1 6 months ago 429MB hello-world latest bf756fb1ae65 8 months ago 13.3kB  Pushing to ECR is achieved by the following:\ndocker push aws_account_id.dkr.region.amazonaws.com/recommendations:pytorch_model  You should see the update progress (this is a large upload!)\nThe push refers to repository [aws_account_id.dkr.ecr.us-east-2.amazonaws.com/recommendations] a5649bbe3e5f: Pushed 5c87fc4d582f: Pushed e1e8d92205bf: Pushed 5c6c81390816: Pushing [=========================\u0026gt; ] 848.8MB/1.635GB fcd8d39597dd: Pushed 875120aa853c: Pushed f2cb0ecef392: Pushed  And the push conclusion:\n(datasci-dev) ttmac:docker-prediction-service theja$ docker push aws_account_id.dkr.ecr.us-east-2.amazonaws.com/recommendations:pytorch_model The push refers to repository [aws_account_id.dkr.ecr.us-east-2.amazonaws.com/recommendations] a5649bbe3e5f: Pushed 5c87fc4d582f: Pushed e1e8d92205bf: Pushed 5c6c81390816: Pushed fcd8d39597dd: Pushed 875120aa853c: Pushed f2cb0ecef392: Pushed pytorch_model: digest: sha256:af5dfaf227cd96c4ca8ca952c511fb4274c59d76574726462137bc7c4230be07 size: 1793  On the ECR page, if we look at the images in the recommendations repository, it will contain our recently uploaded image.\n   There is a friendly help box that details specific (to your account) commands for pushing images to ECR on the above page as well. You could use that as a guiding reference, or the help page.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/ecs/",
	"title": "Orchestration using ECS and ECR - Part II",
	"tags": [],
	"description": "",
	"content": " Elastic Container Service (ECS)  This is a AWS propreitary solution for container orchestration. There are three key concepts to work with this solution:  Service: Manages containers and relates them to EC2 machines as needed Task: Is a specific container Cluster: Is the environment of EC2 machines where containers live  The below diagram illustrates these relationships.  Source: https://aws.com/ \n We will set up a cluster and run a task/container and use a service to manage it. We will use Fargate for this exercise, although there are other more detailed ways to achieve the same end goals. Using Fargate will hide a lot of complexity, especially with privisioning the underlying EC2 instances.\n Lets start by getting to ECS.\n   Hit the get started button above or navigate to the clusters link on the left and then hit the get started button below.   Choose the custom container definition and hit configure.   Name the container, and point it to the ECR image URI. Specify the port to be 80 (this is what we choose in recommend.py). Then hit update.   We will keep the task definition to the default values.   Similarly we will retain the defaults for the service definition on the next page as well.   Name the cluster and then review the settings. Note that a lot is happening under the hood.   Review the settings as shown below:   The cluster gets created and you can see the status for all tasks change to green in due time.   Click view service and then click the Tasks tab at the center/bottom part of the screen.   Clicking on the task (there should only be one task listed) will show the public IP through which we can seek model predicitons.   Navigating to our browser and accessing the IP and making a query such as http://18.220.91.58/?uid=20 gives the following response.  Setting Up a Load Balancer  Directly accessing the IP address (and thus the single container) is not scalable. A Load Balancer will give a static URL and route incoming HTTP(S) requests to ECS tasks managed by an ECS service.\n There are many types of load balancers on AWS, see here. We will use an application load balancer (ALB).\n Using the \u0026lsquo;Get Started\u0026rsquo; workflow on ECS is the easiest to set this up to work with the cluster.\n   Otherwise, we can instantiate the same from the EC2 page.   The load balancer uses a VPC (virtual private cloud, an AWS terminology and service), a security group (who can access our container) and a target group (who to route requests to).\n Aside: VPC essentially isolates your computing environment from the external world.\n   Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. You can use both IPv4 and IPv6 in your VPC for secure and easy access to resources and applications.\n  The ECS service should use the load balancer as shown below (if we did it using \u0026lsquo;Get Started\u0026rsquo; workflow on ECS).   We can also add it separately while creating the service (we will skip the details here).\n You can access the public static url by navigating to the load balancer page.\n First click on the load balancer on the service page shown above.\n Then click on the load balancer link to the top right of the page.\n   The public URL is the DNS name (the IP itself will be dynamic). You can bind it to your domain (using CNAME records).  Recap: Summary for ECS  Since there are a lot of steps involved (as well as quite a few moving parts), its good to revisit what our original goal is.\n Our goal was to set the model prediction/deployment up in such a way that it scales and has no issues with failures.\n The ECS cluster is scalable (we can add more tasks and services easily).\n Further, the ECS service manages these tasks such that even if the underlying EC2 instances that run these containers fail (could be any reason) the task can be restarted on other machines to keep everything running smoothly.\n Finally the load balancer, maps a static external IP to the internal container(s). So if there are multiple model prediction containers, the load balancer will use an algorithm (such as round robin) to distribute the incoming requests.\n While there is a lot more work to set this up unlike the serverless solution, there is more fine grained control and visibility into the components supporting your scalable model deployment effort.\n  Tear Down the ECS Cluster  Follow Step 7 onwards from this link: https://aws.amazon.com/getting-started/hands-on/deploy-docker-containers/  Essentially update the service to ensure that the number of tasks is 0. Then delete the service and stop the task and delete the task and task definition. Then delete the cluster itself. Check on the EC2, VPC and Load balancing pages if you are still consuming resources.   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": " Caveat: Unless we have a truly massive or complex system, we probably don’t need Kubernetes, and using it should be the result of a deliberate cost benefit analysis in comparison to other hosted solutions or managed solutions.\nIntroduction  Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. It was open-sourced by Google. Its predecessor was called borg internally. Kubernetes, or K8s for short, is a central orchestration system in large complex software systems. It coordinates a collection or cluster of computers with the purpose of working as a single unit. When we orchestrate containers with it, we are agnostic to which computer runs which container. K8s takes care of scheduling and distribution of containers to these unnamed computers behind the scenes.\n We will primarily be interested in ML model deployment.\n Lets start by going over some of the key concepts in the K8s container orchestration system below. For more information, have a look at the basics documented at https://kubernetes.io/docs/tutorials/kubernetes-basics/ and go from there.\n A K8s cluster has two resources:\n Master: coordinator of the cluster Nodes: workers are essentially VMs or computers that run containers.  Source: https://kubernetes.io/docs/tutorials/kubernetes-basics/create-cluster/cluster-intro/ \n Each node runs kubelet that manages it and communicated with the master. It also runs Docker daemon (or other technologies) to manage containers.\n The deploy sequence essentially involves the following:\n kubectl tells the master to start application containers. Master schedules containers to run.   Deployment  Once we have a cluster running, we can deploy containerized applications (one or many) using a Deployment configuration. We can think of this as a set of instructions to K8s to set up the application on the cluster. Essentially, the containers are mapped to individual nodes. After the mapping, a Deployment Controller keeps checking these instances allowing for self-healing.\nSource: https://kubernetes.io/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/ \n We create a deployment using a tool such as kubectl that uses an API to interact with the Master.\n When creating a deployment, we need to specify container images as well as the number of copies of each image we want to instantiate.\n  Nodes and Pods  When an app is deployed as a container, it is encapsulated in a pod on a specific node. A pod is a collection of containers that share a few things (see below) and are typically related to each other.  Storage Networking and IP address  These related containers can be a web server and a database for instance. We can also have a single container in a pod (this is what we will do here). Pods are the most basic unit in K8s. It is pods that are created and destroyed, not individual containers.  Source: https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/ \n A node is essentially a VM/machine and can have multiple pods (depending on how K8s schedules pods). A node runs:  Kubelet: a program that communicates between the master and the node and manages the pods on the node. Docker daemon (or equivalent) for pulling and running containers from images.   Source: https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/ \nServices  A Kubernetes Service is an abstraction layer which defines a logical set of Pods and enables external traffic exposure, load balancing and service discovery for those Pods. Source: https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/ \n  It enables coupling between pods (each of which have a unique IP). The pods with IPs cannot be typically accessed from the outside. Instead, a service can be used to allow external connections. For example, a service with a spec that says NodePort exposed pods on the same port of each selected node in the cluster using NAT (Network Address Translation). We will see an example while deploying our model.\nSource: https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/ \n The Service abstraction lets pods get deleted and replicated in the cluster with no influence on our app.\n A set of pods are matched to a service using labels and selectors. See https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/ for more information.\n  Scaling and Updating  See https://kubernetes.io/docs/tutorials/kubernetes-basics/scale/scale-intro/ for how an app can scale.\n See https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/ for information on how to perform a rolling update.\n  Installing kubectl, minikube, Virtualbox and Docker  We have already installed Docker for Desktop.  Virtualbox  Virtualbox is a software product for running virtual machines on a variety of host operating systems (MacOS/Windows/Linux).\n Go to https://www.virtualbox.org/ and download the installer for your OS. The installation is straightforward.\n Once installed, try to download a Linux distribution such as Debian or Fedora to try out how Virtualbox works.\n  Kubectl  kubectl is a client utility to talk to the the K8s server for container orchestration.\n Download kubectl from this page. Here is the example command for MacOS:\n(datasci-dev) ttmac:k8s theja$ curl -LO \u0026quot;https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl\u0026quot;  Change its permissions to make it executable and move it to a place where it can be on the $PATH. Check whether you can access it by querying its version information.\n(datasci-dev) ttmac:k8s theja$ ls -l total 96696 -rw-r--r-- 1 theja staff 49458208 Sep 15 21:18 kubectl (datasci-dev) ttmac:k8s theja$ chmod +x kubectl (datasci-dev) ttmac:k8s theja$ mv kubectl ~/Library/local/bin/ (datasci-dev) ttmac:k8s theja$ kubectl version --client Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;19\u0026quot;, GitVersion:\u0026quot;v1.19.1\u0026quot;, GitCommit:\u0026quot;206bcadf021e76c27513500ca24182692aabd17e\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2020-09-09T11:26:42Z\u0026quot;, GoVersion:\u0026quot;go1.15\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;darwin/amd64\u0026quot;}   Minikube  Minikube (minikube) is a tool that runs a single node K8s cluster in a virtual machine on our local computer. In particular, Minikube is a lightweight K8s implementation that creates a VM on your local machine and deploys a simple cluster containing only one node.\n Follow the instructions for your operating system at https://kubernetes.io/docs/tasks/tools/install-minikube/. For instance, to install it on MacOS, we do the following.\n We check if virtualization is supported. If we run the following command from the terminal, we expect the VMX acronym to be colored:\n(datasci-dev) ttmac:k8s theja$ sysctl -a | grep -E --color 'machdep.cpu.features|VMX' machdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX EST TM2 SSSE3 CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC POPCNT AES PCID XSAVE OSXSAVE TSCTMR AVX1.0 RDRAND F16C  We will download the stand-alone binary just as we did for kubectl and move it to the right path:\n(datasci-dev) ttmac:k8s theja$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 55.5M 100 55.5M 0 0 10.9M 0 0:00:05 0:00:05 --:--:-- 11.7M (datasci-dev) ttmac:k8s theja$ chmod +x minikube (datasci-dev) ttmac:k8s theja$ mv minikube ~/Library/local/bin/  Check that its on the path and working correctly by using the following status check:\n(datasci-dev) ttmac:k8s theja$ minikube status 🤷 There is no local cluster named \u0026quot;minikube\u0026quot; 👉 To fix this, run: \u0026quot;minikube start\u0026quot; (datasci-dev) ttmac:k8s theja$ minikube version minikube version: v1.13.0 commit: 0c5e9de4ca6f9c55147ae7f90af97eff5befef5f-dirty   Weather Service App  We will run the simple weather service application on K8s using Minikube. It will involve building a Docker image of the flask app, similar to before.\n The single node cluster can be started with the following command:\nminikube start   You will see the following blurb which ends with \u0026lsquo;Done! kubectl is now configured to use minikube by default\u0026rsquo; the first time you run it. You can stop minikube using minikube stop after the whole exercise is done.\n(datasci-dev) ttmac:k8s theja$ minikube start 😄 minikube v1.13.0 on Darwin 10.14.6 ✨ Automatically selected the hyperkit driver 💾 Downloading driver docker-machine-driver-hyperkit: \u0026gt; docker-machine-driver-hyperkit.sha256: 65 B / 65 B [---] 100.00% ? p/s 0s \u0026gt; docker-machine-driver-hyperkit: 10.90 MiB / 10.90 MiB 100.00% 7.97 MiB p 🔑 The 'hyperkit' driver requires elevated permissions. The following commands will be executed: $ sudo chown root:wheel /Users/theja/.minikube/bin/docker-machine-driver-hyperkit $ sudo chmod u+s /Users/theja/.minikube/bin/docker-machine-driver-hyperkit Password: 💿 Downloading VM boot image ... \u0026gt; minikube-v1.13.0.iso.sha256: 65 B / 65 B [-------------] 100.00% ? p/s 0s \u0026gt; minikube-v1.13.0.iso: 173.73 MiB / 173.73 MiB [ 100.00% 11.61 MiB p/s 15s 👍 Starting control plane node minikube in cluster minikube 💾 Downloading Kubernetes v1.19.0 preload ... \u0026gt; preloaded-images-k8s-v6-v1.19.0-docker-overlay2-amd64.tar.lz4: 486.28 MiB 🔥 Creating hyperkit VM (CPUs=2, Memory=4000MB, Disk=20000MB) ... 🐳 Preparing Kubernetes v1.19.0 on Docker 19.03.12 ... 🔎 Verifying Kubernetes components... 🌟 Enabled addons: default-storageclass, storage-provisioner 🏄 Done! kubectl is now configured to use \u0026quot;minikube\u0026quot; by default   If you run the status command, you should see the following:\n(datasci-dev) ttmac:k8s theja$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured  We can also query the nodes (we only have one) and more information about them as below:\n(datasci-dev) ttmac:k8s theja$ kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 131m v1.19.0 (datasci-dev) ttmac:k8s theja$ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 140m  Although we have the weather_service image (can be viewed by using the docker images command as long as the Docker daemon is running) in our local repository, we need to access and use the repository specific to Minikube. This can be done using the following two commands:\n(datasci-dev) ttmac:docker-weather-service theja$ minikube docker-env (datasci-dev) ttmac:docker-weather-service theja$ eval $(minikube -p minikube docker-env)  Now if we build the same weather service flask app (recall the corresponding Dockerfile and weather.py from before). This time, we will give a slightly different repository name.\n(datasci-dev) ttmac:docker-weather-service theja$ docker build -t weather-service-k8s/latest .  The newly created repository is now available in the minikube specific image repository (notice the other images for instance):\n(datasci-dev) ttmac:docker-weather-service theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE weather-service-k8s/latest latest ae4f44a22535 11 seconds ago 496MB debian buster-slim 052664ad4351 6 days ago 69.2MB gcr.io/k8s-minikube/storage-provisioner v3 bad58561c4be 2 weeks ago 29.7MB k8s.gcr.io/kube-proxy v1.19.0 bc9c328f379c 2 weeks ago 118MB k8s.gcr.io/kube-apiserver v1.19.0 1b74e93ece2f 2 weeks ago 119MB k8s.gcr.io/kube-controller-manager v1.19.0 09d665d529d0 2 weeks ago 111MB k8s.gcr.io/kube-scheduler v1.19.0 cbdc8369d8b1 2 weeks ago 45.7MB k8s.gcr.io/etcd 3.4.9-1 d4ca8726196c 2 months ago 253MB kubernetesui/dashboard v2.0.3 503bc4b7440b 2 months ago 225MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 3 months ago 45.2MB kubernetesui/metrics-scraper v1.0.4 86262685d9ab 5 months ago 36.9MB k8s.gcr.io/pause 3.2 80d28bedfe5d 7 months ago 683kB  Although there is a local image, we need to set a specific parameter to make kubectl not search for the image elsewhere. In order to do so, we create the following yaml (a data-serialization language thats quite popular to set config parameters and is very tab/space sensitive). See https://en.wikipedia.org/wiki/YAML for more info on yaml files.\n The yaml was generated automatically by running the following command and then modified by adding the line imagePullPolicy: Never:\n(datasci-dev) ttmac:docker-weather-service theja$ kubectl create deployment weather-minikube --image=weather-service-k8s:latest -o yaml --dry-run=client  apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: weather-minikube name: weather-minikube spec: replicas: 1 selector: matchLabels: app: weather-minikube strategy: {} template: metadata: creationTimestamp: null labels: app: weather-minikube spec: containers: - image: weather-service-k8s/latest:latest name: weather-service-k8s resources: {} imagePullPolicy: Never status: {}  Save the above in a file called. weather_minikube.yaml. Next we create a new deployment using the minikube cluster.\n(datasci-dev) ttmac:docker-weather-service theja$ kubectl apply -f weather_minikube.yaml deployment.apps/weather-minikube created (datasci-dev) ttmac:docker-weather-service theja$ kubectl get all NAME READY STATUS RESTARTS AGE pod/weather-minikube-56fd45dd59-zkmmg 1/1 Running 0 71s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 160m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/weather-minikube 1/1 1 1 71s NAME DESIRED CURRENT READY AGE replicaset.apps/weather-minikube-76cb958c4 1 1 1 71s  Next, we will expose this deployment. The option --type=NodePort specifies the type of the Service.\n(datasci-dev) ttmac:docker-weather-service theja$ kubectl expose deployment weather-minikube --type=NodePort --port=5000 service/weather-minikube exposed  We can check that the container is in fact running fine using the following command:\n(datasci-dev) ttmac:docker-weather-service theja$ kubectl get pod NAME READY STATUS RESTARTS AGE weather-minikube-56fd45dd59-zkmmg 1/1 Running 0 171m  Once we have verified that the container is running, we can figure out the URL that is being exposed by the system as follows:\n(datasci-dev) ttmac:docker-weather-service theja$ minikube service weather-minikube --url http://192.168.64.2:32233  We can then access the weather service flask app from the browser and make sure that it works with a few test examples:\n   We may have to debug our container for various reasons. For instance, it can throw up 500 errors, which correspond to internal server errors due to errors in the python function attached to a URL route in Flask. There are a couple of options:\n We can look at the logs (the last argument is the pod name and it has a single container in our example):\nkubectl logs weather-minikube-56fd45dd59-zkmmg  We can access the container shell (assuming it has bash at the appropriate path) by referencing the pod name and try to see which commands are not working:\nkubectl exec --stdin --tty weather-minikube-56fd45dd59-zkmmg -- /bin/bash  Finally, the command kubectl describe also helps in debugging by showing us detailed information associated with any resource.\n  Teardown involves the following commands:\n(datasci-dev) ttmac:docker-weather-service theja$ kubectl delete services weather-minikube service \u0026quot;weather-minikube\u0026quot; deleted (datasci-dev) ttmac:docker-weather-service theja$ kubectl delete deployment weather-minikube deployment.apps \u0026quot;weather-minikube\u0026quot; deleted  We can then stop and delete the minikube cluster using the minikube stop and minikube delete commands:\n(datasci-dev) ttmac:docker-weather-service theja$ minikube stop ✋ Stopping node \u0026quot;minikube\u0026quot; ... 🛑 1 nodes stopped.  Finally, you can delete the cluster using the command minikube delete.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/",
	"title": "Lecture 4",
	"tags": [],
	"description": "",
	"content": " Orchestration using Kubernetes - Kubernetes - Google Kubernetes Engine (GKE) "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/modelkube/",
	"title": "Model Serving using Kubernetes",
	"tags": [],
	"description": "",
	"content": "This time, instead of the weather app, we will deploy a container containing our recommendation model. Here are the steps.\n Lets start minicube\n(datasci-dev) ttmac:docker-prediction-service theja$ minikube start 😄 minikube v1.13.0 on Darwin 10.14.6 ▪ MINIKUBE_ACTIVE_DOCKERD=minikube ✨ Using the hyperkit driver based on existing profile 👍 Starting control plane node minikube in cluster minikube 🔄 Restarting existing hyperkit VM for \u0026quot;minikube\u0026quot; ... 🐳 Preparing Kubernetes v1.19.0 on Docker 19.03.12 ... 🔎 Verifying Kubernetes components... 🌟 Enabled addons: default-storageclass, storage-provisioner 🏄 Done! kubectl is now configured to use \u0026quot;minikube\u0026quot; by default  Ensure that docker can see minicube specific registry\n(datasci-dev) ttmac:docker-prediction-service theja$ minikube docker-env export DOCKER_TLS_VERIFY=\u0026quot;1\u0026quot; export DOCKER_HOST=\u0026quot;tcp://192.168.64.2:2376\u0026quot; export DOCKER_CERT_PATH=\u0026quot;/Users/theja/.minikube/certs\u0026quot; export MINIKUBE_ACTIVE_DOCKERD=\u0026quot;minikube\u0026quot; # To point your shell to minikube's docker-daemon, run: # eval $(minikube -p minikube docker-env) (datasci-dev) ttmac:docker-prediction-service theja$ eval $(minikube -p minikube docker-env)  Go to the directory containing the trained model that we created before. The directory should have\n a Dockerfile to build an image, the pytorch_model for recommendations, movies.dat metadata, and recommend.py flask app.  Once there, build an image. This image will be registered with minikube\u0026rsquo;s registry.\n(datasci-dev) ttmac:docker-prediction-service theja$ docker build -t prediction-service-k8s . Sending build context to Docker daemon 304.6kB Step 1/8 : FROM continuumio/miniconda3:latest latest: Pulling from continuumio/miniconda3 68ced04f60ab: Pull complete 9c388eb6d33c: Pull complete 96cf53b3a9dd: Pull complete Digest: sha256:456e3196bf3ffb13fee7c9216db4b18b5e6f4d37090b31df3e0309926e98cfe2 Status: Downloaded newer image for continuumio/miniconda3:latest ---\u0026gt; b4adc22212f1 Step 2/8 : MAINTAINER Theja Tulabandhula ---\u0026gt; Running in 9b4a3708a4f6 Removing intermediate container 9b4a3708a4f6 ---\u0026gt; 2ebbbd14e3d3 Step 3/8 : RUN conda install -y flask pandas \u0026amp;\u0026amp; conda install -c conda-forge scikit-surprise \u0026amp;\u0026amp; conda install pytorch torchvision cpuonly -c pytorch ---\u0026gt; Running in 0ce30dc6b5a6 Collecting package metadata (current_repodata.json): ...working... done Solving environment: ...working... done ## Package Plan ## environment location: /opt/conda added / updated specs: - flask - pandas The following packages will be downloaded: package | build ---------------------------|----------------- blas-1.0 | mkl 6 KB ca-certificates-2020.7.22 | 0 125 KB certifi-2020.6.20 | py37_0 156 KB click-7.1.2 | py_0 71 KB conda-4.8.4 | py37_0 2.9 MB flask-1.1.2 | py_0 78 KB intel-openmp-2020.2 | 254 786 KB itsdangerous-1.1.0 | py37_0 28 KB jinja2-2.11.2 | py_0 103 KB markupsafe-1.1.1 | py37h14c3975_1 26 KB mkl-2020.2 | 256 138.3 MB mkl-service-2.3.0 | py37he904b0f_0 218 KB mkl_fft-1.1.0 | py37h23d657b_0 143 KB mkl_random-1.1.1 | py37h0573a6f_0 322 KB numpy-1.19.1 | py37hbc911f0_0 21 KB numpy-base-1.19.1 | py37hfa32c7d_0 4.1 MB openssl-1.1.1g | h7b6447c_0 2.5 MB pandas-1.1.1 | py37he6710b0_0 8.2 MB python-dateutil-2.8.1 | py_0 215 KB pytz-2020.1 | py_0 184 KB werkzeug-1.0.1 | py_0 240 KB ------------------------------------------------------------ Total: 158.6 MB The following NEW packages will be INSTALLED: blas pkgs/main/linux-64::blas-1.0-mkl click pkgs/main/noarch::click-7.1.2-py_0 flask pkgs/main/noarch::flask-1.1.2-py_0 intel-openmp pkgs/main/linux-64::intel-openmp-2020.2-254 itsdangerous pkgs/main/linux-64::itsdangerous-1.1.0-py37_0 jinja2 pkgs/main/noarch::jinja2-2.11.2-py_0 markupsafe pkgs/main/linux-64::markupsafe-1.1.1-py37h14c3975_1 mkl pkgs/main/linux-64::mkl-2020.2-256 mkl-service pkgs/main/linux-64::mkl-service-2.3.0-py37he904b0f_0 mkl_fft pkgs/main/linux-64::mkl_fft-1.1.0-py37h23d657b_0 mkl_random pkgs/main/linux-64::mkl_random-1.1.1-py37h0573a6f_0 numpy pkgs/main/linux-64::numpy-1.19.1-py37hbc911f0_0 numpy-base pkgs/main/linux-64::numpy-base-1.19.1-py37hfa32c7d_0 pandas pkgs/main/linux-64::pandas-1.1.1-py37he6710b0_0 python-dateutil pkgs/main/noarch::python-dateutil-2.8.1-py_0 pytz pkgs/main/noarch::pytz-2020.1-py_0 werkzeug pkgs/main/noarch::werkzeug-1.0.1-py_0 The following packages will be UPDATED: ca-certificates 2020.1.1-0 --\u0026gt; 2020.7.22-0 certifi 2019.11.28-py37_0 --\u0026gt; 2020.6.20-py37_0 conda 4.8.2-py37_0 --\u0026gt; 4.8.4-py37_0 openssl 1.1.1d-h7b6447c_4 --\u0026gt; 1.1.1g-h7b6447c_0 Downloading and Extracting Packages blas-1.0 | 6 KB | ########## | 100% flask-1.1.2 | 78 KB | ########## | 100% certifi-2020.6.20 | 156 KB | ########## | 100% markupsafe-1.1.1 | 26 KB | ########## | 100% numpy-base-1.19.1 | 4.1 MB | ########## | 100% pytz-2020.1 | 184 KB | ########## | 100% python-dateutil-2.8. | 215 KB | ########## | 100% itsdangerous-1.1.0 | 28 KB | ########## | 100% openssl-1.1.1g | 2.5 MB | ########## | 100% click-7.1.2 | 71 KB | ########## | 100% conda-4.8.4 | 2.9 MB | ########## | 100% mkl-service-2.3.0 | 218 KB | ########## | 100% werkzeug-1.0.1 | 240 KB | ########## | 100% pandas-1.1.1 | 8.2 MB | ########## | 100% mkl_fft-1.1.0 | 143 KB | ########## | 100% mkl-2020.2 | 138.3 MB | ########## | 100% mkl_random-1.1.1 | 322 KB | ########## | 100% jinja2-2.11.2 | 103 KB | ########## | 100% intel-openmp-2020.2 | 786 KB | ########## | 100% ca-certificates-2020 | 125 KB | ########## | 100% numpy-1.19.1 | 21 KB | ########## | 100% Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done Collecting package metadata (current_repodata.json): ...working... done Solving environment: ...working... done ## Package Plan ## environment location: /opt/conda added / updated specs: - scikit-surprise The following packages will be downloaded: package | build ---------------------------|----------------- ca-certificates-2020.6.20 | hecda079_0 145 KB conda-forge certifi-2020.6.20 | py37hc8dfbb8_0 151 KB conda-forge conda-4.8.5 | py37hc8dfbb8_1 3.0 MB conda-forge joblib-0.16.0 | py_0 203 KB conda-forge openssl-1.1.1g | h516909a_1 2.1 MB conda-forge python_abi-3.7 | 1_cp37m 4 KB conda-forge scikit-surprise-1.1.1 | py37h03ebfcd_0 591 KB conda-forge ------------------------------------------------------------ Total: 6.2 MB The following NEW packages will be INSTALLED: joblib conda-forge/noarch::joblib-0.16.0-py_0 python_abi conda-forge/linux-64::python_abi-3.7-1_cp37m scikit-surprise conda-forge/linux-64::scikit-surprise-1.1.1-py37h03ebfcd_0 The following packages will be UPDATED: conda pkgs/main::conda-4.8.4-py37_0 --\u0026gt; conda-forge::conda-4.8.5-py37hc8dfbb8_1 openssl pkgs/main::openssl-1.1.1g-h7b6447c_0 --\u0026gt; conda-forge::openssl-1.1.1g-h516909a_1 The following packages will be SUPERSEDED by a higher-priority channel: ca-certificates pkgs/main::ca-certificates-2020.7.22-0 --\u0026gt; conda-forge::ca-certificates-2020.6.20-hecda079_0 certifi pkgs/main::certifi-2020.6.20-py37_0 --\u0026gt; conda-forge::certifi-2020.6.20-py37hc8dfbb8_0 Proceed ([y]/n)? Downloading and Extracting Packages ca-certificates-2020 | 145 KB | ########## | 100% conda-4.8.5 | 3.0 MB | ########## | 100% python_abi-3.7 | 4 KB | ########## | 100% certifi-2020.6.20 | 151 KB | ########## | 100% openssl-1.1.1g | 2.1 MB | ########## | 100% scikit-surprise-1.1. | 591 KB | ########## | 100% joblib-0.16.0 | 203 KB | ########## | 100% Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done Collecting package metadata (current_repodata.json): ...working... done Solving environment: ...working... ## Package Plan ## environment location: /opt/conda added / updated specs: - cpuonly - pytorch - torchvision The following packages will be downloaded: package | build ---------------------------|----------------- cpuonly-1.0 | 0 2 KB pytorch freetype-2.10.2 | h5ab3b9f_0 608 KB jpeg-9b | h024ee3a_2 214 KB lcms2-2.11 | h396b838_0 307 KB libpng-1.6.37 | hbc83047_0 278 KB libtiff-4.1.0 | h2733197_0 447 KB ninja-1.10.1 | py37hfd86e86_0 1.4 MB olefile-0.46 | py37_0 50 KB pillow-7.2.0 | py37hb39fc2d_0 617 KB pytorch-1.6.0 | py3.7_cpu_0 59.4 MB pytorch tk-8.6.10 | hbc83047_0 3.0 MB torchvision-0.7.0 | py37_cpu 10.3 MB pytorch zstd-1.3.7 | h0b5b093_0 401 KB ------------------------------------------------------------ Total: 76.9 MB The following NEW packages will be INSTALLED: cpuonly pytorch/noarch::cpuonly-1.0-0 freetype pkgs/main/linux-64::freetype-2.10.2-h5ab3b9f_0 jpeg pkgs/main/linux-64::jpeg-9b-h024ee3a_2 lcms2 pkgs/main/linux-64::lcms2-2.11-h396b838_0 libpng pkgs/main/linux-64::libpng-1.6.37-hbc83047_0 libtiff pkgs/main/linux-64::libtiff-4.1.0-h2733197_0 ninja pkgs/main/linux-64::ninja-1.10.1-py37hfd86e86_0 olefile pkgs/main/linux-64::olefile-0.46-py37_0 pillow pkgs/main/linux-64::pillow-7.2.0-py37hb39fc2d_0 pytorch pytorch/linux-64::pytorch-1.6.0-py3.7_cpu_0 torchvision pytorch/linux-64::torchvision-0.7.0-py37_cpu zstd pkgs/main/linux-64::zstd-1.3.7-h0b5b093_0 The following packages will be UPDATED: ca-certificates conda-forge::ca-certificates-2020.6.2~ --\u0026gt; pkgs/main::ca-certificates-2020.7.22-0 tk 8.6.8-hbc83047_0 --\u0026gt; 8.6.10-hbc83047_0 The following packages will be SUPERSEDED by a higher-priority channel: certifi conda-forge::certifi-2020.6.20-py37hc~ --\u0026gt; pkgs/main::certifi-2020.6.20-py37_0 Proceed ([y]/n)? Downloading and Extracting Packages ninja-1.10.1 | 1.4 MB | ########## | 100% torchvision-0.7.0 | 10.3 MB | ########## | 100% jpeg-9b | 214 KB | ########## | 100% olefile-0.46 | 50 KB | ########## | 100% pillow-7.2.0 | 617 KB | ########## | 100% pytorch-1.6.0 | 59.4 MB | ########## | 100% lcms2-2.11 | 307 KB | ########## | 100% cpuonly-1.0 | 2 KB | ########## | 100% freetype-2.10.2 | 608 KB | ########## | 100% libtiff-4.1.0 | 447 KB | ########## | 100% zstd-1.3.7 | 401 KB | ########## | 100% libpng-1.6.37 | 278 KB | ########## | 100% tk-8.6.10 | 3.0 MB | ########## | 100% Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done Removing intermediate container 0ce30dc6b5a6 ---\u0026gt; 4fafe65a6cf2 Step 4/8 : USER root ---\u0026gt; Running in ad07655303d4 Removing intermediate container ad07655303d4 ---\u0026gt; 66a0f0bbdffd Step 5/8 : WORKDIR /app ---\u0026gt; Running in 15c4304a5210 Removing intermediate container 15c4304a5210 ---\u0026gt; 9e5ade99225b Step 6/8 : ADD . /app ---\u0026gt; 3af47f01d23f Step 7/8 : EXPOSE 80 ---\u0026gt; Running in a9ecfc2e60e9 Removing intermediate container a9ecfc2e60e9 ---\u0026gt; 967dfa99debb Step 8/8 : CMD [\u0026quot;python\u0026quot;, \u0026quot;recommend.py\u0026quot;] ---\u0026gt; Running in 9d93ff09dfef Removing intermediate container 9d93ff09dfef ---\u0026gt; 0821856015d5 Successfully built 0821856015d5 Successfully tagged prediction-service-k8s:latest  Check that the image is present.\n(datasci-dev) ttmac:docker-prediction-service theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE prediction-service-k8s latest 0821856015d5 43 seconds ago 2.06GB weather-service-k8s/latest latest ae4f44a22535 22 hours ago 496MB debian buster-slim 052664ad4351 7 days ago 69.2MB gcr.io/k8s-minikube/storage-provisioner v3 bad58561c4be 2 weeks ago 29.7MB k8s.gcr.io/kube-proxy v1.19.0 bc9c328f379c 3 weeks ago 118MB k8s.gcr.io/kube-controller-manager v1.19.0 09d665d529d0 3 weeks ago 111MB k8s.gcr.io/kube-apiserver v1.19.0 1b74e93ece2f 3 weeks ago 119MB k8s.gcr.io/kube-scheduler v1.19.0 cbdc8369d8b1 3 weeks ago 45.7MB k8s.gcr.io/etcd 3.4.9-1 d4ca8726196c 2 months ago 253MB kubernetesui/dashboard v2.0.3 503bc4b7440b 2 months ago 225MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 3 months ago 45.2MB kubernetesui/metrics-scraper v1.0.4 86262685d9ab 5 months ago 36.9MB continuumio/miniconda3 latest b4adc22212f1 6 months ago 429MB k8s.gcr.io/pause 3.2 80d28bedfe5d 7 months ago 683kB  Next we will create a yaml file using the --dry-run=client command line argument. This will help us avoid trying to search for the image elsewhere.\nkubectl create deployment recommend-minikube --image=prediction-service-k8s:latest -o yaml --dry-run=client  Save the output of that command into a file called recommend-minikube.yaml. Add the additional specification imagePullPolicy: Never to the containers section. Take note of indentation using spaces/tabs as this can lead to errors.\napiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: recommend-minikube name: recommend-minikube spec: replicas: 1 selector: matchLabels: app: recommend-minikube strategy: {} template: metadata: creationTimestamp: null labels: app: recommend-minikube spec: containers: - image: prediction-service-k8s:latest name: prediction-service-k8s resources: {} imagePullPolicy: Never status: {}  Before the deploy the app, we can look at the cluster status using the following:\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h  Run the following command to deploy the app:\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl apply -f recommend-minikube.yaml deployment.apps/recommend-minikube created  Check the state of the cluster:\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl get pod NAME READY STATUS RESTARTS AGE recommend-minikube-5887d99b57-mqjfx 1/1 Running 0 2s (datasci-dev) ttmac:docker-prediction-service theja$ kubectl get all NAME READY STATUS RESTARTS AGE pod/recommend-minikube-5887d99b57-mqjfx 1/1 Running 0 28s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/recommend-minikube 1/1 1 1 28s NAME DESIRED CURRENT READY AGE replicaset.apps/recommend-minikube-5887d99b57 1 1 1 28s  Expose port 80 through a service.\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl expose deployment recommend-minikube --type=NodePort --port=80 service/recommend-minikube exposed  Get the URL to the app.\n(datasci-dev) ttmac:docker-prediction-service theja$ minikube service recommend-minikube --url http://192.168.64.2:32683  Check the URL in the browser. When no uid is passed, we get a json back with a single key.\n   When we do pass a valid uid, we get the recommendations (pytorch inference happened on the server when the request was made) as shown below.   Next we can teardown the cluster using the following command:\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl delete -f recommend-minikube.yaml deployment.apps \u0026quot;recommend-minikube\u0026quot; deleted  If you run the following command quickly, you can see the container getting terminated.\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl get pod NAME READY STATUS RESTARTS AGE recommend-minikube-5887d99b57-mqjfx 1/1 Terminating 0 9m40s (datasci-dev) ttmac:docker-prediction-service theja$ kubectl get pod No resources found in default namespace.  We also need to delete the NodePort service we had created.\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl delete services recommend-minikube service \u0026quot;recommend-minikube\u0026quot; deleted  Next you can stop and even delete the minikube cluster. Below, we are just stopping it.\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h (datasci-dev) ttmac:docker-prediction-service theja$ minikube stop ✋ Stopping node \u0026quot;minikube\u0026quot; ... 🛑 1 nodes stopped. (datasci-dev) ttmac:docker-prediction-service theja$ kubectl get all The connection to the server localhost:8080 was refused - did you specify the right host or port?  Congrats, you have deployed a container containing your model on a kubernetes cluster.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/gke/",
	"title": "Orchestration using GKE",
	"tags": [],
	"description": "",
	"content": " Note: While exploring GKE, keep a tab on billing (check every so often)!\nIntroduction to Google Kubernetes Engine by GCP  Google Kubernetes Engine (GKE) by GCP a managed service for running K8s, with key features such as security, scaling and multi-cluster support taken care of as part of K8s on their infrastructure.\n GKE\u0026rsquo;s operation is very similar to ECS.\n Our goal will be to use GKE for deploying our recommendation system (the ML model we have been using).\n We will first save our docker image to a Docker registry on GCP (this is called the Container Registry). Next we will use that image while setting up a K8s cluster.   Google Container Registry  We will use the docker login command with the previously created service account with the JSON based credentials we had saved.\n(base) ttmac:~ theja$ cat model-user.json | docker login -u _json_key --password-stdin https://us.gcr.io Login Succeeded  Tag the docker image with the Google container registry specific tag as follows:\n(base) ttmac:~ theja$ docker tag prediction_service us.gcr.io/authentic-realm-276822/prediction_service (base) ttmac:~ theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE prediction_service latest dd408a931e14 7 days ago 2.06GB us.gcr.io/authentic-realm-276822/prediction_service latest dd408a931e14 7 days ago 2.06GB weather_service latest 20d340f941c0 9 days ago 496MB debian buster-slim c7346dd7f20e 6 weeks ago 69.2MB continuumio/miniconda3 latest b4adc22212f1 6 months ago 429MB hello-world latest bf756fb1ae65 8 months ago 13.3kB  Next, we push the local image to GCR. The upload status will keep getting updated.\n(base) ttmac:~ theja$ docker push us.gcr.io/authentic-realm-276822/prediction_service The push refers to repository [us.gcr.io/authentic-realm-276822/prediction_service] d4bf100b2f89: Pushed 6719394c8842: Pushed a432b6ec80f7: Pushing [\u0026gt; ] 20.81MB/1.635GB fcd8d39597dd: Pushing [========\u0026gt; ] 24.11MB/149.1MB 875120aa853c: Pushing [=====\u0026gt; ] 23.17MB/210.4MB f2cb0ecef392: Layer already exists  When its done, you will see the following:\n(base) ttmac:~ theja$ docker push us.gcr.io/authentic-realm-276822/prediction_service The push refers to repository [us.gcr.io/authentic-realm-276822/prediction_service] d4bf100b2f89: Pushed 6719394c8842: Pushed a432b6ec80f7: Pushed fcd8d39597dd: Pushed 875120aa853c: Pushed f2cb0ecef392: Layer already exists latest: digest: sha256:f5b19d0e4510194ab8bdbed22f915fec8a07d1a465725ccfa6196782a480172c size: 1582  We can verify that the prediction_service image is present in the GCR page.\n  Google Kubernetes Engine  We will now set up the K8s cluster. Lets start by accessing the GKE page.   Click on Deply container next to the Create cluster button.   Pick the existing cluster option and choose select.   Choose the recently uploaded prediction_service image.   Hit continue.   On the next page, we will leave everything to default except for the name and then hit Deploy button.   It may take some time for the cluster to get fully set up.   Recall the system level diagram.\nSource: https://www.gstatic.com/pantheon/images/container/kubernetes_cluster.svg \n Once the cluster is set up, we can investigate its properties.\n   Just as we did in the local deployment, we will expose the cluster to be able to trigger prediction requests. We can do that by going to the Workload tab and clicking on the cluster.   We will next click on the expose button to the far right.   We will specify the container port as 80 (if you look at recommend.py we have specified port 80 where the flask app listens to requests).   Once the service is running, we can obtain the external IP.   As expected, if we query without a payload we get a default response.   With an example payload, we are able to retrieve the recommendations from real-time execution of the pytorch model.   To tear down the cluster, we first delete the service.   Finally, we can delete the workload itself.   As a next exercise, try to create an explicit K8s cluster and deploy the prediction model.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture5/",
	"title": "Lecture 5",
	"tags": [],
	"description": "",
	"content": " TBD "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture6/",
	"title": "Lecture 6",
	"tags": [],
	"description": "",
	"content": " TBD "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture7/",
	"title": "Lecture 7",
	"tags": [],
	"description": "",
	"content": " TBD "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture8/",
	"title": "Lecture 8",
	"tags": [],
	"description": "",
	"content": " Online Experimentation - A/B testing: sample size considerations - Tackling bandit feedback "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Deploy model on Heroku.\n Set up your custom domain to point to your VPS.\n Repeat the setup on AWS, GCP, DigitalOcean or any other VPS of your choice.\n Read the documentation for flask, mlflow, pytorch, surprise, pandas.\n Replace Flask with Django and Starlette.\n Read up about function decorators in Python (see here and here for instance). Function decorators add functionality to an existing function, and are an example of metaprogramming.\n Try to set up HTTPS with Lets Encrypt for the flask based model deployed on a single VPS such as EC2.\n Add a WSGI server such as gunicorn. A Web Server Gateway Interface (WSGI) server implements the web server side of the WSGI interface for running Python web applications. See here.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Find out how serverless technologies work behind the scene.\n Connect your custom domain to the GCP Cloudn Function and the API Gateway/Lambda function in AWS.\n Learn command line tools for GCP and the difference between programmatic access and manual access.\n Learn about identities, roles and access aspects in GCP and AWS.\n Try deploying a different recommendation model.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Deploy your image to Docker Hub container registry (needs an account, has free tier limits).\n Run a container using the python images from Docker Hub.\n Try to minimize the size of the docker images produced.\n Add checks for out of bound queries in your recommendation function (e.g., http://localhost/?uid=2000 will give a value error on the server and the browser will show that an internal server error occured).\n Add a load balancer to the ECS deployment and study what it does.\n Replicate the linked tutorial: AWS and Docker\n Use cookie cutter data science repository to learn the best practices while training a machine learning moodel in a container.\n Experiment with a different container technology such as https://linuxcontainers.org/.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Launch a kubernetes cluster with a single pod/container that loads and serves Jupyter notebooks, and which can be accessed via the browser. The images from https://hub.docker.com/u/jupyter/#! such as https://hub.docker.com/r/jupyter/datascience-notebook can help.\n Go through the introductory examples from https://k3s.io and from https://microk8s.io/. Both of these allow you to try Kubernetes locally.\n Try switching to different images such as https://hub.docker.com/_/python/  with minikube.\n Go through the documentation for Kubernetes and Docker.\n Try to access the shell of a deployed container using these instructions.\n Read container logs to debug issues (e.g., python bugs or errors that were not caught) using these instructions.\n Try creating a mini-cloud using VMs and not containers using https://multipass.run/.\n Go through the Redis powered Guestbook app tutorial on GKE.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/",
	"title": "MLOps: Operationalizing Machine Learning",
	"tags": [],
	"description": "",
	"content": " Operationalizing Machine Learning (IDS594) Note: Also known as ML Deployment in the course catalog.\nThis practice-oriented course surveys modern best practices around getting machine learning (ML) models into production. It continues where IDS 572 and IDS 575 left off, which is to learn multiple ways of operationalizing machine learning work flows and models in the context of the larger business end-goals. The course is complementary to IDS 561. We will gain a better understanding of strategies for model management, monitoring and deployment. We will also intertwine these topics with online experimentation techniques (A/B testing) and software engineering ideas such as version control, containerization, and continuous integration/continuous deployment.\nA tentative list of topics is as follows:\n Deploying ML models using web servers (Flask) Containers for machine learning: the Docker ecosystem and Kubernetes Git, CI/CD and their modifications for ML workflows A/B testing of KPIs and data considerations Model management: model tracking and logging  Including case studies such as Databricks\u0026rsquo; MLFlow, Google\u0026rsquo;s TFX/Kubeflow, Uber’s Michelangelo, Facebook\u0026rsquo;s FBLearner Flow.   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]