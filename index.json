[
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/",
	"title": "Course Logistics",
	"tags": [],
	"description": "",
	"content": " Course Logistics  Semester: Fall 2020 Lectures: Thursdays 6.30 PM to 9.00 PM Mode: Online synchronous (i.e., location is online). The course will be delivered over Zoom (an invite will be sent before the first day of class). See the online learning page for basic technology requirements. Staff  Instructor: Dr. Theja Tulabandhula (netid: theja) Teaching Assistant: Tengteng Ma (netid: tma24)  Communication: via slack. Office hours: online via slack and zoom (by appointment).  Textbook and Materials  Data Science in Production by Ben Weber (2020, $5 for the ebook/pdf). A sample of the first three chapters is available at the publishers page linked here.  Software  Any OS should be okay. If in doubt, run a virtual machine running linux (this will be discussed in the class). Some of the software we will work with are:  Docker for Desktop Lightweight Kubernetes Python (Anaconda)  \u0026hellip;  Hardware  There will be varied computing resources needed for this course. Try using a virtual machine with linux on your own computer if possible. A Windows virtual desktop is available at desktop.uic.edu if needed. You can refer to these two help pages to get started.  Assignments  There are no graded assignments or exams for this course. Students are expected to go over the lectures and practice the use of technologies discussed each week.  Project  Students are expected to apply what they learn in the course and demonstrate a deployment of an existing machine learning model they have access to. A suitable documentation of this process along with the scripts/codes/commands used is to be submitted on October 14th (with no exceptions). The evaluation criteria and other details will be released shortly. Submission deadline is BEFORE 11.59 PM on the concerned day. Late submissions will have an automatic 20% penalty per day. Use Blackboard for uploading your work as a single zip file.  Grade  Grades will be assigned based on the project (see project evaluation criteria above) (80%) and course participation (20%).  Miscellaneous Information  This is a 2 credit graduate level course offered by the Information and Decision Sciences department at UIC. See the academic calendar for the semester timeline. Students who wish to observe their religious holidays (http://oae.uic.edu/religious-calendar/) should notify the instructor within one week of the first lecture date. Contact the instructor at the earliest, if you require any accommodations for access to and/or participation in this course. Refer to the academic integrity guidelines set by the university.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/",
	"title": "Lecture 1",
	"tags": [],
	"description": "",
	"content": " Web Servers - SSH and Firewall - Conda Environments - Jupyter - Making requests and processing responses - Model Persistence using MLFlow - Serving a model using Flask "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/online_technology_requirements/",
	"title": "Online Learning Details",
	"tags": [],
	"description": "",
	"content": " Online Learning Details To maximize the learning experience, it will be good if students can meet the following basic technology requirements:\n At a minimum, students should have a device and an internet connection. A microphone, and a webcam would be highly recommended. See the Basic Technology Requirements link for more details.\n Laptop, Chromebook or Desktop Computer: Note that Chromebooks are used to perform a variety of browser-based tasks with most data and applications, such as Blackboard Learn, Blackboard Collaborate, Google Docs, and Office 365, residing in the cloud rather than on the machine itself. This can result in somewhat reduced functionality, depending on your needs. If you do not have reliable access to a computer at home, ACCC may have a laptop to lend to you. Please fill out our request form at accc.uic.edu/forms/laptop-request\n Internet: Many service providers are offering connectivity solutions for students without access to Wi-Fi or the internet. The Illinois Citizens Utility Board is maintaining a comprehensive list of the available options here: citizensutilityboard.org/blog/2020/03/19/cubs-guide-utility-services-during-the-covid-19-public-health-emergency.\n The State of Illinois is maintaining a map of publicly available internet hotspots across the state that can be used for academic-related needs. These hotspots are available from within a parked vehicle. The map, and additional information, can be viewed at www.ildceo.net/wifi.\n Additionally, the ACCC has a very limited supply of cellular hotspots available for those students who are unable to take advantage of the above offers. Please fill out our request form at accc.uic.edu/forms/laptop-request/.\n  Microphone: While this may be built into your computer, we recommend using an external device such as a USB microphone or headset.\n Webcam: A built-in camera may be installed on your laptop; if not, you can use an external USB camera for video conferencing.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/schedule/",
	"title": "Schedule",
	"tags": [],
	"description": "",
	"content": " Textbook  Data Science in Production by Ben Weber (2020, $5 for the ebook/pdf). A sample of the first three chapters is available at the publishers page linked here.  Lecture Schedule Lecture 1: Serving ML Models Using Web Servers  Ref Chapter 2  Lecture 2: Serving ML Models Using Serverless Infrastructure  Ref Chapter 3  Lecture 3: Serving ML Models Using Docker  Ref Chapter 4  Lecture 4: ML Model Pipelines  Ref Chapter 5  Lecture 5:  Ref Chapter 6  Lecture 6:  Ref Chapter 7  Lecture 7:  Ref Chapter 8  Lecture 8: Online Experimentation  Ref 1: https://help.optimizely.com/Get_Started/Get_started_with_Optimizely_Web_Recommendations Ref 2: https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/project_instructions/",
	"title": "Project",
	"tags": [],
	"description": "",
	"content": " Aim  The aim of the project is to simulate the real-world process of deploying machine learning models. More specifically, the project component of this course allows you to explore a technology that assists in model deployment, either directly or indirectly, and asks you to report your experience working with that technology (or multiple technologies) to achieve some overall deployment goal.  Group  You should form groups of 4 students for this project component (this is a strict requirement). Reach out to your classmates early. Because this is a group project, a commensurate effort is expected, and each members contributions needs to be reported in the final submission.  Project Outcomes  There is one due-date for the project deliverables. See the course logistics page for the exact date. The deliverables are as follows. Project Report: In at most 8 pages (12 point, single column; you can have an appendix for supplementary material that may or may not be checked), you should explain your contributions in the project. Code and data: Code associated with the project (e.g., Jupyter notebooks), a small sample of the data/inputs/outputs if needed, and all steps necessary to replicate your project should be provided along with/in the report. A link to your github/gitlab/bitbucket/other repository is acceptable here (provide it at the front page of the report). A video presentation: You should provide a 10 minute video walk-through (discussing highlights) of your project and provide the link (say from Youtube where the video can be in unlisted mode) on the front page of the report.  Each team should upload the report (and code and video link) to Blackboard before the deadline.\nExample Report Components  For example, here are some aspects to focus on in your project report:  what was the goal what were the possible solutions what were the specific pros and cons from a business point of view a cost benefit analysis actual handling of the technology and demonstration in a dev environment documenting the experience lessons learned code artifacts and/or Jupyter notebooks \u0026hellip;  Here is an example project idea: try out a technology (or a specific aspect of it) and its competitors by following their documentation in a very extensive and well thought out manner (e.g., MLFlow vs bentoml vs cortex).  Grading Rubric  Projects will be graded based on the creativity shown in handling the technology and the insights drawn. The reports should be very clearly written and presented, and will be evaluated based on the correctness, content, creativity and clarity:  Correctness will be assessed based on the correct application of a technology, valid software setup and discussion of choices, technical correctness and the assumptions laid out. Content will be assessed based on the contributions made in the project (given group size) and project depth (e.g., why this aspect of ML deployment, why this problem, what did you do, visualization and interesting conclusions, insights, discussion of methodology followed). You should try to demonstrate your understanding of the relevant topics and their use in your non-trivial project. Creativity will be assessed based on how no-obvious your solution or contribution is and how different choices were thoughtfully made in the execution of the project. Clarity will be assessed based on the language quality, layout and structure of the report, the adequacy of the references cited, the capability of the team in explaining ideas in a clear and professional manner, and the clarity demonstrated in your discussions etc.  All external material/sources (code/idea/theory/insights) used should be cited prominently without failure. Use of pre-trained models, databases, web servers, front-end frameworks, visualization tools etc for your project is allowed and encouraged. This project cannot be used as part of any other course or requirement.  Additional Pointers  Keep track of costs especially if you are using services that require having a payment mode on file. Also, try to use free resources as much as possible. Do not train deep networks from scratch if it can be avoided. The project should not be centered around model accuracy. It is importantly to make a project plan that allocates sufficient tasks for each team member. It will be great if you can submit the project plan (a Gantt chart for example).  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/intro/",
	"title": "Basics",
	"tags": [],
	"description": "",
	"content": " Python  We will be predominantly concerned with the Python ecosystem A big advanage is that local system development can be easily moved to cloud and or a scalable on-prem solution. Many companies use python to start data science projects in-house (via fresh recruits, interns etc) Python has some relatively easy ways to access databases Big data platforms such as Spark have great python bindings  E.g., Pandas dataframe and Spark dataframe  Latest models (deep learning, pre-trained) are built in the python ecosystem Many many useful libraries: pandas, matplotlib, flask,\u0026hellip;  Our Objective  Learn the patterns, not the specific tools  Deployment Targets  Local machines On-prem or self-hosted machines (needs DevOps skills) Managed cloud  Heroku (PAAS) Azure GCP AWS (IAAS)  The decision to deply on one versus the other depends on  skills business need internal vs external scale, reliability, security costs ease of deployment   Local Deployments are Hard  Need to learn linux security Need to learn how to manage access Need for learn backups Need to learn hot switching / reliability  Cloud Deployments are not Easy  Also need to learn a complex ecosystem Vendor lock-in (for successful businesses, this is not an issue)  Aside: Software Tools Python development can happen:\n In text editors (e.g., sublime-text) In IDEs (e.g., Pycharm or VSCode) In Jupyter notebooks and variants (Google Colab, Databricks notebooks)  vanilla notebook does not allow collaboration as such   Part 1: Setting up Jupyter access on a VPS  We will use Vultr, but all steps are vendor agnostic. Alternatives include: Digitalocean, AWS EC2, Google Cloud; using Google Colab and other vendors. SSH passwordless access is set up. Next, we set up a basic firewall for security. This is followed by installing conda. (Optional) To run the jupyter server uninterrupted, we will run it within a screen session. We will access the server and notebooks on our local browser using SSH tunneling.  Part 2: Preparing an ML Model  We will show how data is accessed, and how the model is trained (this should be familiar to you).\n In particular, we will look at the moive recommendation problem.  There are aspects of saving and loading models that become important in production. For instance, we would like the models to be able to live across dev/staging/prod environments. For this, we think of the notion of model persistence\n Natively:\n For example, pytorch has native save and load methods. Same is the case for scikit-learn and a variety of other packages.  Using MLFlow:\n MLFlow addresses the problem of moving models across different environments without issues of incompatibility (minor version numbers, OS etc) among other things. See these links for more information: https://pypi.org/project/mlflow/ and mlflow.org    "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/ssh_and_firewall/",
	"title": "SSH and Firewall",
	"tags": [],
	"description": "",
	"content": " It is important to secure your connection to the machine. In order to do so, we will configure the ssh access pattern as well as set up a firewall that blocks all incoming requests except ssh port and web server ports.\nWe will assume that we have a non-root account that is in the sudoers group.\nSSH  When you first create the server instance, you may or may not have the ssh server running. If it is not running, you can install it first. On Ubuntu/Debian, you can use the following command:\nsudo apt install openssh-server  Next, we create a ssh key pair on our local machine with which we will access the server. From your local user home directory:\nmkdir .ssh ssh-keygen cd .ssh less id_rsa.pub  Copy this content to the following file authorized_keys in the webserver:\nmkdir .ssh vim authorized_keys #if vim is not present, you can use other editors or install it using `sudo apt install vim` #copy the content and quit (shift+colon\u0026gt; wq -\u0026gt; enter) chmod 600 authorized_keys  We need to edit the following fields in the file /etc/ssh/sshd_config on the server (say using vim):\n Port choose something other than 22 (opttional) PermitRootLogin no (changed from prohibit-password) PubkeyAuthentication yes (already defaults to this) PasswordAuthentication no (disable it for security)  Restart the ssh server. In Ubuntu/Debian this is achieved by sudo systemctl restart ssh\n  Firewall  A basic firewall such as ufw can help provide a layer of security. Install and run it using the following commands (Ubuntu/Debian):\nsudo apt install ufw sudo ufw allow [PortNumber] #here it is 22 or another port that you chose for ssh sudo ufw enable sudo ufw status verbose #this should show what the firewall is doing   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/conda/",
	"title": "Setting up Python",
	"tags": [],
	"description": "",
	"content": " Here are a few notes on installing a user specific python distribution:\nGet Miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh conda install pip #better to use the pip in the base conda env than system pip   The difference between conda and pip: pip is a package manager specifically for python, whereas conda is a package manager for multiple languages as well as is an environment manager. Python module venv is python specific environment manager.  Set up a conda environment and activate it conda create --name datasci-env python #or conda create -n dataeng-env python jupyter pandas numpy matplotlib #or conda create -n datasci-env scipy=0.15.0 #or conda env create -f environment.yml conda activate datasci-env   You don\u0026rsquo;t have to give names, can give prefixes where the env is saved, can create based on specific pages, can use explicit previous conda environments, yaml files, clone/update an existing one, etc. Use this link to get more information.\n Specifying a path to a subdirectory of your project directory when creating an environment can keep everything 100% self contained.\n To deactivate this environment, use conda deactivate datasci-env.\n  Install jupyter and pytorch (and tensorflow, keras, scikit-learn similarly) in a specific environment conda install jupyter conda install pytorch torchvision cpuonly -c pytorch # https://pytorch.org/   Change the command for pytorch installation if you do intend to use GPUs. In particular, install CUDA from conda after installing the latest NVidia drivers on the instance.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/jupyter/",
	"title": "Remote Jupyter Server",
	"tags": [],
	"description": "",
	"content": "The following sets a simple password based login, which is handy:\njupyter notebook --generate-config jupyter notebook password  Unfortuantely, hashed password is sent unencrypted by your browser here. So read up here to do this in a better way.\nStarting jupyter on the server can be done inside a screen session:\nscreen -S jupyter-session #can also use nohup or tmux here jupyter notebook --no-browser --port=8888  SSH tunnel can be setup by running the following on your local machine, and then opening the browser (http://localhost:8889)\nssh -N -f -L localhost:8889:localhost:8888 -p 22 theja@192.168.0.105  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/",
	"title": "Recommendation Models",
	"tags": [],
	"description": "",
	"content": "We will look at two models for recommending movies to existing users.\n Matrix factorization based on the surprise package. Matrix factorization based on Pytorch.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/surprise_training/",
	"title": "Recommendation (SVD) Training",
	"tags": [],
	"description": "",
	"content": "# https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.accuracy import rmse from surprise.dump import dump  # Load the movielens-100k dataset (download it if needed). data = Dataset.load_builtin('ml-100k') trainset = data.build_full_trainset() # Use an example algorithm: SVD. algo = SVD() algo.fit(trainset) # predict ratings for all pairs (u, i) that are in the training set. testset = trainset.build_testset() predictions = algo.test(testset) rmse(predictions) #actual predictions as thse items have not been seen by the users. there is no ground truth. # We predict ratings for all pairs (u, i) that are NOT in the training set. testset = trainset.build_anti_testset() predictions = algo.test(testset)  RMSE: 0.6774  dump('./surprise_model', predictions, algo)  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/deploy_webserver/",
	"title": "Serving ML Models Using Web Servers",
	"tags": [],
	"description": "",
	"content": " Model Serving  Sharing results with others (humans, web services, applications) Batch approach: dump predictions to a database (quite popular) Real-time approach: send a test feature vector, get back the prediction instantly and the computation happens now  How to consume from prediction services?  Using web requests (e.g., using a JSON payload)  How to output predictions?  We will plan to set up a server to serve predictions  It will respond to web requests (GET, POST) We pass some inputs (image, text, vector of numbers), and get some outputs (just like a function) The environment from which we pass inputs may be very different from the environment where the prediction happens (e.g., different hardware)   Our Objective  Use sklearn/keras with flask, gunicorn and heroku to set up a prediction server  Part 1: Making API Calls  Using the requests module from a jupyter notebook (this is an example of a programmatic way) Alternatively, using curl or postman (these are more versatile)  Part 2: Simple Flask App  Function decorators are used in Flask to achive routes to functions mapping. Integrating the model with the app is relatively easy if the model can be read from disk.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/surprise_inference/",
	"title": "Recommendation (SVD) Inference",
	"tags": [],
	"description": "",
	"content": "# https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.dump import load from collections import defaultdict import pandas as pd def get_top_n(predictions, n=10): \u0026quot;\u0026quot;\u0026quot;Return the top-N recommendation for each user from a set of predictions. Args: predictions(list of Prediction objects): The list of predictions, as returned by the test method of an algorithm. n(int): The number of recommendation to output for each user. Default is 10. Returns: A dict where keys are user (raw) ids and values are lists of tuples: [(raw item id, rating estimation), ...] of size n. \u0026quot;\u0026quot;\u0026quot; # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n  df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('./surprise_model')  Output:\n 0 1 2 0 1 Toy Story (1995) Animation|Children's|Comedy 1 2 Jumanji (1995) Adventure|Children's|Fantasy 2 3 Grumpier Old Men (1995) Comedy|Romance 3 4 Waiting to Exhale (1995) Comedy|Drama 4 5 Father of the Bride Part II (1995) Comedy algo: SVD  top_n = get_top_n(predictions, n=5) # Print the recommended items for each user limit = 0 for uid, user_ratings in top_n.items(): print('\\nUser:',uid) seen = [df.loc[int(iid),'name'] for (iid, _) in algo.trainset.ur[int(uid)]] if len(seen) \u0026gt; 10: seen = seen[:10] print('\\tSeen:',seen) print('\\tRecommendations:',[df.loc[int(iid),'name'] for (iid, _) in user_ratings]) limit+=1 if limit\u0026gt;3: break  Output:\nUser: 196 Seen: ['Richie Rich (1994)', 'Getaway, The (1994)', 'Batman Forever (1995)', 'Feast of July (1995)', 'Heidi Fleiss: Hollywood Madam (1995)', 'Shadows (Cienie) (1988)', 'Terminator 2: Judgment Day (1991)', \u0026quot;Nobody's Fool (1994)\u0026quot;, \u0026quot;Breakfast at Tiffany's (1961)\u0026quot;, 'Basic Instinct (1992)'] Recommendations: ['Age of Innocence, The (1993)', 'Bio-Dome (1996)', 'Strawberry and Chocolate (Fresa y chocolate) (1993)', 'Guardian Angel (1994)', \u0026quot;Carlito's Way (1993)\u0026quot;] User: 186 Seen: ['Double Happiness (1994)', 'Mr. Jones (1993)', 'War Room, The (1993)', 'Bloodsport 2 (1995)', 'Usual Suspects, The (1995)', 'Big Green, The (1995)', 'Mighty Morphin Power Rangers: The Movie (1995)', 'Boys on the Side (1995)', 'Cold Fever (� k�ldum klaka) (1994)', 'Sum of Us, The (1994)'] Recommendations: ['Lightning Jack (1994)', 'Robocop 3 (1993)', 'Walk in the Clouds, A (1995)', 'Living in Oblivion (1995)', 'Strawberry and Chocolate (Fresa y chocolate) (1993)'] User: 22 Seen: ['Assassins (1995)', 'Nico Icon (1995)', 'From the Journals of Jean Seberg (1995)', 'Last Summer in the Hamptons (1995)', 'Down Periscope (1996)', 'Bushwhacked (1995)', 'Beyond Bedlam (1993)', 'Client, The (1994)', 'Hoop Dreams (1994)', 'Ladybird Ladybird (1994)'] Recommendations: ['Home for the Holidays (1995)', 'Age of Innocence, The (1993)', 'Balto (1995)', 'City Hall (1996)', 'Ready to Wear (Pret-A-Porter) (1994)'] User: 244 Seen: ['When Night Is Falling (1995)', 'Birdcage, The (1996)', '8 Seconds (1994)', 'Foreign Student (1994)', 'Mighty Aphrodite (1995)', 'Before Sunrise (1995)', 'Lion King, The (1994)', 'Clockers (1995)', 'Underneath, The (1995)', 'Manny \u0026amp; Lo (1996)'] Recommendations: ['Century (1993)', 'Balto (1995)', 'Age of Innocence, The (1993)', 'Remains of the Day, The (1993)', 'Jimmy Hollywood (1994)']  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/flask/",
	"title": "Flask App",
	"tags": [],
	"description": "",
	"content": " Flask is a micro web framework written in Python. We first show how a simple service works, and then show how to load a model (e.g., based on pytorch) and serve it as well.\nWeather Reporting Service The key thing to see here are that the HTTP route / is mapped directly to a function weather. For instance, when someone hits localhost:5000 (5000 is the default unless specified in app.run() below) the function weather starts execution based on any received inputs.\n# load Flask import flask from flask import jsonify from geopy.geocoders import Nominatim import requests app = flask.Flask(__name__) # define a predict function as an endpoint @app.route(\u0026quot;/\u0026quot;, methods=[\u0026quot;GET\u0026quot;,\u0026quot;POST\u0026quot;]) def weather(): data = {\u0026quot;success\u0026quot;: False} #https://pypi.org/project/geopy/ geolocator = Nominatim(user_agent=\u0026quot;cloud_function_weather_app\u0026quot;) params = flask.request.json if params is None: params = flask.request.args # params = request.get_json() if \u0026quot;msg\u0026quot; in params: location = geolocator.geocode(str(params['msg'])) # https://www.weather.gov/documentation/services-web-api result1 = requests.get(f\u0026quot;https://api.weather.gov/points/{location.latitude},{location.longitude}\u0026quot;) result2 = requests.get(f\u0026quot;{result1.json()['properties']['forecast']}\u0026quot;) data[\u0026quot;response\u0026quot;] = result2.json() data[\u0026quot;success\u0026quot;] = True return jsonify(data) # start the flask app, allow remote connections if __name__ == '__main__': app.run(host='0.0.0.0')  This service can be run by using the command python weather.py (assuming that is the filename for the above script) locally. If the port 5000 is open, then this server will be accessible to the world if the server has an external IP address.\nModel Serving We can modify the above to serve the recommendation models we built earlier as follows:\nfrom surprise import Dataset from surprise.dump import load from collections import defaultdict import pandas as pd import flask def get_top_n(predictions, n=10): # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('./surprise_model') top_n = get_top_n(predictions, n=5) app = flask.Flask(__name__) # define a predict function as an endpoint @app.route(\u0026quot;/\u0026quot;, methods=[\u0026quot;GET\u0026quot;]) def predict(): data = {\u0026quot;success\u0026quot;: False} # check for passed in parameters params = flask.request.json if params is None: params = flask.request.args if \u0026quot;uid\u0026quot; in params.keys(): data[\u0026quot;response\u0026quot;] = str([df.loc[int(iid),'name'] for (iid, _) in top_n[params.get(\u0026quot;uid\u0026quot;)]]) data[\u0026quot;success\u0026quot;] = True # return a response in json format return flask.jsonify(data) # start the flask app, allow remote connections app.run(host='0.0.0.0')  You can use the following request in the browser http://0.0.0.0:5000/?uid=196 or use the requests module.\n"
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/",
	"title": "Lecture 2",
	"tags": [],
	"description": "",
	"content": " Serverless Deployments - Managed Solutions - Cloud Functions (GCP) - Lambda Functions (AWS) "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/serverless/",
	"title": "Serverless Deployments",
	"tags": [],
	"description": "",
	"content": " A. TLDR  Models do not need to be complex, but it can be complex to deploy models. - Ben Weber (2020)\n Problem  We have to take care of provisioning and server maintenance while deploying our models. We have to worry about scale: would 1 server be enough? How to minimize the time to deploy (at an acceptable increase in cost)? How can a single developer or data science/analytics professional manage a complex service?  Solution  Software that abstracts away server details and lets you serve your model (any function actually) with few lines of code/UI.\n The software automates\n prrovising servers scaling machines up and down  load balancing\n code versioning\n Our task is then to specify the requirements (pandas, pytorch).\n   B. Our Objective  Write serverless functions that generate predictions when they get triggered by HTTP requests. We will work with:  AWS GCP  We will deploy  a keras model, and a sklearn model.   C. Managed Services  Cloud is responsible for abstracting away various computing components: compute, storage, networking etc Minimizes thinking about dev/staging vs production. Note: what we did last class, ssh\u0026rsquo;ing into a virtual private server (VPS) would be considered as a hosted deployment, which is the opposite of managed deployment For example, serverless technology was introduced in 2015\u0026frasl;2016 by AWS (Amazon web) and GCP (Google cloud). It contrasts with VPS based deployment. Similarly, AWS ECS (Elastic Container Service) managed solution contrasts with the hosted/manual Docker on VPS setup.  When is a managed solution a bad idea?  No need for quick iteration (company cares about processes and protocols) Need a high speed service No need to scale system arbirarily Cost conscious or have an in-house developer.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions/",
	"title": "Cloud Functions",
	"tags": [],
	"description": "",
	"content": " Intro  Cloud Functions (CFs) are a solution from GCP for serverless deployments. Very little boilerplate beyond what we will write for simple offline model inference. In any such deployment, we need to be concerned about:  where the model is stored (recall pickle and mlflow), and what python packages are available.   Empty Deployment  We will set up triggers that will trigger our serving function (in particular, a HTTP request). We will specify the requirements needed for our python function to work The function we deploy here, similar to lecture 1, produces weather forecasts given a location.  Setting up using UI  Sign up with GCP if you haven\u0026rsquo;t already (typically you get a 300$ credit)\n Get to the console and find the Cloud Function page.\n   Go through the UI for creating a function.   We will choose the HTTP trigger and unauthenticated access option.   We may have to enable Cloud Build API   Finally, we choose the Python environment. You can see two default example files (main.py and requirements.txt). We will be modifying these two.  Python Files and Requirements  We will specify the following requirements:\nflask geopy requests  Our main file is the following:\ndef weather(request): from flask import jsonify from geopy.geocoders import Nominatim import requests data = {\u0026quot;success\u0026quot;: False} #https://pypi.org/project/geopy/ geolocator = Nominatim(user_agent=\u0026quot;cloud_function_weather_app\u0026quot;) params = request.get_json() if \u0026quot;msg\u0026quot; in params: location = geolocator.geocode(str(params['msg'])) # https://www.weather.gov/documentation/services-web-api # Example query: https://api.weather.gov/points/39.7456,-97.0892 result1 = requests.get(f\u0026quot;https://api.weather.gov/points/{location.latitude},{location.longitude}\u0026quot;) # Example query: https://api.weather.gov/gridpoints/TOP/31,80 result2 = requests.get(f\u0026quot;{result1.json()['properties']['forecast']}\u0026quot;) data[\u0026quot;response\u0026quot;] = result2.json() data[\u0026quot;success\u0026quot;] = True return jsonify(data)  Once the function is deployed, we can test the function (click actions on the far right in the dashboard)\n   We can pass the JSON string {\u0026quot;msg\u0026quot;:\u0026quot;Chicago\u0026quot;} and see that we indeed get the JSON output for the weather of Chicago.   We can also access the function from the web endpoint https://us-central1-authentic-realm-276822.cloudfunctions.net/function-1 (you will have a different endpoint). Note that unlike previous times, the request to this endpoint is a JSON payload.\n Below is the screen-shot of querying the weather of Chicago using the Postman tool. The way to use it is as follows:\n Insert the URL of the API Se the method type to POST Navigate to body and choose raw and then choose JSON from the dropdown menu. Now add the relevant parameters as a JSON string.    Finally, here is a query you can use from a Jupyter notebook.\nimport requests result = requests.post( \u0026quot;https://us-central1-authentic-realm-276822.cloudfunctions.net/function-1\u0026quot; ,json = { 'msg': 'Chicago' }) print(result.json()) #should match with https://forecast.weather.gov/MapClick.php?textField1=41.98\u0026amp;textField2=-87.9   Saving Model on the Cloud  For our original task of deploying a trained ML model, we need a way to read it from somewhere when the function is triggered.\n One way is to dump the model onto Google Cloud Storage (GCS)\n GCS is similar to the S3 (simple storage service) by AWS.\n We will use the command line to dump our model onto the cloud.\n  GCP access via the commandline  First we need to install the Google Cloud SDK from https://cloud.google.com/sdk/docs/downloads-interactive\ncurl https://sdk.cloud.google.com | bash gcloud init  There are two types of accounts you can work with: a user account or a service account (see https://cloud.google.com/sdk/docs/authorizing?authuser=2).\n Among others, [this page] gives a brief idea of why such an account is needed. In particular, we will create a service account (so that it can be used by an application programmatically anywhere) and store the encrypted credentials on disk for programmatic access through python. To do so, we run the following commands:\n We create the service account and check that it is active by using this command: gcloud iam service-accounts list.\ngcloud iam service-accounts create idsservice \\ --description=\u0026quot;IDS service account\u0026quot; \\ --display-name=\u0026quot;idsservice-displayed\u0026quot;  We then assign a new role for this service account in the project. The account can be disabled using the command gcloud iam service-accounts disable idsservice@authentic-realm-276822.iam.gserviceaccount.com (change idsservice and authentic-realm-276822 to your specific names).\ngcloud projects add-iam-policy-binding authentic-realm-276822 --member=serviceAccount:idsservice@authentic-realm-276822.iam.gserviceaccount.com --role=roles/owner  Finally, we can download the credentials\ngcloud iam service-accounts keys create ~/idsservice.json \\ --iam-account idsservice@authentic-realm-276822.iam.gserviceaccount.com  Once the credentials are downloaded, they can be programmatically accessed using python running on that machine. We just have to explore the location of the file:\nexport GOOGLE_APPLICATION_CREDENTIALS=/Users/theja/idsservice.json   Next we will install a python module to access GCS, so that we can write our model to the cloud:\npip install google-cloud-storage  The following code creates a bucket called theja_model_store\nfrom google.cloud import storage bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() storage_client.create_bucket(bucket_name) for bucket in storage_client.list_buckets(): print(bucket.name)  We can dump the model we used previous here using the following snippet\nfrom google.cloud import storage bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1a\u0026quot;) blob.upload_from_filename(\u0026quot;surprise_model\u0026quot;) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1b\u0026quot;) blob.upload_from_filename(\u0026quot;movies.dat\u0026quot;)   After running the above, the surprise package based recommendation model and the helper data file will be available at gs://theja_model_store/serverless/surprise_model/v1a and gs://theja_model_store/serverless/surprise_model/v1b as seen below.\nWe can either use the URIs above or use a programmatic way with the storage class. For example, here is the way to download the file v1b:\nfrom google.cloud import storage bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1b\u0026quot;) blob.download_to_filename(\u0026quot;movies.dat.from_gcp\u0026quot;)  We can diff it in Jupyter notebook itself using the expression !diff movies.dat movies.dat.from_gcp.\nWe will use this programmatic way of reading external data/model in the cloud function next.\n"
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions_model/",
	"title": "GCP Serverless Model Serving",
	"tags": [],
	"description": "",
	"content": " We modify the flask app that we had before, by again specifying the requirements.txt and the main python file appropriately. We will also increase the memory to 2GB and the timeout to 180 seconds. You will see that the following deployment has a lot of inefficiencies (can you spot the redundacy in loading the model and the predictions below?).\nThe requirements file will have the following entries:\nnumpy flask pandas google-cloud-storage scikit-surprise pickle5  The main file is also modified accordingly. Note that if we reload the model and the metadata on every request, it will be extremely inefficient. To fix that we can use global variables. This is not a good choice in much of python programming, but quite useful here. Essentially, global variables allow us to cache some of the objects, for faster response times.\ntop_n = None def recommend(request): global top_n from surprise import Dataset import pandas as pd import flask from google.cloud import storage import pickle5 def load(file_name): dump_obj = pickle5.load(open(file_name, 'rb')) return dump_obj['predictions'], dump_obj['algo'] def get_top_n(predictions, n=10): def defaultdict(default_type): class DefaultDict(dict): def __getitem__(self, key): if key not in self: dict.__setitem__(self, key, default_type()) return dict.__getitem__(self, key) return DefaultDict() # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n data = {\u0026quot;success\u0026quot;: False} params = request.get_json() if \u0026quot;uid\u0026quot; in params: if not top_n: bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1a\u0026quot;) blob.download_to_filename(\u0026quot;/tmp/surprise_model\u0026quot;) #ideally we should be reading things into memory blob = bucket.blob(\u0026quot;serverless/surprise_model/v1b\u0026quot;) blob.download_to_filename(\u0026quot;/tmp/movies.dat\u0026quot;) df = pd.read_csv('/tmp/movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('/tmp/surprise_model') top_n = get_top_n(predictions, n=5) data[\u0026quot;response\u0026quot;] = str([df.loc[int(iid),'name'] for (iid, _) in top_n[params.get(\u0026quot;uid\u0026quot;)]]) data[\u0026quot;success\u0026quot;] = True # return a response in json format return flask.jsonify(data)  We can test the function on the GCP console with a request JSON {\u0026quot;uid\u0026quot;:\u0026quot;206\u0026quot;}.\nAs an exercise, think of ways to make the whole setup above lightweight in terms of model and data size.\nDeploying the pytorch model after removing the dependecy on surprise is also a good challenge to tackle.\nAccess Management  It is a bad idea to allow unauthenticated access There are ways of restricting who can reach this model serving endpoint.  One way is to disable unauthenticated access while creating the function. Once we do that, we can create a permission structure based on GCP best practices.  We will omit the details here.  Updating Models and Monitoring  We can easily update our endpoint by rewriting the files we uploaded to GCS. A cleaner way is to create another cloud function. The function itself can have logic to reload files using the google-cloud-storage module based on time elapsed (say using datetime). Monitoring is very important, not just to know if our model performance has changed over time, but also to measure things such as latency, bugs and API access patterns. We will revisit this topic in the future.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions/",
	"title": "Lambda Functions",
	"tags": [],
	"description": "",
	"content": "  Lambda by Amazon Web Services (AWS) is an analogous serverless solution. Lambda can be used internall as well as for model deployments (we are focusing on the latter). We will repeat setting up the weather app and the recommender model, using the CLI (command line interface tools)  Aside: Setting up an IAM user  TBD  Hello World in Lambda  Select the lambda service.   Pick the python 3.7 runtime.   You will see the green bar indicating a successful creation.   We will initially not changed the code.   And create a dummy test.   The result looks like this.  Setting up an API Gateway  We will slightly change the function so that it forms a proper HTTP response.   Under the Designer section, we will \u0026lsquo;Add Trigger\u0026rsquo; and select \u0026lsquo;API Gateway\u0026rsquo;.   We choose HTTP API and security to be open for the time being.   A block will appear on the designer tab in the Lambda function page.   Once you click on the API Gateway link provided, you will see the response as expected:  See this documentation for more details.\n"
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions_model/",
	"title": "AWS Serverless Model Serving",
	"tags": [],
	"description": "",
	"content": " Storing the Model on S3 (Simple Storage Service)  S3 is a very popular service used by many large and small companies and individuals. For example, a typical data science work flow may involve ETL work on databases (such as Amazon Redshift) and then storing outputs on S3. To set up S3 for model serving, we have to perform a number of steps. We start with the s3 page.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/pytorch_training/",
	"title": "Recommendation (Pytorch) Training",
	"tags": [],
	"description": "",
	"content": "Please install the package using the command conda install -c conda-forge scikit-surprise in the ight environment.\n# https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.accuracy import rmse from surprise.dump import dump import numpy as np import torch from torch import nn import torch.nn.functional as F from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator from ignite.metrics import Loss, MeanSquaredError from datetime import datetime from sklearn.utils import shuffle class Loader(): current = 0 def __init__(self, x, y, batchsize=1024, do_shuffle=True): self.shuffle = shuffle self.x = x self.y = y self.batchsize = batchsize self.batches = range(0, len(self.y), batchsize) if do_shuffle: # Every epoch re-shuffle the dataset self.x, self.y = shuffle(self.x, self.y) def __iter__(self): # Reset \u0026amp; return a new iterator self.x, self.y = shuffle(self.x, self.y, random_state=0) self.current = 0 return self def __len__(self): # Return the number of batches return int(len(self.x) / self.batchsize) def __next__(self): n = self.batchsize if self.current + n \u0026gt;= len(self.y): raise StopIteration i = self.current xs = torch.from_numpy(self.x[i:i + n]) ys = torch.from_numpy(self.y[i:i + n]) self.current += n return (xs, ys) def l2_regularize(array): loss = torch.sum(array ** 2.0) return loss class MF(nn.Module): itr = 0 def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0): super(MF, self).__init__() self.k = k self.n_user = n_user self.n_item = n_item self.c_bias = c_bias self.c_vector = c_vector self.user = nn.Embedding(n_user, k) self.item = nn.Embedding(n_item, k) # We've added new terms here: self.bias_user = nn.Embedding(n_user, 1) self.bias_item = nn.Embedding(n_item, 1) self.bias = nn.Parameter(torch.ones(1)) def __call__(self, train_x): user_id = train_x[:, 0] item_id = train_x[:, 1] vector_user = self.user(user_id) vector_item = self.item(item_id) # Pull out biases bias_user = self.bias_user(user_id).squeeze() bias_item = self.bias_item(item_id).squeeze() biases = (self.bias + bias_user + bias_item) ui_interaction = torch.sum(vector_user * vector_item, dim=1) # Add bias prediction to the interaction prediction prediction = ui_interaction + biases return prediction def loss(self, prediction, target): loss_mse = F.mse_loss(prediction, target.squeeze()) # Add new regularization to the biases prior_bias_user = l2_regularize(self.bias_user.weight) * self.c_bias prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias prior_user = l2_regularize(self.user.weight) * self.c_vector prior_item = l2_regularize(self.item.weight) * self.c_vector total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item return total def log_training_loss(engine, log_interval=400): epoch = engine.state.epoch itr = engine.state.iteration fmt = \u0026quot;Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\u0026quot; msg = fmt.format(epoch, itr, len(train_loader), engine.state.output) model.itr = itr if itr % log_interval == 0: print(msg) def log_validation_results(engine): evaluat.run(test_loader) metrics = evaluat.state.metrics avg_accuracy = metrics['accuracy'] print(\u0026quot;Epoch[{}] Validation MSE: {:.2f} \u0026quot; .format(engine.state.epoch, avg_accuracy))  #Data data = Dataset.load_builtin('ml-100k') trainset = data.build_full_trainset() uir = np.array([x for x in trainset.all_ratings()]) train_x = test_x = uir[:,:2].astype(np.int64) train_y = test_y = uir[:,2].astype(np.float32)  #Parameters lr = 1e-2 k = 10 #latent dimension c_bias = 1e-6 c_vector = 1e-6 batchsize = 1024 model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector) optimizer = torch.optim.Adam(model.parameters()) trainer = create_supervised_trainer(model, optimizer, model.loss) metrics = {'accuracy': MeanSquaredError()} evaluat = create_supervised_evaluator(model, metrics=metrics) train_loader = Loader(train_x, train_y, batchsize=batchsize) test_loader = Loader(test_x, test_y, batchsize=batchsize) trainer.add_event_handler(event_name=Events.ITERATION_COMPLETED, handler=log_training_loss) trainer.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=log_validation_results)  \u0026lt;ignite.engine.events.RemovableEventHandle at 0x7f011e8a67c0\u0026gt;  model  MF( (user): Embedding(943, 10) (item): Embedding(1682, 10) (bias_user): Embedding(943, 1) (bias_item): Embedding(1682, 1) )  trainer.run(train_loader, max_epochs=50)  Epoch[1] Validation MSE: 17.82 Epoch[2] Validation MSE: 16.02 Epoch[3] Validation MSE: 14.43 Epoch[4] Validation MSE: 13.04 Epoch[5] Iteration[400/97] Loss: 12.52 Epoch[5] Validation MSE: 11.79 Epoch[6] Validation MSE: 10.70 Epoch[7] Validation MSE: 9.71 Epoch[8] Validation MSE: 8.85 Epoch[9] Iteration[800/97] Loss: 8.95 Epoch[9] Validation MSE: 8.08 Epoch[10] Validation MSE: 7.40 Epoch[11] Validation MSE: 6.80 Epoch[12] Validation MSE: 6.27 Epoch[13] Iteration[1200/97] Loss: 6.50 Epoch[13] Validation MSE: 5.79 Epoch[14] Validation MSE: 5.36 Epoch[15] Validation MSE: 4.98 Epoch[16] Validation MSE: 4.63 Epoch[17] Iteration[1600/97] Loss: 4.80 Epoch[17] Validation MSE: 4.32 Epoch[18] Validation MSE: 4.04 Epoch[19] Validation MSE: 3.79 Epoch[20] Validation MSE: 3.56 Epoch[21] Iteration[2000/97] Loss: 3.35 Epoch[21] Validation MSE: 3.35 Epoch[22] Validation MSE: 3.15 Epoch[23] Validation MSE: 2.97 Epoch[24] Validation MSE: 2.81 Epoch[25] Iteration[2400/97] Loss: 2.73 Epoch[25] Validation MSE: 2.66 Epoch[26] Validation MSE: 2.53 Epoch[27] Validation MSE: 2.40 Epoch[28] Validation MSE: 2.28 Epoch[29] Iteration[2800/97] Loss: 2.31 Epoch[29] Validation MSE: 2.17 Epoch[30] Validation MSE: 2.07 Epoch[31] Validation MSE: 1.97 Epoch[32] Validation MSE: 1.89 Epoch[33] Iteration[3200/97] Loss: 1.82 Epoch[33] Validation MSE: 1.81 Epoch[34] Validation MSE: 1.73 Epoch[35] Validation MSE: 1.66 Epoch[36] Validation MSE: 1.60 Epoch[37] Validation MSE: 1.54 Epoch[38] Iteration[3600/97] Loss: 1.60 Epoch[38] Validation MSE: 1.48 Epoch[39] Validation MSE: 1.43 Epoch[40] Validation MSE: 1.38 Epoch[41] Validation MSE: 1.34 Epoch[42] Iteration[4000/97] Loss: 1.27 Epoch[42] Validation MSE: 1.29 Epoch[43] Validation MSE: 1.25 Epoch[44] Validation MSE: 1.22 Epoch[45] Validation MSE: 1.19 Epoch[46] Iteration[4400/97] Loss: 1.11 Epoch[46] Validation MSE: 1.15 Epoch[47] Validation MSE: 1.13 Epoch[48] Validation MSE: 1.10 Epoch[49] Validation MSE: 1.07 Epoch[50] Iteration[4800/97] Loss: 1.11 Epoch[50] Validation MSE: 1.05 State: iteration: 4850 epoch: 50 epoch_length: 97 max_epochs: 50 output: 1.0818936824798584 batch: \u0026lt;class 'tuple'\u0026gt; metrics: \u0026lt;class 'dict'\u0026gt; dataloader: \u0026lt;class '__main__.Loader'\u0026gt; seed: \u0026lt;class 'NoneType'\u0026gt; times: \u0026lt;class 'dict'\u0026gt;  torch.save(model.state_dict(), \u0026quot;./pytorch_model\u0026quot;)   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/pytorch_inference/",
	"title": "Recommendation (Pytorch) Inference",
	"tags": [],
	"description": "",
	"content": "from surprise import Dataset import numpy as np import torch from torch import nn import pandas as pd class MF(nn.Module): itr = 0 def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0): super(MF, self).__init__() self.k = k self.n_user = n_user self.n_item = n_item self.c_bias = c_bias self.c_vector = c_vector self.user = nn.Embedding(n_user, k) self.item = nn.Embedding(n_item, k) # We've added new terms here: self.bias_user = nn.Embedding(n_user, 1) self.bias_item = nn.Embedding(n_item, 1) self.bias = nn.Parameter(torch.ones(1)) def __call__(self, train_x): user_id = train_x[:, 0] item_id = train_x[:, 1] vector_user = self.user(user_id) vector_item = self.item(item_id) # Pull out biases bias_user = self.bias_user(user_id).squeeze() bias_item = self.bias_item(item_id).squeeze() biases = (self.bias + bias_user + bias_item) ui_interaction = torch.sum(vector_user * vector_item, dim=1) # Add bias prediction to the interaction prediction prediction = ui_interaction + biases return prediction def loss(self, prediction, target): loss_mse = F.mse_loss(prediction, target.squeeze()) # Add new regularization to the biases prior_bias_user = l2_regularize(self.bias_user.weight) * self.c_bias prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias prior_user = l2_regularize(self.user.weight) * self.c_vector prior_item = l2_regularize(self.item.weight) * self.c_vector total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item return total def get_top_n(model,testset,trainset,uid0,n=10): uid0 = trainset.to_inner_uid(uid0) # First map the predictions to each user. preds = [] for uid, iid, _ in testset: uid = trainset.to_inner_uid(uid) if uid==uid0: preds.append((iid,float(model(torch.tensor([[int(uid),int(trainset.to_inner_iid(iid))]]))))) # Then sort the predictions for each user and retrieve the k highest ones. preds.sort(key=lambda x: x[1], reverse=True) return preds[:n]  #Data df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) print(df.head()) data = Dataset.load_builtin('ml-100k') trainset = data.build_full_trainset() testset = trainset.build_anti_testset()   name genre iid 1 Toy Story (1995) Animation|Children's|Comedy 2 Jumanji (1995) Adventure|Children's|Fantasy 3 Grumpier Old Men (1995) Comedy|Romance 4 Waiting to Exhale (1995) Comedy|Drama 5 Father of the Bride Part II (1995) Comedy  #Parameters lr = 1e-2 k = 10 #latent dimension c_bias = 1e-6 c_vector = 1e-6 model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector) model.load_state_dict(torch.load('./pytorch_model')) model.eval()  MF( (user): Embedding(943, 10) (item): Embedding(1682, 10) (bias_user): Embedding(943, 1) (bias_item): Embedding(1682, 1) )  # Print the recommended items for each user limit = 0 for uid,_,_ in testset: print('\\nUser:',uid) seen = [df.loc[int(iid),'name'] for (iid, _) in trainset.ur[int(uid)]] if len(seen) \u0026gt; 10: seen = seen[:10] print('\\tSeen:',seen) user_ratings = get_top_n(model,testset,trainset,uid,n=10) print('\\tRecommendations:',[df.loc[int(iid),'name'] for (iid, _) in user_ratings]) limit+=1 if limit\u0026gt;3: break  User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)'] User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)'] User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)'] User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)']  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/",
	"title": "Lecture 3",
	"tags": [],
	"description": "",
	"content": " Serving ML Models Using Docker "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/",
	"title": "Lecture 4",
	"tags": [],
	"description": "",
	"content": " ML Model Pipelines - A/B testing: sample size considerations - Tackling bandit feedback "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture5/",
	"title": "Lecture 5",
	"tags": [],
	"description": "",
	"content": " TBD "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture6/",
	"title": "Lecture 6",
	"tags": [],
	"description": "",
	"content": " TBD "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture7/",
	"title": "Lecture 7",
	"tags": [],
	"description": "",
	"content": " TBD "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture8/",
	"title": "Lecture 8",
	"tags": [],
	"description": "",
	"content": " Online Experimentation - A/B testing: sample size considerations - Tackling bandit feedback "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Deploy model on Heroku.\n Set up your custom domain to point to your VPS.\n Repeat the setup on AWS, GCP, DigitalOcean or any other VPS of your choice.\n Read the documentation for flask, mlflow, pytorch, surprise, pandas.\n Replace Flask with Django and Starlette.\n Read up about function decorators in Python.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Find out how serverless technologies work behind the scene.\n Connect your custom domain to the GCP Cloudn Function and the API Gateway/Lambda function in AWS.\n Learn command line tools for GCP and the difference between programmatic access and manual access.\n Learn about identities, roles and access aspects in GCP and AWS.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/",
	"title": "MLOps: Operationalizing Machine Learning",
	"tags": [],
	"description": "",
	"content": " Operationalizing Machine Learning (IDS594) Note: Also known as ML Deployment in the course catalog.\nThis practice-oriented course surveys modern best practices around getting machine learning (ML) models into production. It continues where IDS 572 and IDS 575 left off, which is to learn multiple ways of operationalizing machine learning work flows and models in the context of the larger business end-goals. The course is complementary to IDS 561. We will gain a better understanding of strategies for model management, monitoring and deployment. We will also intertwine these topics with online experimentation techniques (A/B testing) and software engineering ideas such as version control, containerization, and continuous integration/continuous deployment.\nA tentative list of topics is as follows:\n Deploying ML models using web servers Containers for machine learning: the docker ecosystem and Kubernetes Git, CI/CD and their modifications for ML workflows A/B Testing of KPIs and data considerations Model management: model tracking and logging Case studies: Databricks\u0026rsquo; MLFlow, Google\u0026rsquo;s TFX/Kubeflow, Uber’s Michelangelo, Facebook\u0026rsquo;s FBLearner Flow.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]