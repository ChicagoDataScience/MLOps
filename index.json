[
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/",
	"title": "Course Logistics",
	"tags": [],
	"description": "",
	"content": " Course Logistics  Semester: Fall 2020 Lectures: Thursdays 6.30 PM to 9.00 PM Mode: Online synchronous (i.e., location is online). The course will be delivered over Zoom (an invite will be sent before the first day of class). See the online learning page for basic technology requirements. Staff  Instructor: Dr. Theja Tulabandhula (netid: theja) Teaching Assistant: Tengteng Ma (netid: tma24)  Communication: via slack, zoom and one note class notebook. Office hours: online via slack and zoom](https://uic.zoom.com).  Textbook and Materials  Data Science in Production by Ben Weber (2020, $5 for the ebook/pdf). A sample of the first three chapters is available at the publishers page linked here.  Software  Any OS should be okay. If in doubt, run a virtual machine running linux (this will be discussed in the class). Some of the software we will work with are: Docker for Desktop  Lightweight Kubernetes  Python (Anaconda)  flask requests pandas pytorch scikit-learn matplotlib \u0026hellip;  \u0026hellip;  Hardware  There will be varied computing resources needed for this course. Try using a virtual machine with linux on your own computer if possible. A Windows virtual desktop is available at desktop.uic.edu if needed. You can refer to these two help pages to get started.  Assignments  There are no graded assignments or exams for this course. Students are expected to go over the lectures and practice the use of technologies discussed each week.  Project  Students are expected to apply what they learn in the course and demonstrate a deployment of an existing machine learning model they have access to. A suitable documentation of this process along with the scripts/codes/commands used is to be submitted on October 14th (with no exceptions). The evaluation criteria and other details will be released shortly. Submission deadline is BEFORE 11.59 PM on the concerned day. Late submissions will have an automatic 20% penalty per day. Use Blackboard for uploading your work as a single zip file.  Grade  Grades will be assigned based on the project (see project evaluation criteria above) (80%) and course participation (20%).  Miscellaneous Information  This is a 2 credit graduate level course offered by the Information and Decision Sciences department at UIC. See the academic calendar for the semester timeline. Students who wish to observe their religious holidays (http://oae.uic.edu/religious-calendar/) should notify the instructor within one week of the first lecture date. Contact the instructor at the earliest, if you require any accommodations for access to and/or participation in this course. Refer to the academic integrity guidelines set by the university.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/",
	"title": "Lecture 1",
	"tags": [],
	"description": "",
	"content": " Web Servers - SSH and Firewall - Conda Environments - Jupyter - Making requests and processing responses - Model Persistence using MLFlow - Serving a model using Flask "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/online_technology_requirements/",
	"title": "Online Learning Details",
	"tags": [],
	"description": "",
	"content": " Online Learning Details To maximize the learning experience, it will be good if students can meet the following basic technology requirements:\n At a minimum, students should have a device and an internet connection. A microphone, and a webcam would be highly recommended. See the Basic Technology Requirements link for more details.\n Laptop, Chromebook or Desktop Computer: Note that Chromebooks are used to perform a variety of browser-based tasks with most data and applications, such as Blackboard Learn, Blackboard Collaborate, Google Docs, and Office 365, residing in the cloud rather than on the machine itself. This can result in somewhat reduced functionality, depending on your needs. If you do not have reliable access to a computer at home, ACCC may have a laptop to lend to you. Please fill out our request form at accc.uic.edu/forms/laptop-request\n Internet: Many service providers are offering connectivity solutions for students without access to Wi-Fi or the internet. The Illinois Citizens Utility Board is maintaining a comprehensive list of the available options here: citizensutilityboard.org/blog/2020/03/19/cubs-guide-utility-services-during-the-covid-19-public-health-emergency.\n The State of Illinois is maintaining a map of publicly available internet hotspots across the state that can be used for academic-related needs. These hotspots are available from within a parked vehicle. The map, and additional information, can be viewed at www.ildceo.net/wifi.\n Additionally, the ACCC has a very limited supply of cellular hotspots available for those students who are unable to take advantage of the above offers. Please fill out our request form at accc.uic.edu/forms/laptop-request/.\n  Microphone: While this may be built into your computer, we recommend using an external device such as a USB microphone or headset.\n Webcam: A built-in camera may be installed on your laptop; if not, you can use an external USB camera for video conferencing.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/schedule/",
	"title": "Schedule",
	"tags": [],
	"description": "",
	"content": " Textbook  Data Science in Production by Ben Weber (2020, $5 for the ebook/pdf). A sample of the first three chapters is available at the publishers page linked here.  Lecture Schedule Lecture 1: Serving ML Models Using Web Servers  Ref Chapter 2  Lecture 2: Serving ML Models Using Serverless Infrastructure  Ref Chapter 3  Lecture 3: Serving ML Models Using Docker  Ref Chapter 4, upto 4.2  Lecture 4: Kubernetes for Orchestration  Ref Chapter 4, 4.3 onwards  Lecture 5: ML Model Pipelines  Ref Chapter 5  Lecture 6:  Ref Chapter 6  Lecture 7:  Ref Chapter 7  Lecture 8: Online Experimentation  Ref 1: https://help.optimizely.com/Get_Started/Get_started_with_Optimizely_Web_Recommendations Ref 2: https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/project_instructions/",
	"title": "Project",
	"tags": [],
	"description": "",
	"content": " Aim  The aim of the project is to simulate the real-world process of deploying machine learning models. More specifically, the project component of this course allows you to explore a technology that assists in model deployment, either directly or indirectly, and asks you to report your experience working with that technology (or multiple technologies) to achieve some overall deployment goal.  Group  You should form groups of 4 students for this project component (this is a strict requirement). Reach out to your classmates early. Because this is a group project, a commensurate effort is expected, and each members contributions needs to be reported in the final submission.  Project Outcomes  There is one due-date for the project deliverables. See the course logistics page for the exact date. The deliverables are as follows. Project Report: In at most 8 pages (12 point, single column; you can have an appendix for supplementary material that may or may not be checked), you should explain your contributions in the project. Code and data: Code associated with the project (e.g., Jupyter notebooks), a small sample of the data/inputs/outputs if needed, and all steps necessary to replicate your project should be provided along with/in the report. A link to your github/gitlab/bitbucket/other repository is acceptable here (provide it at the front page of the report). A video presentation: You should provide a 10 minute video walk-through (discussing highlights) of your project and provide the link (say from Youtube where the video can be in unlisted mode) on the front page of the report.  Each team should upload the report (and code and video link) to Blackboard before the deadline.\nExample Report Components  For example, here are some aspects to focus on in your project report:  what was the goal what were the possible solutions what were the specific pros and cons from a business point of view a cost benefit analysis actual handling of the technology and demonstration in a dev environment documenting the experience lessons learned code artifacts and/or Jupyter notebooks \u0026hellip;  Here is an example project idea: try out a technology (or a specific aspect of it) and its competitors by following their documentation in a very extensive and well thought out manner (e.g., MLFlow vs bentoml vs cortex).  Grading Rubric  Projects will be graded based on the creativity shown in handling the technology and the insights drawn. The reports should be very clearly written and presented, and will be evaluated based on the correctness, content, creativity and clarity:  Correctness will be assessed based on the correct application of a technology, valid software setup and discussion of choices, technical correctness and the assumptions laid out. Content will be assessed based on the contributions made in the project (given group size) and project depth (e.g., why this aspect of ML deployment, why this problem, what did you do, visualization and interesting conclusions, insights, discussion of methodology followed). You should try to demonstrate your understanding of the relevant topics and their use in your non-trivial project. Creativity will be assessed based on how no-obvious your solution or contribution is and how different choices were thoughtfully made in the execution of the project. Clarity will be assessed based on the language quality, layout and structure of the report, the adequacy of the references cited, the capability of the team in explaining ideas in a clear and professional manner, and the clarity demonstrated in your discussions etc.  All external material/sources (code/idea/theory/insights) used should be cited prominently without failure. Use of pre-trained models, databases, web servers, front-end frameworks, visualization tools etc for your project is allowed and encouraged. This project cannot be used as part of any other course or requirement.  Additional Pointers  Keep track of costs especially if you are using services that require having a payment mode on file. Also, try to use free resources as much as possible. Do not train deep networks from scratch if it can be avoided. The project should not be centered around model accuracy. It is importantly to make a project plan that allocates sufficient tasks for each team member. It will be great if you can submit the project plan (a Gantt chart for example).  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/intro/",
	"title": "Basics",
	"tags": [],
	"description": "",
	"content": " Python  We will be predominantly concerned with the Python ecosystem A big advanage is that local system development can be easily moved to cloud and or a scalable on-prem solution. Many companies use python to start data science projects in-house (via fresh recruits, interns etc) Python has some relatively easy ways to access databases Big data platforms such as Spark have great python bindings  E.g., Pandas dataframe and Spark dataframe  Latest models (deep learning, pre-trained) are built in the python ecosystem Many many useful libraries: pandas, matplotlib, flask,\u0026hellip;  Our Objective  Learn the patterns, not the specific tools  Deployment Targets  Local machines On-prem or self-hosted machines (needs DevOps skills) Managed cloud  Heroku (PAAS) Azure GCP AWS (IAAS)  The decision to deply on one versus the other depends on  skills business need internal vs external scale, reliability, security costs ease of deployment   Local Deployments are Hard  Need to learn linux security Need to learn how to manage access Need for learn backups Need to learn hot switching / reliability  Cloud Deployments are not Easy  Also need to learn a complex ecosystem Vendor lock-in (for successful businesses, this is not an issue)  Aside: Software Tools Python development can happen:\n In text editors (e.g., sublime-text) In IDEs (e.g., Pycharm or VSCode) In Jupyter notebooks and variants (Google Colab, Databricks notebooks)  vanilla notebook does not allow collaboration as such   Part 1: Setting up Jupyter access on a VPS  We will use Vultr, but all steps are vendor agnostic. Alternatives include: Digitalocean, AWS EC2, Google Cloud; using Google Colab and other vendors. SSH passwordless access is set up. Next, we set up a basic firewall for security. This is followed by installing conda. (Optional) To run the jupyter server uninterrupted, we will run it within a screen session. We will access the server and notebooks on our local browser using SSH tunneling.  Part 2: Preparing an ML Model  We will show how data is accessed, and how the model is trained (this should be familiar to you).\n In particular, we will look at the moive recommendation problem.  There are aspects of saving and loading models that become important in production. For instance, we would like the models to be able to live across dev/staging/prod environments. For this, we think of the notion of model persistence\n Natively:\n For example, pytorch has native save and load methods. Same is the case for scikit-learn and a variety of other packages.  Using MLFlow:\n MLFlow addresses the problem of moving models across different environments without issues of incompatibility (minor version numbers, OS etc) among other things. See these links for more information: https://pypi.org/project/mlflow/ and mlflow.org    "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/ssh_and_firewall/",
	"title": "SSH and Firewall",
	"tags": [],
	"description": "",
	"content": " It is important to secure your connection to the machine. In order to do so, we will configure the ssh access pattern as well as set up a firewall that blocks all incoming requests except ssh port and web server ports.\nWe will assume that we have a non-root account that is in the sudoers group.\nSSH  When you first create the server instance, you may or may not have the ssh server running. If it is not running, you can install it first. On Ubuntu/Debian, you can use the following command:\nsudo apt install openssh-server  Next, we create a ssh key pair on our local machine with which we will access the server. From your local user home directory:\nmkdir .ssh ssh-keygen cd .ssh less id_rsa.pub  Copy this content to the following file authorized_keys in the webserver:\nmkdir .ssh vim authorized_keys #if vim is not present, you can use other editors or install it using `sudo apt install vim` #copy the content and quit (shift+colon\u0026gt; wq -\u0026gt; enter) chmod 600 authorized_keys  We need to edit the following fields in the file /etc/ssh/sshd_config on the server (say using vim):\n Port choose something other than 22 (opttional) PermitRootLogin no (changed from prohibit-password) PubkeyAuthentication yes (already defaults to this) PasswordAuthentication no (disable it for security)  Restart the ssh server. In Ubuntu/Debian this is achieved by sudo systemctl restart ssh\n  Firewall  A basic firewall such as ufw can help provide a layer of security. Install and run it using the following commands (Ubuntu/Debian):\nsudo apt install ufw sudo ufw allow [PortNumber] #here it is 22 or another port that you chose for ssh sudo ufw enable sudo ufw status verbose #this should show what the firewall is doing   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/conda/",
	"title": "Setting up Python",
	"tags": [],
	"description": "",
	"content": " Here are a few notes on installing a user specific python distribution:\nGet Miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh conda install pip #better to use the pip in the base conda env than system pip   The difference between conda and pip: pip is a package manager specifically for python, whereas conda is a package manager for multiple languages as well as is an environment manager. Python module venv is python specific environment manager.  Set up a conda environment and activate it conda create --name datasci-env python #or conda create -n dataeng-env python jupyter pandas numpy matplotlib #or conda create -n datasci-env scipy=0.15.0 #or conda env create -f environment.yml conda activate datasci-env   You don\u0026rsquo;t have to give names, can give prefixes where the env is saved, can create based on specific pages, can use explicit previous conda environments, yaml files, clone/update an existing one, etc. Use this link to get more information.\n Specifying a path to a subdirectory of your project directory when creating an environment can keep everything 100% self contained.\n To deactivate this environment, use conda deactivate datasci-env.\n  Install jupyter and pytorch (and tensorflow, keras, scikit-learn similarly) in a specific environment conda install jupyter conda install pytorch torchvision cpuonly -c pytorch # https://pytorch.org/   Change the command for pytorch installation if you do intend to use GPUs. In particular, install CUDA from conda after installing the latest NVidia drivers on the instance.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/jupyter/",
	"title": "Remote Jupyter Server",
	"tags": [],
	"description": "",
	"content": "The following sets a simple password based login, which is handy:\njupyter notebook --generate-config jupyter notebook password  Unfortuantely, hashed password is sent unencrypted by your browser here. So read up here to do this in a better way.\nStarting jupyter on the server can be done inside a screen session:\nscreen -S jupyter-session #can also use nohup or tmux here jupyter notebook --no-browser --port=8888  SSH tunnel can be setup by running the following on your local machine, and then opening the browser (http://localhost:8889)\nssh -N -f -L localhost:8889:localhost:8888 -p 22 theja@192.168.0.105  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/",
	"title": "Recommendation Models",
	"tags": [],
	"description": "",
	"content": "We will look at two models for recommending movies to existing users.\n Matrix factorization based on the surprise package. Matrix factorization based on Pytorch.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/surprise_training/",
	"title": "Recommendation (SVD) Training",
	"tags": [],
	"description": "",
	"content": "# https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.accuracy import rmse from surprise.dump import dump  # Load the movielens-100k dataset (download it if needed). data = Dataset.load_builtin('ml-100k') trainset = data.build_full_trainset() # Use an example algorithm: SVD. algo = SVD() algo.fit(trainset) # predict ratings for all pairs (u, i) that are in the training set. testset = trainset.build_testset() predictions = algo.test(testset) rmse(predictions) #actual predictions as thse items have not been seen by the users. there is no ground truth. # We predict ratings for all pairs (u, i) that are NOT in the training set. testset = trainset.build_anti_testset() predictions = algo.test(testset)  RMSE: 0.6774  dump('./surprise_model', predictions, algo)  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/deploy_webserver/",
	"title": "Serving ML Models Using Web Servers",
	"tags": [],
	"description": "",
	"content": " Model Serving  Sharing results with others (humans, web services, applications) Batch approach: dump predictions to a database (quite popular) Real-time approach: send a test feature vector, get back the prediction instantly and the computation happens now  How to consume from prediction services?  Using web requests (e.g., using a JSON payload)  How to output predictions?  We will plan to set up a server to serve predictions  It will respond to web requests (GET, POST) We pass some inputs (image, text, vector of numbers), and get some outputs (just like a function) The environment from which we pass inputs may be very different from the environment where the prediction happens (e.g., different hardware)   Our Objective  Use sklearn/keras with flask, gunicorn and heroku to set up a prediction server  Part 1: Making API Calls  Using the requests module from a jupyter notebook (this is an example of a programmatic way) Alternatively, using curl or postman (these are more versatile)  Part 2: Simple Flask App  Function decorators are used in Flask to achive routes to functions mapping. Integrating the model with the app is relatively easy if the model can be read from disk.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/surprise_inference/",
	"title": "Recommendation (SVD) Inference",
	"tags": [],
	"description": "",
	"content": "# https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.dump import load from collections import defaultdict import pandas as pd def get_top_n(predictions, n=10): \u0026quot;\u0026quot;\u0026quot;Return the top-N recommendation for each user from a set of predictions. Args: predictions(list of Prediction objects): The list of predictions, as returned by the test method of an algorithm. n(int): The number of recommendation to output for each user. Default is 10. Returns: A dict where keys are user (raw) ids and values are lists of tuples: [(raw item id, rating estimation), ...] of size n. \u0026quot;\u0026quot;\u0026quot; # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n  df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('./surprise_model')  Output:\n 0 1 2 0 1 Toy Story (1995) Animation|Children's|Comedy 1 2 Jumanji (1995) Adventure|Children's|Fantasy 2 3 Grumpier Old Men (1995) Comedy|Romance 3 4 Waiting to Exhale (1995) Comedy|Drama 4 5 Father of the Bride Part II (1995) Comedy algo: SVD  top_n = get_top_n(predictions, n=5) # Print the recommended items for each user limit = 0 for uid, user_ratings in top_n.items(): print('\\nUser:',uid) seen = [df.loc[int(iid),'name'] for (iid, _) in algo.trainset.ur[int(uid)]] if len(seen) \u0026gt; 10: seen = seen[:10] print('\\tSeen:',seen) print('\\tRecommendations:',[df.loc[int(iid),'name'] for (iid, _) in user_ratings]) limit+=1 if limit\u0026gt;3: break  Output:\nUser: 196 Seen: ['Richie Rich (1994)', 'Getaway, The (1994)', 'Batman Forever (1995)', 'Feast of July (1995)', 'Heidi Fleiss: Hollywood Madam (1995)', 'Shadows (Cienie) (1988)', 'Terminator 2: Judgment Day (1991)', \u0026quot;Nobody's Fool (1994)\u0026quot;, \u0026quot;Breakfast at Tiffany's (1961)\u0026quot;, 'Basic Instinct (1992)'] Recommendations: ['Age of Innocence, The (1993)', 'Bio-Dome (1996)', 'Strawberry and Chocolate (Fresa y chocolate) (1993)', 'Guardian Angel (1994)', \u0026quot;Carlito's Way (1993)\u0026quot;] User: 186 Seen: ['Double Happiness (1994)', 'Mr. Jones (1993)', 'War Room, The (1993)', 'Bloodsport 2 (1995)', 'Usual Suspects, The (1995)', 'Big Green, The (1995)', 'Mighty Morphin Power Rangers: The Movie (1995)', 'Boys on the Side (1995)', 'Cold Fever (� k�ldum klaka) (1994)', 'Sum of Us, The (1994)'] Recommendations: ['Lightning Jack (1994)', 'Robocop 3 (1993)', 'Walk in the Clouds, A (1995)', 'Living in Oblivion (1995)', 'Strawberry and Chocolate (Fresa y chocolate) (1993)'] User: 22 Seen: ['Assassins (1995)', 'Nico Icon (1995)', 'From the Journals of Jean Seberg (1995)', 'Last Summer in the Hamptons (1995)', 'Down Periscope (1996)', 'Bushwhacked (1995)', 'Beyond Bedlam (1993)', 'Client, The (1994)', 'Hoop Dreams (1994)', 'Ladybird Ladybird (1994)'] Recommendations: ['Home for the Holidays (1995)', 'Age of Innocence, The (1993)', 'Balto (1995)', 'City Hall (1996)', 'Ready to Wear (Pret-A-Porter) (1994)'] User: 244 Seen: ['When Night Is Falling (1995)', 'Birdcage, The (1996)', '8 Seconds (1994)', 'Foreign Student (1994)', 'Mighty Aphrodite (1995)', 'Before Sunrise (1995)', 'Lion King, The (1994)', 'Clockers (1995)', 'Underneath, The (1995)', 'Manny \u0026amp; Lo (1996)'] Recommendations: ['Century (1993)', 'Balto (1995)', 'Age of Innocence, The (1993)', 'Remains of the Day, The (1993)', 'Jimmy Hollywood (1994)']  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/flask/",
	"title": "Flask App",
	"tags": [],
	"description": "",
	"content": " Flask is a micro web framework written in Python. We first show how a simple service works, and then show how to load a model (e.g., based on pytorch) and serve it as well.\nWeather Reporting Service The key thing to see here are that the HTTP route / is mapped directly to a function weather. For instance, when someone hits localhost:5000 (5000 is the default unless specified in app.run() below) the function weather starts execution based on any received inputs.\n# load Flask import flask from flask import jsonify from geopy.geocoders import Nominatim import requests app = flask.Flask(__name__) # define a predict function as an endpoint @app.route(\u0026quot;/\u0026quot;, methods=[\u0026quot;GET\u0026quot;,\u0026quot;POST\u0026quot;]) def weather(): data = {\u0026quot;success\u0026quot;: False} #https://pypi.org/project/geopy/ geolocator = Nominatim(user_agent=\u0026quot;cloud_function_weather_app\u0026quot;) params = flask.request.json if params is None: params = flask.request.args # params = request.get_json() if \u0026quot;msg\u0026quot; in params: location = geolocator.geocode(str(params['msg'])) # https://www.weather.gov/documentation/services-web-api result1 = requests.get(f\u0026quot;https://api.weather.gov/points/{location.latitude},{location.longitude}\u0026quot;) result2 = requests.get(f\u0026quot;{result1.json()['properties']['forecast']}\u0026quot;) data[\u0026quot;response\u0026quot;] = result2.json() data[\u0026quot;success\u0026quot;] = True return jsonify(data) # start the flask app, allow remote connections if __name__ == '__main__': app.run(host='0.0.0.0')  This service can be run by using the command python weather.py (assuming that is the filename for the above script) locally. If the port 5000 is open, then this server will be accessible to the world if the server has an external IP address.\nModel Serving We can modify the above to serve the recommendation models we built earlier as follows:\nfrom surprise import Dataset from surprise.dump import load from collections import defaultdict import pandas as pd import flask def get_top_n(predictions, n=10): # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('./surprise_model') top_n = get_top_n(predictions, n=5) app = flask.Flask(__name__) # define a predict function as an endpoint @app.route(\u0026quot;/\u0026quot;, methods=[\u0026quot;GET\u0026quot;]) def predict(): data = {\u0026quot;success\u0026quot;: False} # check for passed in parameters params = flask.request.json if params is None: params = flask.request.args if \u0026quot;uid\u0026quot; in params.keys(): data[\u0026quot;response\u0026quot;] = str([df.loc[int(iid),'name'] for (iid, _) in top_n[params.get(\u0026quot;uid\u0026quot;)]]) data[\u0026quot;success\u0026quot;] = True # return a response in json format return flask.jsonify(data) # start the flask app, allow remote connections app.run(host='0.0.0.0')  You can use the following request in the browser http://0.0.0.0:5000/?uid=196 or use the requests module.\n"
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/",
	"title": "Lecture 2",
	"tags": [],
	"description": "",
	"content": " Serverless Deployments - Managed Solutions - Cloud Functions (GCP) - Lambda Functions (AWS) "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/serverless/",
	"title": "Serverless Deployments",
	"tags": [],
	"description": "",
	"content": " A. TLDR  Models do not need to be complex, but it can be complex to deploy models. - Ben Weber (2020)\n Problem  We have to take care of provisioning and server maintenance while deploying our models. We have to worry about scale: would 1 server be enough? How to minimize the time to deploy (at an acceptable increase in cost)? How can a single developer or data science/analytics professional manage a complex service?  Solution  Software that abstracts away server details and lets you serve your model (any function actually) with few lines of code/UI.\n The software automates\n prrovising servers scaling machines up and down  load balancing\n code versioning\n Our task is then to specify the requirements (pandas, pytorch).\n   B. Our Objective  Write serverless functions that generate predictions when they get triggered by HTTP requests. We will work with:  AWS GCP  We will deploy  a keras model, and a sklearn model.   C. Managed Services  Cloud is responsible for abstracting away various computing components: compute, storage, networking etc Minimizes thinking about dev/staging vs production. Note: what we did last class, ssh\u0026rsquo;ing into a virtual private server (VPS) would be considered as a hosted deployment, which is the opposite of managed deployment For example, serverless technology was introduced in 2015\u0026frasl;2016 by AWS (Amazon web) and GCP (Google cloud). It contrasts with VPS based deployment. Similarly, AWS ECS (Elastic Container Service) managed solution contrasts with the hosted/manual Docker on VPS setup.  When is a managed solution a bad idea?  No need for quick iteration (company cares about processes and protocols) Need a high speed service No need to scale system arbirarily Cost conscious or have an in-house developer.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions/",
	"title": "Cloud Functions",
	"tags": [],
	"description": "",
	"content": " Intro  Cloud Functions (CFs) are a solution from GCP for serverless deployments. Very little boilerplate beyond what we will write for simple offline model inference. In any such deployment, we need to be concerned about:  where the model is stored (recall pickle and mlflow), and what python packages are available.   Empty Deployment  We will set up triggers that will trigger our serving function (in particular, a HTTP request). We will specify the requirements needed for our python function to work The function we deploy here, similar to lecture 1, produces weather forecasts given a location.  Setting up using UI  Sign up with GCP if you haven\u0026rsquo;t already (typically you get a 300$ credit)\n Get to the console and find the Cloud Function page.\n   Go through the UI for creating a function.   We will choose the HTTP trigger and unauthenticated access option.   We may have to enable Cloud Build API   Finally, we choose the Python environment. You can see two default example files (main.py and requirements.txt). We will be modifying these two.  Python Files and Requirements  We will specify the following requirements:\nflask geopy requests  Our main file is the following:\ndef weather(request): from flask import jsonify from geopy.geocoders import Nominatim import requests data = {\u0026quot;success\u0026quot;: False} #https://pypi.org/project/geopy/ geolocator = Nominatim(user_agent=\u0026quot;cloud_function_weather_app\u0026quot;) params = request.get_json() if \u0026quot;msg\u0026quot; in params: location = geolocator.geocode(str(params['msg'])) # https://www.weather.gov/documentation/services-web-api # Example query: https://api.weather.gov/points/39.7456,-97.0892 result1 = requests.get(f\u0026quot;https://api.weather.gov/points/{location.latitude},{location.longitude}\u0026quot;) # Example query: https://api.weather.gov/gridpoints/TOP/31,80 result2 = requests.get(f\u0026quot;{result1.json()['properties']['forecast']}\u0026quot;) data[\u0026quot;response\u0026quot;] = result2.json() data[\u0026quot;success\u0026quot;] = True return jsonify(data)  Once the function is deployed, we can test the function (click actions on the far right in the dashboard)\n   We can pass the JSON string {\u0026quot;msg\u0026quot;:\u0026quot;Chicago\u0026quot;} and see that we indeed get the JSON output for the weather of Chicago.   We can also access the function from the web endpoint https://us-central1-authentic-realm-276822.cloudfunctions.net/function-1 (you will have a different endpoint). Note that unlike previous times, the request to this endpoint is a JSON payload.\n Below is the screen-shot of querying the weather of Chicago using the Postman tool. The way to use it is as follows:\n Insert the URL of the API Se the method type to POST Navigate to body and choose raw and then choose JSON from the dropdown menu. Now add the relevant parameters as a JSON string.    Finally, here is a query you can use from a Jupyter notebook.\nimport requests result = requests.post( \u0026quot;https://us-central1-authentic-realm-276822.cloudfunctions.net/function-1\u0026quot; ,json = { 'msg': 'Chicago' }) print(result.json()) #should match with https://forecast.weather.gov/MapClick.php?textField1=41.98\u0026amp;textField2=-87.9   Saving Model on the Cloud  For our original task of deploying a trained ML model, we need a way to read it from somewhere when the function is triggered.\n One way is to dump the model onto Google Cloud Storage (GCS)\n GCS is similar to the S3 (simple storage service) by AWS.\n We will use the command line to dump our model onto the cloud.\n  GCP access via the commandline  First we need to install the Google Cloud SDK from https://cloud.google.com/sdk/docs/downloads-interactive\ncurl https://sdk.cloud.google.com | bash gcloud init  There are two types of accounts you can work with: a user account or a service account (see https://cloud.google.com/sdk/docs/authorizing?authuser=2).\n Among others, [this page] gives a brief idea of why such an account is needed. In particular, we will create a service account (so that it can be used by an application programmatically anywhere) and store the encrypted credentials on disk for programmatic access through python. To do so, we run the following commands:\n We create the service account and check that it is active by using this command: gcloud iam service-accounts list.\ngcloud iam service-accounts create idsservice \\ --description=\u0026quot;IDS service account\u0026quot; \\ --display-name=\u0026quot;idsservice-displayed\u0026quot;  We then assign a new role for this service account in the project. The account can be disabled using the command gcloud iam service-accounts disable idsservice@authentic-realm-276822.iam.gserviceaccount.com (change idsservice and authentic-realm-276822 to your specific names).\ngcloud projects add-iam-policy-binding authentic-realm-276822 --member=serviceAccount:idsservice@authentic-realm-276822.iam.gserviceaccount.com --role=roles/owner  Finally, we can download the credentials\ngcloud iam service-accounts keys create ~/idsservice.json \\ --iam-account idsservice@authentic-realm-276822.iam.gserviceaccount.com  Once the credentials are downloaded, they can be programmatically accessed using python running on that machine. We just have to explore the location of the file:\nexport GOOGLE_APPLICATION_CREDENTIALS=/Users/theja/idsservice.json   Next we will install a python module to access GCS, so that we can write our model to the cloud:\npip install google-cloud-storage  The following code creates a bucket called theja_model_store\nfrom google.cloud import storage bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() storage_client.create_bucket(bucket_name) for bucket in storage_client.list_buckets(): print(bucket.name)  We can dump the model we used previous here using the following snippet\nfrom google.cloud import storage bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1a\u0026quot;) blob.upload_from_filename(\u0026quot;surprise_model\u0026quot;) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1b\u0026quot;) blob.upload_from_filename(\u0026quot;movies.dat\u0026quot;)   After running the above, the surprise package based recommendation model and the helper data file will be available at gs://theja_model_store/serverless/surprise_model/v1a and gs://theja_model_store/serverless/surprise_model/v1b as seen below.\nWe can either use the URIs above or use a programmatic way with the storage class. For example, here is the way to download the file v1b:\nfrom google.cloud import storage bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1b\u0026quot;) blob.download_to_filename(\u0026quot;movies.dat.from_gcp\u0026quot;)  We can diff it in Jupyter notebook itself using the expression !diff movies.dat movies.dat.from_gcp.\nWe will use this programmatic way of reading external data/model in the cloud function next.\n"
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions_model/",
	"title": "GCP Serverless Model Serving",
	"tags": [],
	"description": "",
	"content": " We modify the flask app that we had before, by again specifying the requirements.txt and the main python file appropriately. We will also increase the memory to 2GB and the timeout to 180 seconds. You will see that the following deployment has a lot of inefficiencies (can you spot the redundacy in loading the model and the predictions below?).\nThe requirements file will have the following entries:\nnumpy flask pandas google-cloud-storage scikit-surprise pickle5  The main file is also modified accordingly. Note that if we reload the model and the metadata on every request, it will be extremely inefficient. To fix that we can use global variables. This is not a good choice in much of python programming, but quite useful here. Essentially, global variables allow us to cache some of the objects, for faster response times.\ntop_n = None def recommend(request): global top_n from surprise import Dataset import pandas as pd import flask from google.cloud import storage import pickle5 def load(file_name): dump_obj = pickle5.load(open(file_name, 'rb')) return dump_obj['predictions'], dump_obj['algo'] def get_top_n(predictions, n=10): def defaultdict(default_type): class DefaultDict(dict): def __getitem__(self, key): if key not in self: dict.__setitem__(self, key, default_type()) return dict.__getitem__(self, key) return DefaultDict() # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n data = {\u0026quot;success\u0026quot;: False} params = request.get_json() if \u0026quot;uid\u0026quot; in params: if not top_n: bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1a\u0026quot;) blob.download_to_filename(\u0026quot;/tmp/surprise_model\u0026quot;) #ideally we should be reading things into memory blob = bucket.blob(\u0026quot;serverless/surprise_model/v1b\u0026quot;) blob.download_to_filename(\u0026quot;/tmp/movies.dat\u0026quot;) df = pd.read_csv('/tmp/movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('/tmp/surprise_model') top_n = get_top_n(predictions, n=5) data[\u0026quot;response\u0026quot;] = str([df.loc[int(iid),'name'] for (iid, _) in top_n[params.get(\u0026quot;uid\u0026quot;)]]) data[\u0026quot;success\u0026quot;] = True # return a response in json format return flask.jsonify(data)  We can test the function on the GCP console with a request JSON {\u0026quot;uid\u0026quot;:\u0026quot;206\u0026quot;}.\nAs an exercise, think of ways to make the whole setup above lightweight in terms of model and data size.\nDeploying the pytorch model after removing the dependecy on surprise is also a good challenge to tackle.\nAccess Management  It is a bad idea to allow unauthenticated access There are ways of restricting who can reach this model serving endpoint.  One way is to disable unauthenticated access while creating the function. Once we do that, we can create a permission structure based on GCP best practices.  We will omit the details here.  Updating Models and Monitoring  We can easily update our endpoint by rewriting the files we uploaded to GCS. A cleaner way is to create another cloud function. The function itself can have logic to reload files using the google-cloud-storage module based on time elapsed (say using datetime). Monitoring is very important, not just to know if our model performance has changed over time, but also to measure things such as latency, bugs and API access patterns. We will revisit this topic in the future.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions/",
	"title": "Lambda Functions",
	"tags": [],
	"description": "",
	"content": "  Lambda by Amazon Web Services (AWS) is an analogous serverless solution. Lambda can be used internall as well as for model deployments (we are focusing on the latter). We will repeat setting up the weather app and the recommender model, using the CLI (command line interface tools)  Aside: Setting up an IAM user  TBD  Hello World in Lambda  Select the lambda service.   Pick the python 3.7 runtime.   You will see the green bar indicating a successful creation.   We will initially not changed the code.   And create a dummy test.   The result looks like this.  Setting up an API Gateway  We will slightly change the function so that it forms a proper HTTP response.   Under the Designer section, we will \u0026lsquo;Add Trigger\u0026rsquo; and select \u0026lsquo;API Gateway\u0026rsquo;.   We choose HTTP API and security to be open for the time being.   A block will appear on the designer tab in the Lambda function page.   Once you click on the API Gateway link provided, you will see the response as expected:  See this documentation for more details.\nSet up a Programmatic User to Access S3  What is IAM? IAM (Identity and Access Management) is a very useful tool for an organization to administer and control access to various resources.\n We will create a programmatic user, similar to the service account in GCP.  What is S3? S3 (Simple Storage Service) is a very popular service used by many large and small companies and individuals.\n For example, a typical data science work flow may involve ETL work on databases (such as Amazon Redshift) and then storing outputs on S3.  Start with finding the IAM service.\n   Click on create a new user.   We will make this user have programmatic access (for example via Python later on).   For now, we let the user have full read and write access to S3.   Lets review the user setup.   In this step, copy the access key and secret key that will be needed for the programmatic access. They will not be shown again, so store them safely and securely.   Here is the description of the user we just created.  Set up AWS CLI  The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.\n  Head over to https://aws.amazon.com/cli/ and download a suitable installer for your OS. Install it and open your terminal. If it is in the PATH, you should be able to verify the following:\nwhich aws #/usr/local/bin/aws #your location could be different. aws --version #aws-cli/2.0.45 Python/3.7.4 Darwin/18.6.0 exe/x86_64  Using the access and secret keys to configure aws using the aws configure command.\n   Test S3 access by issing the following commmand: aws s3 ls\n$ aws s3 ls 2020-09-03 12:27:48 chicagodatascience-com 2020-09-03 12:02:25 theja-model-store  Instead of programmatically creating a bucket (with the aws s3 mb s3://bucket-name) we will do it using the GUI below.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions_model/",
	"title": "AWS Serverless Model Serving",
	"tags": [],
	"description": "",
	"content": " Storing the Model on S3  To set up S3 for model serving, we have to perform a number of steps. We start with the s3 page.  - Create a bucket with an informative name.\n We don\u0026rsquo;t have to touch any of these for now.   Here the summary to review.   And we can see the bucket in the list of buckets.  Zip of Local Environment  We need a zip of local environment that includes all dependent libraries. This is because there is no way to specify requirements.txt like in Cloud Functions. This zip file can be uploaded to the bucket we created above. We start with creating a directory of all dependencies.\nmkdir lambda_model cd lambda_model # pip install pickle5 -t . #if we had a specific requirement, we would execute this line  First we will read the model and the movie.dat files (presumably in the parent directory) to create two new dictionaries:\n#Dumping the recommendations and movie info as dictionaries. from surprise import Dataset import pandas as pd import pickle5 import pickle import json def load(file_name): dump_obj = pickle5.load(open(file_name, 'rb')) return dump_obj['predictions'], dump_obj['algo'] def get_top_n(predictions, n=10): # First map the predictions to each user. top_n = {} for uid, iid, true_r, est, _ in predictions: if uid not in top_n: top_n[uid] = [] top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n def defaultdict(default_type): class DefaultDict(dict): def __getitem__(self, key): if key not in self: dict.__setitem__(self, key, default_type()) return dict.__getitem__(self, key) return DefaultDict() df = pd.read_csv('../movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('../surprise_model') top_n = get_top_n(predictions, n=5) df = df.drop(['genre'],axis=1) movie_dict = df.T.to_dict() pickle.dump(movie_dict,open('movie_dict.pkl','wb')) pickle.dump(top_n,open('top_n.pkl','wb'))  Next we will create the following file called lambda_function.py in this directory:\nimport json # top_n = {'196':[(1,3),(2,4)]} # movie_dict = {1:{'name':'a'},2:{'name':'b'}} def lambda_handler(event,context): data = {\u0026quot;success\u0026quot;: False} with open(\u0026quot;top_n.json\u0026quot;, \u0026quot;r\u0026quot;) as read_file: top_n = json.load(read_file) with open(\u0026quot;movie_dict.json\u0026quot;, \u0026quot;r\u0026quot;) as read_file: movie_dict = json.load(read_file) print(event) #debug if \u0026quot;body\u0026quot; in event: event = event[\u0026quot;body\u0026quot;] if event is not None: event = json.loads(event) else: event = {} if \u0026quot;uid\u0026quot; in event: data[\u0026quot;response\u0026quot;] = str([movie_dict.get(iid,{'name':None})['name'] for (iid, _) in top_n[event.get(\u0026quot;uid\u0026quot;)]]) data[\u0026quot;success\u0026quot;] = True return { 'statusCode': 200, 'headers':{'Content-Type':'application/json'}, 'body': json.dumps(data) }  Here instead of a request object in GCP, a pair (event,context) are taken as input. The event object will have the query values. See this for more details.\n Next we zip the model and its dependencies and upload to S3.\nzip -r recommend.zip . aws s3 cp recommend.zip s3://theja-model-store/recommend.zip aws s3 ls s3://theja-model-store/  Add the API Gateway as before and see the predictions in action!\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/pytorch_training/",
	"title": "Recommendation (Pytorch) Training",
	"tags": [],
	"description": "",
	"content": "Please install the package using the command conda install -c conda-forge scikit-surprise in the ight environment.\n# https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.accuracy import rmse from surprise.dump import dump import numpy as np import torch from torch import nn import torch.nn.functional as F from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator from ignite.metrics import Loss, MeanSquaredError from datetime import datetime from sklearn.utils import shuffle class Loader(): current = 0 def __init__(self, x, y, batchsize=1024, do_shuffle=True): self.shuffle = shuffle self.x = x self.y = y self.batchsize = batchsize self.batches = range(0, len(self.y), batchsize) if do_shuffle: # Every epoch re-shuffle the dataset self.x, self.y = shuffle(self.x, self.y) def __iter__(self): # Reset \u0026amp; return a new iterator self.x, self.y = shuffle(self.x, self.y, random_state=0) self.current = 0 return self def __len__(self): # Return the number of batches return int(len(self.x) / self.batchsize) def __next__(self): n = self.batchsize if self.current + n \u0026gt;= len(self.y): raise StopIteration i = self.current xs = torch.from_numpy(self.x[i:i + n]) ys = torch.from_numpy(self.y[i:i + n]) self.current += n return (xs, ys) def l2_regularize(array): loss = torch.sum(array ** 2.0) return loss class MF(nn.Module): itr = 0 def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0): super(MF, self).__init__() self.k = k self.n_user = n_user self.n_item = n_item self.c_bias = c_bias self.c_vector = c_vector self.user = nn.Embedding(n_user, k) self.item = nn.Embedding(n_item, k) # We've added new terms here: self.bias_user = nn.Embedding(n_user, 1) self.bias_item = nn.Embedding(n_item, 1) self.bias = nn.Parameter(torch.ones(1)) def __call__(self, train_x): user_id = train_x[:, 0] item_id = train_x[:, 1] vector_user = self.user(user_id) vector_item = self.item(item_id) # Pull out biases bias_user = self.bias_user(user_id).squeeze() bias_item = self.bias_item(item_id).squeeze() biases = (self.bias + bias_user + bias_item) ui_interaction = torch.sum(vector_user * vector_item, dim=1) # Add bias prediction to the interaction prediction prediction = ui_interaction + biases return prediction def loss(self, prediction, target): loss_mse = F.mse_loss(prediction, target.squeeze()) # Add new regularization to the biases prior_bias_user = l2_regularize(self.bias_user.weight) * self.c_bias prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias prior_user = l2_regularize(self.user.weight) * self.c_vector prior_item = l2_regularize(self.item.weight) * self.c_vector total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item return total def log_training_loss(engine, log_interval=400): epoch = engine.state.epoch itr = engine.state.iteration fmt = \u0026quot;Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\u0026quot; msg = fmt.format(epoch, itr, len(train_loader), engine.state.output) model.itr = itr if itr % log_interval == 0: print(msg) def log_validation_results(engine): evaluat.run(test_loader) metrics = evaluat.state.metrics avg_accuracy = metrics['accuracy'] print(\u0026quot;Epoch[{}] Validation MSE: {:.2f} \u0026quot; .format(engine.state.epoch, avg_accuracy))  #Data data = Dataset.load_builtin('ml-100k') trainset = data.build_full_trainset() uir = np.array([x for x in trainset.all_ratings()]) train_x = test_x = uir[:,:2].astype(np.int64) train_y = test_y = uir[:,2].astype(np.float32)  #Parameters lr = 1e-2 k = 10 #latent dimension c_bias = 1e-6 c_vector = 1e-6 batchsize = 1024 model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector) optimizer = torch.optim.Adam(model.parameters()) trainer = create_supervised_trainer(model, optimizer, model.loss) metrics = {'accuracy': MeanSquaredError()} evaluat = create_supervised_evaluator(model, metrics=metrics) train_loader = Loader(train_x, train_y, batchsize=batchsize) test_loader = Loader(test_x, test_y, batchsize=batchsize) trainer.add_event_handler(event_name=Events.ITERATION_COMPLETED, handler=log_training_loss) trainer.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=log_validation_results)  \u0026lt;ignite.engine.events.RemovableEventHandle at 0x7f011e8a67c0\u0026gt;  model  MF( (user): Embedding(943, 10) (item): Embedding(1682, 10) (bias_user): Embedding(943, 1) (bias_item): Embedding(1682, 1) )  trainer.run(train_loader, max_epochs=50)  Epoch[1] Validation MSE: 17.82 Epoch[2] Validation MSE: 16.02 Epoch[3] Validation MSE: 14.43 Epoch[4] Validation MSE: 13.04 Epoch[5] Iteration[400/97] Loss: 12.52 Epoch[5] Validation MSE: 11.79 Epoch[6] Validation MSE: 10.70 Epoch[7] Validation MSE: 9.71 Epoch[8] Validation MSE: 8.85 Epoch[9] Iteration[800/97] Loss: 8.95 Epoch[9] Validation MSE: 8.08 Epoch[10] Validation MSE: 7.40 Epoch[11] Validation MSE: 6.80 Epoch[12] Validation MSE: 6.27 Epoch[13] Iteration[1200/97] Loss: 6.50 Epoch[13] Validation MSE: 5.79 Epoch[14] Validation MSE: 5.36 Epoch[15] Validation MSE: 4.98 Epoch[16] Validation MSE: 4.63 Epoch[17] Iteration[1600/97] Loss: 4.80 Epoch[17] Validation MSE: 4.32 Epoch[18] Validation MSE: 4.04 Epoch[19] Validation MSE: 3.79 Epoch[20] Validation MSE: 3.56 Epoch[21] Iteration[2000/97] Loss: 3.35 Epoch[21] Validation MSE: 3.35 Epoch[22] Validation MSE: 3.15 Epoch[23] Validation MSE: 2.97 Epoch[24] Validation MSE: 2.81 Epoch[25] Iteration[2400/97] Loss: 2.73 Epoch[25] Validation MSE: 2.66 Epoch[26] Validation MSE: 2.53 Epoch[27] Validation MSE: 2.40 Epoch[28] Validation MSE: 2.28 Epoch[29] Iteration[2800/97] Loss: 2.31 Epoch[29] Validation MSE: 2.17 Epoch[30] Validation MSE: 2.07 Epoch[31] Validation MSE: 1.97 Epoch[32] Validation MSE: 1.89 Epoch[33] Iteration[3200/97] Loss: 1.82 Epoch[33] Validation MSE: 1.81 Epoch[34] Validation MSE: 1.73 Epoch[35] Validation MSE: 1.66 Epoch[36] Validation MSE: 1.60 Epoch[37] Validation MSE: 1.54 Epoch[38] Iteration[3600/97] Loss: 1.60 Epoch[38] Validation MSE: 1.48 Epoch[39] Validation MSE: 1.43 Epoch[40] Validation MSE: 1.38 Epoch[41] Validation MSE: 1.34 Epoch[42] Iteration[4000/97] Loss: 1.27 Epoch[42] Validation MSE: 1.29 Epoch[43] Validation MSE: 1.25 Epoch[44] Validation MSE: 1.22 Epoch[45] Validation MSE: 1.19 Epoch[46] Iteration[4400/97] Loss: 1.11 Epoch[46] Validation MSE: 1.15 Epoch[47] Validation MSE: 1.13 Epoch[48] Validation MSE: 1.10 Epoch[49] Validation MSE: 1.07 Epoch[50] Iteration[4800/97] Loss: 1.11 Epoch[50] Validation MSE: 1.05 State: iteration: 4850 epoch: 50 epoch_length: 97 max_epochs: 50 output: 1.0818936824798584 batch: \u0026lt;class 'tuple'\u0026gt; metrics: \u0026lt;class 'dict'\u0026gt; dataloader: \u0026lt;class '__main__.Loader'\u0026gt; seed: \u0026lt;class 'NoneType'\u0026gt; times: \u0026lt;class 'dict'\u0026gt;  torch.save(model.state_dict(), \u0026quot;./pytorch_model\u0026quot;)   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/pytorch_inference/",
	"title": "Recommendation (Pytorch) Inference",
	"tags": [],
	"description": "",
	"content": "from surprise import Dataset import numpy as np import torch from torch import nn import pandas as pd class MF(nn.Module): itr = 0 def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0): super(MF, self).__init__() self.k = k self.n_user = n_user self.n_item = n_item self.c_bias = c_bias self.c_vector = c_vector self.user = nn.Embedding(n_user, k) self.item = nn.Embedding(n_item, k) # We've added new terms here: self.bias_user = nn.Embedding(n_user, 1) self.bias_item = nn.Embedding(n_item, 1) self.bias = nn.Parameter(torch.ones(1)) def __call__(self, train_x): user_id = train_x[:, 0] item_id = train_x[:, 1] vector_user = self.user(user_id) vector_item = self.item(item_id) # Pull out biases bias_user = self.bias_user(user_id).squeeze() bias_item = self.bias_item(item_id).squeeze() biases = (self.bias + bias_user + bias_item) ui_interaction = torch.sum(vector_user * vector_item, dim=1) # Add bias prediction to the interaction prediction prediction = ui_interaction + biases return prediction def loss(self, prediction, target): loss_mse = F.mse_loss(prediction, target.squeeze()) # Add new regularization to the biases prior_bias_user = l2_regularize(self.bias_user.weight) * self.c_bias prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias prior_user = l2_regularize(self.user.weight) * self.c_vector prior_item = l2_regularize(self.item.weight) * self.c_vector total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item return total def get_top_n(model,testset,trainset,uid_input,n=10): preds = [] try: uid_input = int(trainset.to_inner_uid(uid_input)) except KeyError: return preds # First map the predictions to each user. for uid, iid, _ in testset: #inefficient try: uid_internal = int(trainset.to_inner_uid(uid)) except KeyError: continue if uid_internal==uid_input: try: iid_internal = int(trainset.to_inner_iid(iid)) movie_name = df.loc[int(iid),'name'] preds.append((iid,movie_name,float(model(torch.tensor([[uid_input,iid_internal]]))))) except KeyError: pass # Then sort the predictions for each user and retrieve the k highest ones if preds is not None: preds.sort(key=lambda x: x[1], reverse=True) if len(preds) \u0026gt; n: preds = preds[:n] return preds  #Data df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) print(df.head()) data = Dataset.load_builtin('ml-100k') trainset = data.build_full_trainset() testset = trainset.build_anti_testset()   name genre iid 1 Toy Story (1995) Animation|Children's|Comedy 2 Jumanji (1995) Adventure|Children's|Fantasy 3 Grumpier Old Men (1995) Comedy|Romance 4 Waiting to Exhale (1995) Comedy|Drama 5 Father of the Bride Part II (1995) Comedy  #Parameters lr = 1e-2 k = 10 #latent dimension c_bias = 1e-6 c_vector = 1e-6 model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector) model.load_state_dict(torch.load('./pytorch_model')) model.eval()  MF( (user): Embedding(943, 10) (item): Embedding(1682, 10) (bias_user): Embedding(943, 1) (bias_item): Embedding(1682, 1) )  # Print the recommended items for each user limit = 0 for uid,_,_ in testset: print('\\nUser:',uid) seen = [df.loc[int(iid),'name'] for (iid, _) in trainset.ur[int(uid)]] if len(seen) \u0026gt; 10: seen = seen[:10] print('\\tSeen:',seen) print('\\tRecommendations:',get_top_n(model,testset,trainset,uid,n=10)) limit+=1 if limit\u0026gt;3: break  User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)'] User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)'] User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)'] User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)']  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/intro/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " TLDR Problem  Environmental compatibility is a headache, in addition to scaling, security, maintenance and upgrade of software and hardware.  For instance, in the serverless examples, the need for pickle5 instead of pickle was due to such incompatibility.  For hosted environments, we have to work hard on the devops to ensure the environments are the same. For serverless, we did this via the requirements file (Cloud Functions) and locally installing python packages (Lambda functions)  Solution  Containers\n they allow you to run arbitrary programs on new environments as long as they have a common piece of software (e.g., docker).  Their use cases include:\n Webapp deployments (real time dashboards) Model deployments (our objective) Packaging up projects (like a snapshot) and archiving so you can return to the project with the runtime/environment all set to go.   Containers  Containers are tools to produce reproducible environments by isolating and packaging everything that your program needs, independent of the target/host environment where you intended to run the program. Many tasks can be isolated and served in such a way, including model deployments. A Container is a process with features to keep it isolated from the host system. Because of their architecture, they are much lighter on resources compared to a full blown VM. You could think of them as stripped down virtual machines (VMs). Obviously this comes with some trade-offs as well. Containers with our code/program/model in it can be distributed to team members and can be publicly released with minimal environment incompatibility issues. One key variant of this solution is Docker. There are others, but we will stick with this one. In summary, they are useful because:  consistency: in app development as well as its delivery portable: local machine or cloud lightweight: multiple containers on single host   Elastic Container Service by AWS  A proprietary solution by AWS Elastic Container Service (ECS) similar to Lambda functions, and allows one to be agnostic to devops details (which server, server configs, security etc). Lifts certain restrictions of Lambda functions:  Not specific to the runtimes available in Lambda functions No memory limit (e.g., 256MB)  In ECS, we have to define the container explicitly, and then the rest (scaling, fault-tolerance) is behind the scenes.  Note: While exploring ECS, keep a tab on billing (check every so often)!\nWhat we will study  Docker local experimentation Run docker on EC2/AWS Use ECS (and Elastic Container Registry/ECR) on AWS The end goal is a functionality that is almost serverless, but has more work, potentially costing less and also allowing for more flexibility.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/",
	"title": "Lecture 3",
	"tags": [],
	"description": "",
	"content": " Serving ML Models Using Containers on AWS - Docker - AWS Elastic Container Service with Fargate "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/docker/",
	"title": "Docker",
	"tags": [],
	"description": "",
	"content": " We will first get learn a bit more about docker. From their website:\n Docker is an open platform (written in Go) for developing, shipping, and running applications.\nDocker enables you to separate your applications from your infrastructure so you can deliver software quickly.\nWith Docker, you can manage your infrastructure in the same ways you manage your applications.\nBy taking advantage of Docker’s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.\n Docker Overview  A loosely isolated environment (a container) can be created using Docker. We can run multiple containers ona host (like our operating system: Mac/Win/Linux) Containers are light on resources compared to VMs because they run directly with the host machine\u0026rsquo;s kernel.  Example, you may have heard of the linux OS kernel. A kernel is a computer program at the core of a computer\u0026rsquo;s operating system with complete control over everything in the system   Source: https://en.wikipedia.org/wiki/Kernel_(operating_system) \nDocker Engine  Is an app that is comprised of\n a server (dockerd) that handles creating containers an API to talk to the server a CLI (docker) that uses the API\n The client and the daemon they interact via the API to handle creation and deployment of containers.\n   Source: https://docs.docker.com/get-started/overview/ \nDocker Architecture Source: https://docs.docker.com/get-started/overview/ \n Docker registry: stores Docker images. E.g., Docker hub is a public registry. Docker objects:  Images Networks Containers Volumnes Plugins  Image:\n Read only template of instructions to create a container. It is often based on other images. Dockerfile creates images: slightly changing it does not mean rebuilding from scratch (lightweight)  When we run the following command:\n$ docker run -i -t ubuntu /bin/bash   Docker pulls the image from the registry Creates a container Allocates a private filesystem Creates a network inteface Executes /bin/bash because of the -i -t flag  Services: Docker (\u0026gt; 1.12) has the capability to make multiple daemons work together (as a swarm) with load balancing automatically handled.\n Container:\n A runnable instance of an image Several operations: create, start, stop, move, delete. When deleted, it does NOT store state! In contrast, a VM is a guest operating system that accesses host resources through a hypervisor.   Source: https://docs.docker.com/get-started/\nRunning a Container  Download Docker for Desktop from here. You will have to create a free account with them.\n Check version docker --version\n Run a hello world container: docker run hello-world\n   In the above, I changed to the appropriate conda env, although it is not necessary to do so. Its just a good practice.\n We can check the list of images we have locally via:\n(datasci-dev) ttmac:~ theja$ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest bf756fb1ae65 8 months ago 13.3kB  We can check the active/recently run containers via the following command:\n(datasci-dev) ttmac:~ theja$ docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 76cf92eaec76 hello-world \u0026quot;/hello\u0026quot; 4 minutes ago Exited (0) 4 minutes ago great_sinoussi  create a folder and copy the flask weather service app (weather.py) from before.\n(datasci-dev) ttmac:code theja$ mkdir docker-weather-service (datasci-dev) ttmac:code theja$ cd docker-weather-service/ (datasci-dev) ttmac:code theja$ cp ../lecture1/weather.py docker-weather-service/  Recall that running python weather.py and accessing it from the browser using the url http://localhost:5000/?msg=Chicago should give you a JSON response back.\n We will write a Dockerfile as below to build a custom image.\nFROM debian:buster-slim MAINTAINER Your Name RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y python3-pip python3-dev \\ \u0026amp;\u0026amp; cd /usr/local/bin \\ \u0026amp;\u0026amp; ln -s /usr/bin/python3 python \\ \u0026amp;\u0026amp; pip3 install flask geopy requests COPY weather.py weather.py ENTRYPOINT [\u0026quot;python3\u0026quot;,\u0026quot;weather.py\u0026quot;]  Building the custom image is achieved by the following commands:\ndocker image build -t \u0026quot;weather_service\u0026quot; . docker images #to check the recently built image  You should see the following output (truncated, only showing the last)\nStep 4/5 : COPY weather.py weather.py ---\u0026gt; 6ad721a3d5ef Step 5/5 : ENTRYPOINT [\u0026quot;python3\u0026quot;,\u0026quot;weather.py\u0026quot;] ---\u0026gt; Running in 70347eb72094 Removing intermediate container 70347eb72094 ---\u0026gt; ffd1e8b9172f Successfully built ffd1e8b9172f Successfully tagged weather_service:latest (datasci-dev) ttmac:docker-weather-service theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE weather_service latest ffd1e8b9172f 9 seconds ago 492MB debian buster-slim c7346dd7f20e 4 weeks ago 69.2MB hello-world latest bf756fb1ae65 8 months ago 13.3kB  We will run the newly created weather_service image as a container locally suing the docker run command (recall how we did the hello world example).\ndocker run -d -p 5000:5000 weather_service docker ps  Here, the -d -p flags are to make the container run as a daemon (i.e., not interactive with the user) and to do port forwarding.\n(datasci-dev) ttmac:docker-weather-service theja$ docker run -d -p 5000:5000 weather_service 50dbba727a24c9534f0a88165a0601f9447ec5e2e3a6c21fe8443c12cfac63e4 (datasci-dev) ttmac:docker-weather-service theja$ docker ps -all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 50dbba727a24 weather_service \u0026quot;python3 weather.py\u0026quot; 4 seconds ago Up 3 seconds 0.0.0.0:5000-\u0026gt;5000/tcp tender_shtern  Going to your browser and accessing http://localhost:5000/?msg=Chicago should give the appropriate weather response JSON object.\n The model serving python files (both pytorch and surprise based) can be deployed locally or on a VPS (EC2/Vultr/Digitalocean) similarly.\n Since hosting a container does not by itself give us additional features such as fault tolerance and scalability (being able to respond to many requests simultaneously), we will make use of ECS and GKE to address thee.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/ecr/",
	"title": "Orchestration using ECS and ECR - Part I",
	"tags": [],
	"description": "",
	"content": " Intro  Orchestration means managing container life cycle from building them to deploying (which requires provisioning of appropriate compute resources, storage resources, networking resources), scaling, load-balancing and other tasks, while accounting for failures throughout.\n While there are many orchestration solutions, we will focus on a couple of them: ECS by AWS and Kubernetes (local hosted solution and managed by GCP). While there is Elastic Kubernetes Service (EKS) by AWS as well, we will omit it here, as the ideas are the same.\nWhy should data science professions know such orchestration solutions?\n Pro: Get key devops features (fault tolerance, scalability etc) with low on-going effort. The deployed service will not likely break down. Con: There is a non-trivial setup cost/complexity.  Next, we will (a) deploy our model serving docker image to the AWS container registry ECR, (b) use ECS to deploy a container based on that image, and \u0026copy; set up a load balancer that mediates requests to the prediction model.\nElastic Container Registry (ECR)  Sending the image to a model registry such as ECR is needed to access it at other places to create containers. ECR allows for better integration with the AWS platform, and works with EKS as well. We will create a docker image locally (can also be done on EC2) and push it to an ECR repository that we will create.\n Navigate to the ECR link on the AWS console.\n   Click create a repository \u0026lsquo;Get Started\u0026rsquo; button. ECR can have multiple repositories and each repository can hold multiple images.   Give a name to the repository. It will contain multiple Docker images.   Review the current repository list.   Next we will allow the programmatic user we created for accessing S3 (we gave the user name model-user) to also manage the ECR repositories. Navigate to IAM as shown below.   Click on the user who we want to edit access controls for.   Currently this user had S3 access (not relevant for us).   Click on attaching existing policies and search for AmazonEC2ContainerRegistryFullAccess.   Review your policy choice and proceed.   You can see the summary of permissions that this programmatic user has.  Creating the Model Prediction Docker Image  We will use the following flask app that uses the pytorch model to serve recommendations:\nfrom surprise import Dataset import torch import pandas as pd import flask class MF(torch.nn.Module): def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0): super(MF, self).__init__() self.k = k self.n_user = n_user self.n_item = n_item self.c_bias = c_bias self.c_vector = c_vector self.user = torch.nn.Embedding(n_user, k) self.item = torch.nn.Embedding(n_item, k) # We've added new terms here: self.bias_user = torch.nn.Embedding(n_user, 1) self.bias_item = torch.nn.Embedding(n_item, 1) self.bias = torch.nn.Parameter(torch.ones(1)) def __call__(self, train_x): user_id = train_x[:, 0] item_id = train_x[:, 1] vector_user = self.user(user_id) vector_item = self.item(item_id) # Pull out biases bias_user = self.bias_user(user_id).squeeze() bias_item = self.bias_item(item_id).squeeze() biases = (self.bias + bias_user + bias_item) ui_interaction = torch.sum(vector_user * vector_item, dim=1) # Add bias prediction to the interaction prediction prediction = ui_interaction + biases return prediction def loss(self, prediction, target): loss_mse = F.mse_loss(prediction, target.squeeze()) # Add new regularization to the biases prior_bias_user = l2_regularize(self.bias_user.weight) * self.c_bias prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias prior_user = l2_regularize(self.user.weight) * self.c_vector prior_item = l2_regularize(self.item.weight) * self.c_vector total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item return total def get_top_n(model,testset,trainset,uid_input,n=10): preds = [] try: uid_input = int(trainset.to_inner_uid(uid_input)) except KeyError: return preds # First map the predictions to each user. for uid, iid, _ in testset: #inefficient try: uid_internal = int(trainset.to_inner_uid(uid)) except KeyError: continue if uid_internal==uid_input: try: iid_internal = int(trainset.to_inner_iid(iid)) movie_name = df.loc[int(iid),'name'] preds.append((iid,movie_name,float(model(torch.tensor([[uid_input,iid_internal]]))))) except KeyError: pass # Then sort the predictions for each user and retrieve the k highest ones if preds is not None: preds.sort(key=lambda x: x[1], reverse=True) if len(preds) \u0026gt; n: preds = preds[:n] return preds app = flask.Flask(__name__) #Data df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) data = Dataset.load_builtin('ml-100k',prompt=False) ''' Exercise: remove the above dependency. Currently it downloads data from grouplens website and stores in .surprise folder in $HOME ''' trainset = data.build_full_trainset() testset = trainset.build_anti_testset() #Parameters that are needed to reload the model from disk k = 10 #latent dimension c_bias = 1e-6 c_vector = 1e-6 model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector) model.load_state_dict(torch.load('./pytorch_model')) model.eval() #no need for gradient computations in this setting # define a predict function as an endpoint @app.route(\u0026quot;/\u0026quot;, methods=[\u0026quot;GET\u0026quot;]) def predict(): data = {\u0026quot;success\u0026quot;: False} # check for passed in parameters params = flask.request.json if params is None: params = flask.request.args if \u0026quot;uid\u0026quot; in params.keys(): data[\u0026quot;response\u0026quot;] = get_top_n(model,testset,trainset,params['uid'],n=10) data[\u0026quot;success\u0026quot;] = True # return a response in json format return flask.jsonify(data) # start the flask app, allow remote connections app.run(host='0.0.0.0', port=80)  The corresponding Dockerfile is below. The key additional files in addition to recommend.py above are:\n movies.dat pytorch_model\nFROM continuumio/miniconda3:latest RUN conda install -y flask pandas \\ \u0026amp;\u0026amp; conda install -c conda-forge scikit-surprise \\ \u0026amp;\u0026amp; conda install pytorch torchvision cpuonly -c pytorch COPY recommend.py recommend.py COPY movies.dat movies.dat COPY pytorch_model pytorch_model ENTRYPOINT [\u0026quot;python\u0026quot;,\u0026quot;recommend.py\u0026quot;]   The miniconda image above is from https://hub.docker.com/r/continuumio/miniconda3.\n Building an image based on the above file and running our prediction locally can be done using the following commands:\ndocker image build -t \u0026quot;prediction_service\u0026quot; . docker run -d -p 5000:5000 prediction_service docker ps -a #check what all containers were/are running docker kill container_id #after checking that the service runs, we can safely stop and delete the container. docker rm container_id  If we run a container based on this image, the python file and others will be in the root (/) folder and will be run by the root user. While we will not improve this here, it is better to run services as non-root users.\n  Sending our Docker Image to ECR  We will follow the instruction here to push our image to the repository we just created.\n Assuming you have the aws CLI configured with the secret keys, run the following command:\naws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com  Substitute region with us-east-1 etc (check the URL on the ECR page) as well as aws_account_id with the actual account id. We should get a prompt saying \u0026lsquo;Login Succeeded\u0026rsquo;.\n Lets tag our image before sending it to ECR (replace account id and region below as well):\n(datasci-dev) ttmac:docker-prediction-service theja$ docker tag prediction_service aws_account_id.dkr.ecr.region.amazonaws.com/models:recommendations (datasci-dev) ttmac:docker-prediction-service theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE aws_account_id.dkr.ecr.region.amazonaws.com/recommendations pytorch_model 364179b27eb1 21 minutes ago 2.06GB weather_service latest 20d340f941c0 2 days ago 496MB debian buster-slim c7346dd7f20e 5 weeks ago 69.2MB continuumio/miniconda3 latest b4adc22212f1 6 months ago 429MB hello-world latest bf756fb1ae65 8 months ago 13.3kB  Pushing to ECR is achieved by the following:\ndocker push aws_account_id.dkr.region.amazonaws.com/recommendations:pytorch_model  You should see the update progress (this is a large upload!)\nThe push refers to repository [aws_account_id.dkr.ecr.us-east-2.amazonaws.com/recommendations] a5649bbe3e5f: Pushed 5c87fc4d582f: Pushed e1e8d92205bf: Pushed 5c6c81390816: Pushing [=========================\u0026gt; ] 848.8MB/1.635GB fcd8d39597dd: Pushed 875120aa853c: Pushed f2cb0ecef392: Pushed  And the push conclusion:\n(datasci-dev) ttmac:docker-prediction-service theja$ docker push aws_account_id.dkr.ecr.us-east-2.amazonaws.com/recommendations:pytorch_model The push refers to repository [aws_account_id.dkr.ecr.us-east-2.amazonaws.com/recommendations] a5649bbe3e5f: Pushed 5c87fc4d582f: Pushed e1e8d92205bf: Pushed 5c6c81390816: Pushed fcd8d39597dd: Pushed 875120aa853c: Pushed f2cb0ecef392: Pushed pytorch_model: digest: sha256:af5dfaf227cd96c4ca8ca952c511fb4274c59d76574726462137bc7c4230be07 size: 1793  On the ECR page, if we look at the images in the recommendations repository, it will contain our recently uploaded image.\n   There is a friendly help box that details specific (to your account) commands for pushing images to ECR on the above page as well. You could use that as a guiding reference, or the help page.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/ecs/",
	"title": "Orchestration using ECS and ECR - Part II",
	"tags": [],
	"description": "",
	"content": " Elastic Container Service (ECS)  This is a AWS propreitary solution for container orchestration. There are three key concepts to work with this solution:  Service: Manages containers and relates them to EC2 machines as needed Task: Is a specific container Cluster: Is the environment of EC2 machines where containers live  The below diagram illustrates these relationships.  Source: https://aws.com/ \n We will set up a cluster and run a task/container and use a service to manage it. We will use Fargate for this exercise, although there are other more detailed ways to achieve the same end goals. Using Fargate will hide a lot of complexity, especially with privisioning the underlying EC2 instances.\n Lets start by getting to ECS.\n   Hit the get started button above or navigate to the clusters link on the left and then hit the get started button below.   Choose the custom container definition and hit configure.   Name the container, and point it to the ECR image URI. Specify the port to be 80 (this is what we choose in recommend.py). Then hit update.   We will keep the task definition to the default values.   Similarly we will retain the defaults for the service definition on the next page as well.   Name the cluster and then review the settings. Note that a lot is happening under the hood.   Review the settings as shown below:   The cluster gets created and you can see the status for all tasks change to green in due time.   Click view service and then click the Tasks tab at the center/bottom part of the screen.   Clicking on the task (there should only be one task listed) will show the public IP through which we can seek model predicitons.   Navigating to our browser and accessing the IP and making a query such as http://18.220.91.58/?uid=20 gives the following response.  Tear Down the ECS Cluster  Follow Step 7 onwards from this link: https://aws.amazon.com/getting-started/hands-on/deploy-docker-containers/  Essentially update the service to ensure that the number of tasks is 0. Then delete the service and stop the task and delete the task and task definition. Then delete the cluster itself. Check on the EC2, VPC and Load balancing pages if you are still consuming resources.   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "TBD\n"
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/gke/",
	"title": "Orchestration using GKE",
	"tags": [],
	"description": "",
	"content": " Google Kubernetes Engine by GCP  Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications A container is a basic building block in Kubernetes. Google Kubernetes Engine (GKE) a managed service for running Kubernetes, with key features such as security, scaling and multi-cluster support.  Note: While exploring GKE, keep a tab on billing (check every so often)!\nWhat we will Study  Kubernetes on Desktop Use GKE (Google Kubernetes Engine) on GCP for deploying the recommendation system  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/",
	"title": "Lecture 4",
	"tags": [],
	"description": "",
	"content": " Orchestration using Kubernetes - Kubernetes - Google Kubernetes Engine (GKE) "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture5/",
	"title": "Lecture 5",
	"tags": [],
	"description": "",
	"content": " TBD "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture6/",
	"title": "Lecture 6",
	"tags": [],
	"description": "",
	"content": " TBD "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture7/",
	"title": "Lecture 7",
	"tags": [],
	"description": "",
	"content": " TBD "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture8/",
	"title": "Lecture 8",
	"tags": [],
	"description": "",
	"content": " Online Experimentation - A/B testing: sample size considerations - Tackling bandit feedback "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Deploy model on Heroku.\n Set up your custom domain to point to your VPS.\n Repeat the setup on AWS, GCP, DigitalOcean or any other VPS of your choice.\n Read the documentation for flask, mlflow, pytorch, surprise, pandas.\n Replace Flask with Django and Starlette.\n Read up about function decorators in Python.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Find out how serverless technologies work behind the scene.\n Connect your custom domain to the GCP Cloudn Function and the API Gateway/Lambda function in AWS.\n Learn command line tools for GCP and the difference between programmatic access and manual access.\n Learn about identities, roles and access aspects in GCP and AWS.\n Try deploying a different recommendation model.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Deploy your image to Docker Hub container registry (needs an account, has free tier limits).\n Run a container using the python images from Docker Hub.\n Try to minimize the size of the docker images produced.\n Add checks for out of bound queries in your recommendation function (e.g., http://localhost/?uid=2000 will give a value error on the server and the browser will show that an internal server error occured).\n Add a load balancer to the ECS deployment and study what it does.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/",
	"title": "MLOps: Operationalizing Machine Learning",
	"tags": [],
	"description": "",
	"content": " Operationalizing Machine Learning (IDS594) Note: Also known as ML Deployment in the course catalog.\nThis practice-oriented course surveys modern best practices around getting machine learning (ML) models into production. It continues where IDS 572 and IDS 575 left off, which is to learn multiple ways of operationalizing machine learning work flows and models in the context of the larger business end-goals. The course is complementary to IDS 561. We will gain a better understanding of strategies for model management, monitoring and deployment. We will also intertwine these topics with online experimentation techniques (A/B testing) and software engineering ideas such as version control, containerization, and continuous integration/continuous deployment.\nA tentative list of topics is as follows:\n Deploying ML models using web servers Containers for machine learning: the docker ecosystem and Kubernetes Git, CI/CD and their modifications for ML workflows A/B Testing of KPIs and data considerations Model management: model tracking and logging Case studies: Databricks\u0026rsquo; MLFlow, Google\u0026rsquo;s TFX/Kubeflow, Uber’s Michelangelo, Facebook\u0026rsquo;s FBLearner Flow.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]