[
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/",
	"title": "Course Logistics",
	"tags": [],
	"description": "",
	"content": " Course Logistics  Semester: Fall 2020 Lectures: Thursdays 6.30 PM to 9.00 PM Mode: Online synchronous (i.e., location is online). The course will be delivered over Zoom (an invite will be sent before the first day of class). See the online learning page for basic technology requirements. Staff  Instructor: Dr. Theja Tulabandhula (netid: theja) Teaching Assistant: Tengteng Ma (netid: tma24)  Communication: via slack, zoom and one note class notebook. Office hours: online via slack and zoom.  Textbook and Materials  Data Science in Production by Ben Weber (2020, $5 for the ebook/pdf). A sample of the first three chapters is available at the publishers page linked here.  Software  Any OS should be okay. If in doubt, run a virtual machine running linux (this will be discussed in the class). Some of the software we will work with are: Containers  Docker for Desktop kubectl and minikube  Python (Anaconda)  flask requests pandas pytorch scikit-learn matplotlib \u0026hellip;  Command Line Utilities (CLIs) from AWS, Google Cloud etc. \u0026hellip;  Hardware  There will be varied computing resources needed for this course. Try using a virtual machine with linux on your own computer if possible. A Windows virtual desktop is available at desktop.uic.edu if needed. You can refer to these two help pages to get started.  Assignments  There are no graded assignments or exams for this course. Students are expected to go over the lectures and practice the use of technologies discussed each week.  Project  Students are expected to apply what they learn in the course and demonstrate a deployment of an existing machine learning model they have access to. A suitable documentation of this process along with the scripts/codes/commands used is to be submitted on October 14th (with no exceptions). The evaluation criteria and other details will be released shortly. Submission deadline is BEFORE 11.59 PM on the concerned day. Late submissions will have an automatic 20% penalty per day. Use Blackboard for uploading your work as a single zip file.  Grade  Grades will be assigned based on the project (see project evaluation criteria above) (80%) and course participation (20%).  Miscellaneous Information  This is a 2 credit graduate level course offered by the Information and Decision Sciences department at UIC. See the academic calendar for the semester timeline. Students who wish to observe their religious holidays (http://oae.uic.edu/religious-calendar/) should notify the instructor within one week of the first lecture date. Contact the instructor at the earliest, if you require any accommodations for access to and/or participation in this course. Refer to the academic integrity guidelines set by the university.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/",
	"title": "Lecture 1",
	"tags": [],
	"description": "",
	"content": " Web Servers - SSH and Firewall - Conda Environments - Jupyter - Making requests and processing responses - Model Persistence using MLFlow - Serving a model using Flask "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/online_technology_requirements/",
	"title": "Online Learning Details",
	"tags": [],
	"description": "",
	"content": " Online Learning Details To maximize the learning experience, it will be good if students can meet the following basic technology requirements:\n At a minimum, students should have a device and an internet connection. A microphone, and a webcam would be highly recommended. See the Basic Technology Requirements link for more details.\n Laptop, Chromebook or Desktop Computer: Note that Chromebooks are used to perform a variety of browser-based tasks with most data and applications, such as Blackboard Learn, Blackboard Collaborate, Google Docs, and Office 365, residing in the cloud rather than on the machine itself. This can result in somewhat reduced functionality, depending on your needs. If you do not have reliable access to a computer at home, ACCC may have a laptop to lend to you. Please fill out our request form at accc.uic.edu/forms/laptop-request\n Internet: Many service providers are offering connectivity solutions for students without access to Wi-Fi or the internet. The Illinois Citizens Utility Board is maintaining a comprehensive list of the available options here: citizensutilityboard.org/blog/2020/03/19/cubs-guide-utility-services-during-the-covid-19-public-health-emergency.\n The State of Illinois is maintaining a map of publicly available internet hotspots across the state that can be used for academic-related needs. These hotspots are available from within a parked vehicle. The map, and additional information, can be viewed at www.ildceo.net/wifi.\n Additionally, the ACCC has a very limited supply of cellular hotspots available for those students who are unable to take advantage of the above offers. Please fill out our request form at accc.uic.edu/forms/laptop-request/.\n  Microphone: While this may be built into your computer, we recommend using an external device such as a USB microphone or headset.\n Webcam: A built-in camera may be installed on your laptop; if not, you can use an external USB camera for video conferencing.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/schedule/",
	"title": "Schedule",
	"tags": [],
	"description": "",
	"content": " Textbook  Data Science in Production by Ben Weber (2020, $5 for the ebook/pdf). A sample of the first three chapters is available at the publishers page linked here.  Lecture Schedule Lecture 1: Serving ML Models Using Web Servers Reference: Chapter 2  Learning Goals:  Be able to set up a Python environment Be able to set up a jupyter session with SSH tunneling Be able to secure a web server Be able to use Flask to serve a ML model   Lecture 2: Serving ML Models Using Serverless Infrastructure Reference: Chapter 3  Learning Goals:  Be able to differentiate hosted vs managed solutions Assess deops effort for web server vs serverless deployments Be able to deploy a ML model using Google Cloud Functions and AWS Lambda Functions   Lecture 3: Serving ML Models Using Docker Reference: Chapter 4, upto 4.2  Learning Goals:  Be able to reason the pros and cons of container technologies Be able to differentiate containers from virtual machines Be able to create a new Docker image using Dockerfile Be able to upload the image to a remote registry   Lecture 4: Kubernetes for Orchestrating ML Deployments Reference: Chapter 4, 4.3 onwards  Learning Goals:  Understand the uses of Kubernetes Be able to set up a single node Kubernetes cluster using kubectl and minicube Be able to serve a prediction model on a container in the Kuebernetes cluster Be able to deploy a prediction model on Google Kubernetes Engine (GKE)   Lecture 5: ML Model Pipelines Reference: Chapter 5  Learning Goals:  Learn how to manage a sklearn workflow Learn how to set up automated jobs using cron Learn the basics of Apache Airflow Learn what a managed workflow tool is   Lecture 6: Reference: Chapter 6  Learning Goals:  TBD   Lecture 7: Reference: Chapter 7  Learning Goals:  TBD   Lecture 8: Online Experimentation Reference: 1: https://help.optimizely.com/Get_Started/Get_started_with_Optimizely_Web_Recommendations Reference: 2: https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html  Learning Goals:  TBD   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/logistics/project_instructions/",
	"title": "Project",
	"tags": [],
	"description": "",
	"content": " Aim  The aim of the project is to simulate the real-world process of deploying machine learning models. More specifically, the project component of this course allows you to explore a technology that assists in model deployment, either directly or indirectly, and asks you to report your experience working with that technology (or multiple technologies) to achieve some overall deployment goal.  Group  You should form groups of 4 students for this project component (this is a strict requirement). Reach out to your classmates early. Because this is a group project, a commensurate effort is expected, and each members contributions needs to be reported in the final submission.  Project Outcomes  There is one due-date for the project deliverables. See the course logistics page for the exact date. The deliverables are as follows. Project Report: In at most 8 pages (12 point, single column; you can have an appendix for supplementary material that may or may not be checked), you should explain your contributions in the project. Code and data: Code associated with the project (e.g., Jupyter notebooks), a small sample of the data/inputs/outputs if needed, and all steps necessary to replicate your project should be provided along with/in the report. A link to your github/gitlab/bitbucket/other repository is acceptable here (provide it at the front page of the report). A video presentation: You should provide a 10 minute video walk-through (discussing highlights) of your project and provide the link (say from Youtube where the video can be in unlisted mode) on the front page of the report.  Each team should upload the report (and code and video link) to Blackboard before the deadline.\nExample Report Components  For example, here are some aspects to focus on in your project report:  what was the goal what were the possible solutions what were the specific pros and cons from a business point of view a cost benefit analysis actual handling of the technology and demonstration in a dev environment documenting the experience lessons learned code artifacts and/or Jupyter notebooks \u0026hellip;  Here is an example project idea: try out a technology (or a specific aspect of it) and its competitors by following their documentation in a very extensive and well thought out manner (e.g., MLFlow vs bentoml vs cortex).  Grading Rubric  Projects will be graded based on the creativity shown in handling the technology and the insights drawn. The reports should be very clearly written and presented, and will be evaluated based on the correctness, content, creativity and clarity:  Correctness will be assessed based on the correct application of a technology, valid software setup and discussion of choices, technical correctness and the assumptions laid out. Content will be assessed based on the contributions made in the project (given group size) and project depth (e.g., why this aspect of ML deployment, why this problem, what did you do, visualization and interesting conclusions, insights, discussion of methodology followed). You should try to demonstrate your understanding of the relevant topics and their use in your non-trivial project. Creativity will be assessed based on how no-obvious your solution or contribution is and how different choices were thoughtfully made in the execution of the project. Clarity will be assessed based on the language quality, layout and structure of the report, the adequacy of the references cited, the capability of the team in explaining ideas in a clear and professional manner, and the clarity demonstrated in your discussions etc.  All external material/sources (code/idea/theory/insights) used should be cited prominently without failure. Use of pre-trained models, databases, web servers, front-end frameworks, visualization tools etc for your project is allowed and encouraged. This project cannot be used as part of any other course or requirement.  Additional Pointers  Keep track of costs especially if you are using services that require having a payment mode on file. Also, try to use free resources as much as possible. Do not train deep networks from scratch if it can be avoided. The project should not be centered around model accuracy. It is importantly to make a project plan that allocates sufficient tasks for each team member. It will be great if you can submit the project plan (a Gantt chart for example).  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/intro/",
	"title": "Basics",
	"tags": [],
	"description": "",
	"content": " Python  We will be predominantly concerned with the Python ecosystem A big advanage is that local system development can be easily moved to cloud and or a scalable on-prem solution. Many companies use python to start data science projects in-house (via fresh recruits, interns etc) Python has some relatively easy ways to access databases Big data platforms such as Spark have great python bindings  E.g., Pandas dataframe and Spark dataframe  Latest models (deep learning, pre-trained) are built in the python ecosystem Many many useful libraries: pandas, matplotlib, flask,\u0026hellip;  Our Objective  Learn the patterns, not the specific tools  Deployment Targets  Local machines On-prem or self-hosted machines (needs DevOps skills) Managed cloud  Heroku (PAAS) Azure GCP AWS (IAAS)  The decision to deply on one versus the other depends on  skills business need internal vs external scale, reliability, security costs ease of deployment   Local Deployments are Hard  Need to learn linux security Need to learn how to manage access Need for learn backups Need to learn hot switching / reliability  Cloud Deployments are not Easy  Also need to learn a complex ecosystem Vendor lock-in (for successful businesses, this is not an issue)  Aside: Software Tools Python development can happen:\n In text editors (e.g., sublime-text) In IDEs (e.g., Pycharm or VSCode) In Jupyter notebooks and variants (Google Colab, Databricks notebooks)  vanilla notebook does not allow collaboration as such   Part 1: Setting up Jupyter access on a VPS  We will use Vultr, but all steps are vendor agnostic. Alternatives include: Digitalocean, AWS EC2, Google Cloud; using Google Colab and other vendors. SSH passwordless access is set up. Next, we set up a basic firewall for security. This is followed by installing conda. (Optional) To run the jupyter server uninterrupted, we will run it within a screen session. We will access the server and notebooks on our local browser using SSH tunneling.  Part 2: Preparing an ML Model  We will show how data is accessed, and how the model is trained (this should be familiar to you).\n In particular, we will look at the moive recommendation problem.  There are aspects of saving and loading models that become important in production. For instance, we would like the models to be able to live across dev/staging/prod environments. For this, we think of the notion of model persistence\n Natively:\n For example, pytorch has native save and load methods. Same is the case for scikit-learn and a variety of other packages.  Using MLFlow:\n MLFlow addresses the problem of moving models across different environments without issues of incompatibility (minor version numbers, OS etc) among other things. See these links for more information: https://pypi.org/project/mlflow/ and mlflow.org    "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/ssh_and_firewall/",
	"title": "SSH and Firewall",
	"tags": [],
	"description": "",
	"content": " It is important to secure your connection to the machine. In order to do so, we will configure the ssh access pattern as well as set up a firewall that blocks all incoming requests except ssh port and web server ports.\nWe will assume that we have a non-root account that is in the sudoers group.\nSSH  When you first create the server instance, you may or may not have the ssh server running. If it is not running, you can install it first. On Ubuntu/Debian, you can use the following command:\nsudo apt install openssh-server  Next, we create a ssh key pair on our local machine with which we will access the server. From your local user home directory:\nmkdir .ssh ssh-keygen cd .ssh less id_rsa.pub  Copy this content to the following file authorized_keys in the webserver:\nmkdir .ssh vim authorized_keys #if vim is not present, you can use other editors or install it using `sudo apt install vim` #copy the content and quit (shift+colon\u0026gt; wq -\u0026gt; enter) chmod 600 authorized_keys  We need to edit the following fields in the file /etc/ssh/sshd_config on the server (say using vim):\n Port choose something other than 22 (opttional) PermitRootLogin no (changed from prohibit-password) PubkeyAuthentication yes (already defaults to this) PasswordAuthentication no (disable it for security)  Restart the ssh server. In Ubuntu/Debian this is achieved by sudo systemctl restart ssh\n  Firewall  A basic firewall such as ufw can help provide a layer of security. Install and run it using the following commands (Ubuntu/Debian):\nsudo apt install ufw sudo ufw allow [PortNumber] #here it is 22 or another port that you chose for ssh sudo ufw enable sudo ufw status verbose #this should show what the firewall is doing   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/conda/",
	"title": "Setting up Python",
	"tags": [],
	"description": "",
	"content": " Here are a few notes on installing a user specific python distribution:\nGet Miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh conda install pip #better to use the pip in the base conda env than system pip   The difference between conda and pip: pip is a package manager specifically for python, whereas conda is a package manager for multiple languages as well as is an environment manager. Python module venv is python specific environment manager.  Set up a conda environment and activate it conda create --name datasci-env python #or conda create -n dataeng-env python jupyter pandas numpy matplotlib #or conda create -n datasci-env scipy=0.15.0 #or conda env create -f environment.yml conda activate datasci-env   You don\u0026rsquo;t have to give names, can give prefixes where the env is saved, can create based on specific pages, can use explicit previous conda environments, yaml files, clone/update an existing one, etc. Use this link to get more information.\n Specifying a path to a subdirectory of your project directory when creating an environment can keep everything 100% self contained.\n To deactivate this environment, use conda deactivate datasci-env.\n  Install jupyter and pytorch (and tensorflow, keras, scikit-learn similarly) in a specific environment conda install jupyter conda install pytorch torchvision cpuonly -c pytorch # https://pytorch.org/   Change the command for pytorch installation if you do intend to use GPUs. In particular, install CUDA from conda after installing the latest NVidia drivers on the instance.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/jupyter/",
	"title": "Remote Jupyter Server",
	"tags": [],
	"description": "",
	"content": "The following sets a simple password based login, which is handy:\njupyter notebook --generate-config jupyter notebook password  Unfortuantely, hashed password is sent unencrypted by your browser here. So read up here to do this in a better way.\nStarting jupyter on the server can be done inside a screen session:\nscreen -S jupyter-session #can also use nohup or tmux here jupyter notebook --no-browser --port=8888  SSH tunnel can be setup by running the following on your local machine, and then opening the browser (http://localhost:8889)\nssh -N -f -L localhost:8889:localhost:8888 -p 22 theja@192.168.0.105  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/",
	"title": "Recommendation Models",
	"tags": [],
	"description": "",
	"content": "We will look at two models for recommending movies to existing users.\n Matrix factorization based on the surprise package. Matrix factorization based on Pytorch.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/surprise_training/",
	"title": "Recommendation (SVD) Training",
	"tags": [],
	"description": "",
	"content": "# https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.accuracy import rmse from surprise.dump import dump  # Load the movielens-100k dataset (download it if needed). data = Dataset.load_builtin('ml-100k') trainset = data.build_full_trainset() # Use an example algorithm: SVD. algo = SVD() algo.fit(trainset) # predict ratings for all pairs (u, i) that are in the training set. testset = trainset.build_testset() predictions = algo.test(testset) rmse(predictions) #actual predictions as thse items have not been seen by the users. there is no ground truth. # We predict ratings for all pairs (u, i) that are NOT in the training set. testset = trainset.build_anti_testset() predictions = algo.test(testset)  RMSE: 0.6774  dump('./surprise_model', predictions, algo)  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/deploy_webserver/",
	"title": "Serving ML Models Using Web Servers",
	"tags": [],
	"description": "",
	"content": " Model Serving  Sharing results with others (humans, web services, applications) Batch approach: dump predictions to a database (quite popular) Real-time approach: send a test feature vector, get back the prediction instantly and the computation happens now  How to consume from prediction services?  Using web requests (e.g., using a JSON payload)  How to output predictions?  We will plan to set up a server to serve predictions  It will respond to web requests (GET, POST) We pass some inputs (image, text, vector of numbers), and get some outputs (just like a function) The environment from which we pass inputs may be very different from the environment where the prediction happens (e.g., different hardware)   Our Objective  Use sklearn/keras with flask, gunicorn and heroku to set up a prediction server  Part 1: Making API Calls  Using the requests module from a jupyter notebook (this is an example of a programmatic way) Alternatively, using curl or postman (these are more versatile)  Part 2: Simple Flask App  Function decorators are used in Flask to achive routes to functions mapping. Integrating the model with the app is relatively easy if the model can be read from disk.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/surprise_inference/",
	"title": "Recommendation (SVD) Inference",
	"tags": [],
	"description": "",
	"content": "# https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.dump import load from collections import defaultdict import pandas as pd def get_top_n(predictions, n=10): \u0026quot;\u0026quot;\u0026quot;Return the top-N recommendation for each user from a set of predictions. Args: predictions(list of Prediction objects): The list of predictions, as returned by the test method of an algorithm. n(int): The number of recommendation to output for each user. Default is 10. Returns: A dict where keys are user (raw) ids and values are lists of tuples: [(raw item id, rating estimation), ...] of size n. \u0026quot;\u0026quot;\u0026quot; # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n  df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('./surprise_model')  Output:\n 0 1 2 0 1 Toy Story (1995) Animation|Children's|Comedy 1 2 Jumanji (1995) Adventure|Children's|Fantasy 2 3 Grumpier Old Men (1995) Comedy|Romance 3 4 Waiting to Exhale (1995) Comedy|Drama 4 5 Father of the Bride Part II (1995) Comedy algo: SVD  top_n = get_top_n(predictions, n=5) # Print the recommended items for each user limit = 0 for uid, user_ratings in top_n.items(): print('\\nUser:',uid) seen = [df.loc[int(iid),'name'] for (iid, _) in algo.trainset.ur[int(uid)]] if len(seen) \u0026gt; 10: seen = seen[:10] print('\\tSeen:',seen) print('\\tRecommendations:',[df.loc[int(iid),'name'] for (iid, _) in user_ratings]) limit+=1 if limit\u0026gt;3: break  Output:\nUser: 196 Seen: ['Richie Rich (1994)', 'Getaway, The (1994)', 'Batman Forever (1995)', 'Feast of July (1995)', 'Heidi Fleiss: Hollywood Madam (1995)', 'Shadows (Cienie) (1988)', 'Terminator 2: Judgment Day (1991)', \u0026quot;Nobody's Fool (1994)\u0026quot;, \u0026quot;Breakfast at Tiffany's (1961)\u0026quot;, 'Basic Instinct (1992)'] Recommendations: ['Age of Innocence, The (1993)', 'Bio-Dome (1996)', 'Strawberry and Chocolate (Fresa y chocolate) (1993)', 'Guardian Angel (1994)', \u0026quot;Carlito's Way (1993)\u0026quot;] User: 186 Seen: ['Double Happiness (1994)', 'Mr. Jones (1993)', 'War Room, The (1993)', 'Bloodsport 2 (1995)', 'Usual Suspects, The (1995)', 'Big Green, The (1995)', 'Mighty Morphin Power Rangers: The Movie (1995)', 'Boys on the Side (1995)', 'Cold Fever (� k�ldum klaka) (1994)', 'Sum of Us, The (1994)'] Recommendations: ['Lightning Jack (1994)', 'Robocop 3 (1993)', 'Walk in the Clouds, A (1995)', 'Living in Oblivion (1995)', 'Strawberry and Chocolate (Fresa y chocolate) (1993)'] User: 22 Seen: ['Assassins (1995)', 'Nico Icon (1995)', 'From the Journals of Jean Seberg (1995)', 'Last Summer in the Hamptons (1995)', 'Down Periscope (1996)', 'Bushwhacked (1995)', 'Beyond Bedlam (1993)', 'Client, The (1994)', 'Hoop Dreams (1994)', 'Ladybird Ladybird (1994)'] Recommendations: ['Home for the Holidays (1995)', 'Age of Innocence, The (1993)', 'Balto (1995)', 'City Hall (1996)', 'Ready to Wear (Pret-A-Porter) (1994)'] User: 244 Seen: ['When Night Is Falling (1995)', 'Birdcage, The (1996)', '8 Seconds (1994)', 'Foreign Student (1994)', 'Mighty Aphrodite (1995)', 'Before Sunrise (1995)', 'Lion King, The (1994)', 'Clockers (1995)', 'Underneath, The (1995)', 'Manny \u0026amp; Lo (1996)'] Recommendations: ['Century (1993)', 'Balto (1995)', 'Age of Innocence, The (1993)', 'Remains of the Day, The (1993)', 'Jimmy Hollywood (1994)']  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/flask/",
	"title": "Flask App",
	"tags": [],
	"description": "",
	"content": " Flask is a micro web framework written in Python. We first show how a simple service works, and then show how to load a model (e.g., based on pytorch) and serve it as well.\nWeather Reporting Service The key thing to see here are that the HTTP route / is mapped directly to a function weather. For instance, when someone hits localhost:5000 (5000 is the default unless specified in app.run() below) the function weather starts execution based on any received inputs.\n# load Flask import flask from flask import jsonify from geopy.geocoders import Nominatim import requests app = flask.Flask(__name__) # define a predict function as an endpoint @app.route(\u0026quot;/\u0026quot;, methods=[\u0026quot;GET\u0026quot;,\u0026quot;POST\u0026quot;]) def weather(): data = {\u0026quot;success\u0026quot;: False} #https://pypi.org/project/geopy/ geolocator = Nominatim(user_agent=\u0026quot;cloud_function_weather_app\u0026quot;) params = flask.request.json if params is None: params = flask.request.args # params = request.get_json() if \u0026quot;msg\u0026quot; in params: location = geolocator.geocode(str(params['msg'])) # https://www.weather.gov/documentation/services-web-api result1 = requests.get(f\u0026quot;https://api.weather.gov/points/{location.latitude},{location.longitude}\u0026quot;) result2 = requests.get(f\u0026quot;{result1.json()['properties']['forecast']}\u0026quot;) data[\u0026quot;response\u0026quot;] = result2.json() data[\u0026quot;success\u0026quot;] = True return jsonify(data) # start the flask app, allow remote connections if __name__ == '__main__': app.run(host='0.0.0.0')  This service can be run by using the command python weather.py (assuming that is the filename for the above script) locally. If the port 5000 is open, then this server will be accessible to the world if the server has an external IP address.\nModel Serving We can modify the above to serve the recommendation models we built earlier as follows:\nfrom surprise import Dataset from surprise.dump import load from collections import defaultdict import pandas as pd import flask def get_top_n(predictions, n=10): # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('./surprise_model') top_n = get_top_n(predictions, n=5) app = flask.Flask(__name__) # define a predict function as an endpoint @app.route(\u0026quot;/\u0026quot;, methods=[\u0026quot;GET\u0026quot;]) def predict(): data = {\u0026quot;success\u0026quot;: False} # check for passed in parameters params = flask.request.json if params is None: params = flask.request.args if \u0026quot;uid\u0026quot; in params.keys(): data[\u0026quot;response\u0026quot;] = str([df.loc[int(iid),'name'] for (iid, _) in top_n[params.get(\u0026quot;uid\u0026quot;)]]) data[\u0026quot;success\u0026quot;] = True # return a response in json format return flask.jsonify(data) # start the flask app, allow remote connections app.run(host='0.0.0.0')  You can use the following request in the browser http://0.0.0.0:5000/?uid=196 or use the requests module.\n"
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/",
	"title": "Lecture 2",
	"tags": [],
	"description": "",
	"content": " Serverless Deployments - Managed Solutions - Cloud Functions (GCP) - Lambda Functions (AWS) "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/serverless/",
	"title": "Serverless Deployments",
	"tags": [],
	"description": "",
	"content": " A. TLDR  Models do not need to be complex, but it can be complex to deploy models. - Ben Weber (2020)\n Problem  We have to take care of provisioning and server maintenance while deploying our models. We have to worry about scale: would 1 server be enough? How to minimize the time to deploy (at an acceptable increase in cost)? How can a single developer or data science/analytics professional manage a complex service?  Solution  Software that abstracts away server details and lets you serve your model (any function actually) with few lines of code/UI.\n The software automates\n prrovising servers scaling machines up and down  load balancing\n code versioning\n Our task is then to specify the requirements (pandas, pytorch).\n   B. Our Objective  Write serverless functions that generate predictions when they get triggered by HTTP requests. We will work with:  AWS GCP  We will deploy  a keras model, and a sklearn model.   C. Managed Services  Cloud is responsible for abstracting away various computing components: compute, storage, networking etc Minimizes thinking about dev/staging vs production. Note: what we did last class, ssh\u0026rsquo;ing into a virtual private server (VPS) would be considered as a hosted deployment, which is the opposite of managed deployment For example, serverless technology was introduced in 2015\u0026frasl;2016 by AWS (Amazon web) and GCP (Google cloud). It contrasts with VPS based deployment. Similarly, AWS ECS (Elastic Container Service) managed solution contrasts with the hosted/manual Docker on VPS setup.  When is a managed solution a bad idea?  No need for quick iteration (company cares about processes and protocols) Need a high speed service No need to scale system arbirarily Cost conscious or have an in-house developer.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions/",
	"title": "Cloud Functions",
	"tags": [],
	"description": "",
	"content": " Intro  Cloud Functions (CFs) are a solution from GCP for serverless deployments. Very little boilerplate beyond what we will write for simple offline model inference. In any such deployment, we need to be concerned about:  where the model is stored (recall pickle and mlflow), and what python packages are available.   Empty Deployment  We will set up triggers that will trigger our serving function (in particular, a HTTP request). We will specify the requirements needed for our python function to work The function we deploy here, similar to lecture 1, produces weather forecasts given a location.  Setting up using UI  Sign up with GCP if you haven\u0026rsquo;t already (typically you get a 300$ credit)\n Get to the console and find the Cloud Function page.\n   Go through the UI for creating a function.   We will choose the HTTP trigger and unauthenticated access option.   We may have to enable Cloud Build API   Finally, we choose the Python environment. You can see two default example files (main.py and requirements.txt). We will be modifying these two.  Python Files and Requirements  We will specify the following requirements:\nflask geopy requests  Our main file is the following:\ndef weather(request): from flask import jsonify from geopy.geocoders import Nominatim import requests data = {\u0026quot;success\u0026quot;: False} #https://pypi.org/project/geopy/ geolocator = Nominatim(user_agent=\u0026quot;cloud_function_weather_app\u0026quot;) params = request.get_json() if \u0026quot;msg\u0026quot; in params: location = geolocator.geocode(str(params['msg'])) # https://www.weather.gov/documentation/services-web-api # Example query: https://api.weather.gov/points/39.7456,-97.0892 result1 = requests.get(f\u0026quot;https://api.weather.gov/points/{location.latitude},{location.longitude}\u0026quot;) # Example query: https://api.weather.gov/gridpoints/TOP/31,80 result2 = requests.get(f\u0026quot;{result1.json()['properties']['forecast']}\u0026quot;) data[\u0026quot;response\u0026quot;] = result2.json() data[\u0026quot;success\u0026quot;] = True return jsonify(data)  Once the function is deployed, we can test the function (click actions on the far right in the dashboard)\n   We can pass the JSON string {\u0026quot;msg\u0026quot;:\u0026quot;Chicago\u0026quot;} and see that we indeed get the JSON output for the weather of Chicago.   We can also access the function from the web endpoint https://us-central1-authentic-realm-276822.cloudfunctions.net/function-1 (you will have a different endpoint). Note that unlike previous times, the request to this endpoint is a JSON payload.\n Below is the screen-shot of querying the weather of Chicago using the Postman tool. The way to use it is as follows:\n Insert the URL of the API Se the method type to POST Navigate to body and choose raw and then choose JSON from the dropdown menu. Now add the relevant parameters as a JSON string.    Finally, here is a query you can use from a Jupyter notebook.\nimport requests result = requests.post( \u0026quot;https://us-central1-authentic-realm-276822.cloudfunctions.net/function-1\u0026quot; ,json = { 'msg': 'Chicago' }) print(result.json()) #should match with https://forecast.weather.gov/MapClick.php?textField1=41.98\u0026amp;textField2=-87.9   Saving Model on the Cloud  For our original task of deploying a trained ML model, we need a way to read it from somewhere when the function is triggered.\n One way is to dump the model onto Google Cloud Storage (GCS)\n GCS is similar to the S3 (simple storage service) by AWS.\n We will use the command line to dump our model onto the cloud.\n  GCP access via the commandline  First we need to install the Google Cloud SDK from https://cloud.google.com/sdk/docs/downloads-interactive\ncurl https://sdk.cloud.google.com | bash gcloud init  There are two types of accounts you can work with: a user account or a service account (see https://cloud.google.com/sdk/docs/authorizing?authuser=2).\n Among others, [this page] gives a brief idea of why such an account is needed. In particular, we will create a service account (so that it can be used by an application programmatically anywhere) and store the encrypted credentials on disk for programmatic access through python. To do so, we run the following commands:\n We create the service account and check that it is active by using this command: gcloud iam service-accounts list.\ngcloud iam service-accounts create idsservice \\ --description=\u0026quot;IDS service account\u0026quot; \\ --display-name=\u0026quot;idsservice-displayed\u0026quot;  We then assign a new role for this service account in the project. The account can be disabled using the command gcloud iam service-accounts disable idsservice@authentic-realm-276822.iam.gserviceaccount.com (change idsservice and authentic-realm-276822 to your specific names).\ngcloud projects add-iam-policy-binding authentic-realm-276822 --member=serviceAccount:idsservice@authentic-realm-276822.iam.gserviceaccount.com --role=roles/owner  Finally, we can download the credentials\ngcloud iam service-accounts keys create ~/idsservice.json \\ --iam-account idsservice@authentic-realm-276822.iam.gserviceaccount.com  Once the credentials are downloaded, they can be programmatically accessed using python running on that machine. We just have to explore the location of the file:\nexport GOOGLE_APPLICATION_CREDENTIALS=/Users/theja/idsservice.json   Next we will install a python module to access GCS, so that we can write our model to the cloud:\npip install google-cloud-storage  The following code creates a bucket called theja_model_store\nfrom google.cloud import storage bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() storage_client.create_bucket(bucket_name) for bucket in storage_client.list_buckets(): print(bucket.name)  We can dump the model we used previous here using the following snippet\nfrom google.cloud import storage bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1a\u0026quot;) blob.upload_from_filename(\u0026quot;surprise_model\u0026quot;) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1b\u0026quot;) blob.upload_from_filename(\u0026quot;movies.dat\u0026quot;)   After running the above, the surprise package based recommendation model and the helper data file will be available at gs://theja_model_store/serverless/surprise_model/v1a and gs://theja_model_store/serverless/surprise_model/v1b as seen below.\nWe can either use the URIs above or use a programmatic way with the storage class. For example, here is the way to download the file v1b:\nfrom google.cloud import storage bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1b\u0026quot;) blob.download_to_filename(\u0026quot;movies.dat.from_gcp\u0026quot;)  We can diff it in Jupyter notebook itself using the expression !diff movies.dat movies.dat.from_gcp.\nWe will use this programmatic way of reading external data/model in the cloud function next.\n"
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions_model/",
	"title": "GCP Serverless Model Serving",
	"tags": [],
	"description": "",
	"content": " We modify the flask app that we had before, by again specifying the requirements.txt and the main python file appropriately. We will also increase the memory to 2GB and the timeout to 180 seconds. You will see that the following deployment has a lot of inefficiencies (can you spot the redundacy in loading the model and the predictions below?).\nThe requirements file will have the following entries:\nnumpy flask pandas google-cloud-storage scikit-surprise pickle5  The main file is also modified accordingly. Note that if we reload the model and the metadata on every request, it will be extremely inefficient. To fix that we can use global variables. This is not a good choice in much of python programming, but quite useful here. Essentially, global variables allow us to cache some of the objects, for faster response times.\ntop_n = None def recommend(request): global top_n from surprise import Dataset import pandas as pd import flask from google.cloud import storage import pickle5 def load(file_name): dump_obj = pickle5.load(open(file_name, 'rb')) return dump_obj['predictions'], dump_obj['algo'] def get_top_n(predictions, n=10): def defaultdict(default_type): class DefaultDict(dict): def __getitem__(self, key): if key not in self: dict.__setitem__(self, key, default_type()) return dict.__getitem__(self, key) return DefaultDict() # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n data = {\u0026quot;success\u0026quot;: False} params = request.get_json() if \u0026quot;uid\u0026quot; in params: if not top_n: bucket_name = \u0026quot;theja_model_store\u0026quot; storage_client = storage.Client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(\u0026quot;serverless/surprise_model/v1a\u0026quot;) blob.download_to_filename(\u0026quot;/tmp/surprise_model\u0026quot;) #ideally we should be reading things into memory blob = bucket.blob(\u0026quot;serverless/surprise_model/v1b\u0026quot;) blob.download_to_filename(\u0026quot;/tmp/movies.dat\u0026quot;) df = pd.read_csv('/tmp/movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('/tmp/surprise_model') top_n = get_top_n(predictions, n=5) data[\u0026quot;response\u0026quot;] = str([df.loc[int(iid),'name'] for (iid, _) in top_n[params.get(\u0026quot;uid\u0026quot;)]]) data[\u0026quot;success\u0026quot;] = True # return a response in json format return flask.jsonify(data)  We can test the function on the GCP console with a request JSON {\u0026quot;uid\u0026quot;:\u0026quot;206\u0026quot;}.\nAs an exercise, think of ways to make the whole setup above lightweight in terms of model and data size.\nDeploying the pytorch model after removing the dependecy on surprise is also a good challenge to tackle.\nAccess Management  It is a bad idea to allow unauthenticated access There are ways of restricting who can reach this model serving endpoint.  One way is to disable unauthenticated access while creating the function. Once we do that, we can create a permission structure based on GCP best practices.  We will omit the details here.  Updating Models and Monitoring  We can easily update our endpoint by rewriting the files we uploaded to GCS. A cleaner way is to create another cloud function. The function itself can have logic to reload files using the google-cloud-storage module based on time elapsed (say using datetime). Monitoring is very important, not just to know if our model performance has changed over time, but also to measure things such as latency, bugs and API access patterns. We will revisit this topic in the future.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions/",
	"title": "Lambda Functions",
	"tags": [],
	"description": "",
	"content": "  Lambda by Amazon Web Services (AWS) is an analogous serverless solution. Lambda can be used internall as well as for model deployments (we are focusing on the latter). We will repeat setting up the weather app and the recommender model, using the CLI (command line interface tools)  Aside: Setting up an IAM user  TBD  Hello World in Lambda  Select the lambda service.   Pick the python 3.7 runtime.   You will see the green bar indicating a successful creation.   We will initially not changed the code.   And create a dummy test.   The result looks like this.  Setting up an API Gateway  We will slightly change the function so that it forms a proper HTTP response.   Under the Designer section, we will \u0026lsquo;Add Trigger\u0026rsquo; and select \u0026lsquo;API Gateway\u0026rsquo;.   We choose HTTP API and security to be open for the time being.   A block will appear on the designer tab in the Lambda function page.   Once you click on the API Gateway link provided, you will see the response as expected:  See this documentation for more details.\nSet up a Programmatic User to Access S3  What is IAM? IAM (Identity and Access Management) is a very useful tool for an organization to administer and control access to various resources.\n We will create a programmatic user, similar to the service account in GCP.  What is S3? S3 (Simple Storage Service) is a very popular service used by many large and small companies and individuals.\n For example, a typical data science work flow may involve ETL work on databases (such as Amazon Redshift) and then storing outputs on S3.  Start with finding the IAM service.\n   Click on create a new user.   We will make this user have programmatic access (for example via Python later on).   For now, we let the user have full read and write access to S3.   Lets review the user setup.   In this step, copy the access key and secret key that will be needed for the programmatic access. They will not be shown again, so store them safely and securely.   Here is the description of the user we just created.  Set up AWS CLI  The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.\n  Head over to https://aws.amazon.com/cli/ and download a suitable installer for your OS. Install it and open your terminal. If it is in the PATH, you should be able to verify the following:\nwhich aws #/usr/local/bin/aws #your location could be different. aws --version #aws-cli/2.0.45 Python/3.7.4 Darwin/18.6.0 exe/x86_64  Using the access and secret keys to configure aws using the aws configure command.\n   Test S3 access by issing the following commmand: aws s3 ls\n$ aws s3 ls 2020-09-03 12:27:48 chicagodatascience-com 2020-09-03 12:02:25 theja-model-store  Instead of programmatically creating a bucket (with the aws s3 mb s3://bucket-name) we will do it using the GUI below.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions_model/",
	"title": "AWS Serverless Model Serving",
	"tags": [],
	"description": "",
	"content": " Storing the Model on S3  To set up S3 for model serving, we have to perform a number of steps. We start with the s3 page.  - Create a bucket with an informative name.\n We don\u0026rsquo;t have to touch any of these for now.   Here the summary to review.   And we can see the bucket in the list of buckets.  Zip of Local Environment  We need a zip of local environment that includes all dependent libraries. This is because there is no way to specify requirements.txt like in Cloud Functions. This zip file can be uploaded to the bucket we created above. We start with creating a directory of all dependencies.\nmkdir lambda_model cd lambda_model # pip install pickle5 -t . #if we had a specific requirement, we would execute this line  First we will read the model and the movie.dat files (presumably in the parent directory) to create two new dictionaries:\n#Dumping the recommendations and movie info as dictionaries. from surprise import Dataset import pandas as pd import pickle5 import pickle import json def load(file_name): dump_obj = pickle5.load(open(file_name, 'rb')) return dump_obj['predictions'], dump_obj['algo'] def get_top_n(predictions, n=10): # First map the predictions to each user. top_n = {} for uid, iid, true_r, est, _ in predictions: if uid not in top_n: top_n[uid] = [] top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n def defaultdict(default_type): class DefaultDict(dict): def __getitem__(self, key): if key not in self: dict.__setitem__(self, key, default_type()) return dict.__getitem__(self, key) return DefaultDict() df = pd.read_csv('../movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) predictions, algo = load('../surprise_model') top_n = get_top_n(predictions, n=5) df = df.drop(['genre'],axis=1) movie_dict = df.T.to_dict() pickle.dump(movie_dict,open('movie_dict.pkl','wb')) pickle.dump(top_n,open('top_n.pkl','wb'))  Next we will create the following file called lambda_function.py in this directory:\nimport json # top_n = {'196':[(1,3),(2,4)]} # movie_dict = {1:{'name':'a'},2:{'name':'b'}} def lambda_handler(event,context): data = {\u0026quot;success\u0026quot;: False} with open(\u0026quot;top_n.json\u0026quot;, \u0026quot;r\u0026quot;) as read_file: top_n = json.load(read_file) with open(\u0026quot;movie_dict.json\u0026quot;, \u0026quot;r\u0026quot;) as read_file: movie_dict = json.load(read_file) print(event) #debug if \u0026quot;body\u0026quot; in event: event = event[\u0026quot;body\u0026quot;] if event is not None: event = json.loads(event) else: event = {} if \u0026quot;uid\u0026quot; in event: data[\u0026quot;response\u0026quot;] = str([movie_dict.get(iid,{'name':None})['name'] for (iid, _) in top_n[event.get(\u0026quot;uid\u0026quot;)]]) data[\u0026quot;success\u0026quot;] = True return { 'statusCode': 200, 'headers':{'Content-Type':'application/json'}, 'body': json.dumps(data) }  Here instead of a request object in GCP, a pair (event,context) are taken as input. The event object will have the query values. See this for more details.\n Next we zip the model and its dependencies and upload to S3.\nzip -r recommend.zip . aws s3 cp recommend.zip s3://theja-model-store/recommend.zip aws s3 ls s3://theja-model-store/  Add the API Gateway as before and see the predictions in action!\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/pytorch_training/",
	"title": "Recommendation (Pytorch) Training",
	"tags": [],
	"description": "",
	"content": "Please install the package using the command conda install -c conda-forge scikit-surprise in the ight environment.\n# https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.accuracy import rmse from surprise.dump import dump import numpy as np import torch from torch import nn import torch.nn.functional as F from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator from ignite.metrics import Loss, MeanSquaredError from datetime import datetime from sklearn.utils import shuffle class Loader(): current = 0 def __init__(self, x, y, batchsize=1024, do_shuffle=True): self.shuffle = shuffle self.x = x self.y = y self.batchsize = batchsize self.batches = range(0, len(self.y), batchsize) if do_shuffle: # Every epoch re-shuffle the dataset self.x, self.y = shuffle(self.x, self.y) def __iter__(self): # Reset \u0026amp; return a new iterator self.x, self.y = shuffle(self.x, self.y, random_state=0) self.current = 0 return self def __len__(self): # Return the number of batches return int(len(self.x) / self.batchsize) def __next__(self): n = self.batchsize if self.current + n \u0026gt;= len(self.y): raise StopIteration i = self.current xs = torch.from_numpy(self.x[i:i + n]) ys = torch.from_numpy(self.y[i:i + n]) self.current += n return (xs, ys) def l2_regularize(array): loss = torch.sum(array ** 2.0) return loss class MF(nn.Module): itr = 0 def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0): super(MF, self).__init__() self.k = k self.n_user = n_user self.n_item = n_item self.c_bias = c_bias self.c_vector = c_vector self.user = nn.Embedding(n_user, k) self.item = nn.Embedding(n_item, k) # We've added new terms here: self.bias_user = nn.Embedding(n_user, 1) self.bias_item = nn.Embedding(n_item, 1) self.bias = nn.Parameter(torch.ones(1)) def __call__(self, train_x): user_id = train_x[:, 0] item_id = train_x[:, 1] vector_user = self.user(user_id) vector_item = self.item(item_id) # Pull out biases bias_user = self.bias_user(user_id).squeeze() bias_item = self.bias_item(item_id).squeeze() biases = (self.bias + bias_user + bias_item) ui_interaction = torch.sum(vector_user * vector_item, dim=1) # Add bias prediction to the interaction prediction prediction = ui_interaction + biases return prediction def loss(self, prediction, target): loss_mse = F.mse_loss(prediction, target.squeeze()) # Add new regularization to the biases prior_bias_user = l2_regularize(self.bias_user.weight) * self.c_bias prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias prior_user = l2_regularize(self.user.weight) * self.c_vector prior_item = l2_regularize(self.item.weight) * self.c_vector total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item return total def log_training_loss(engine, log_interval=400): epoch = engine.state.epoch itr = engine.state.iteration fmt = \u0026quot;Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\u0026quot; msg = fmt.format(epoch, itr, len(train_loader), engine.state.output) model.itr = itr if itr % log_interval == 0: print(msg) def log_validation_results(engine): evaluat.run(test_loader) metrics = evaluat.state.metrics avg_accuracy = metrics['accuracy'] print(\u0026quot;Epoch[{}] Validation MSE: {:.2f} \u0026quot; .format(engine.state.epoch, avg_accuracy))  #Data data = Dataset.load_builtin('ml-100k') trainset = data.build_full_trainset() uir = np.array([x for x in trainset.all_ratings()]) train_x = test_x = uir[:,:2].astype(np.int64) train_y = test_y = uir[:,2].astype(np.float32)  #Parameters lr = 1e-2 k = 10 #latent dimension c_bias = 1e-6 c_vector = 1e-6 batchsize = 1024 model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector) optimizer = torch.optim.Adam(model.parameters()) trainer = create_supervised_trainer(model, optimizer, model.loss) metrics = {'accuracy': MeanSquaredError()} evaluat = create_supervised_evaluator(model, metrics=metrics) train_loader = Loader(train_x, train_y, batchsize=batchsize) test_loader = Loader(test_x, test_y, batchsize=batchsize) trainer.add_event_handler(event_name=Events.ITERATION_COMPLETED, handler=log_training_loss) trainer.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=log_validation_results)  \u0026lt;ignite.engine.events.RemovableEventHandle at 0x7f011e8a67c0\u0026gt;  model  MF( (user): Embedding(943, 10) (item): Embedding(1682, 10) (bias_user): Embedding(943, 1) (bias_item): Embedding(1682, 1) )  trainer.run(train_loader, max_epochs=50)  Epoch[1] Validation MSE: 17.82 Epoch[2] Validation MSE: 16.02 Epoch[3] Validation MSE: 14.43 Epoch[4] Validation MSE: 13.04 Epoch[5] Iteration[400/97] Loss: 12.52 Epoch[5] Validation MSE: 11.79 Epoch[6] Validation MSE: 10.70 Epoch[7] Validation MSE: 9.71 Epoch[8] Validation MSE: 8.85 Epoch[9] Iteration[800/97] Loss: 8.95 Epoch[9] Validation MSE: 8.08 Epoch[10] Validation MSE: 7.40 Epoch[11] Validation MSE: 6.80 Epoch[12] Validation MSE: 6.27 Epoch[13] Iteration[1200/97] Loss: 6.50 Epoch[13] Validation MSE: 5.79 Epoch[14] Validation MSE: 5.36 Epoch[15] Validation MSE: 4.98 Epoch[16] Validation MSE: 4.63 Epoch[17] Iteration[1600/97] Loss: 4.80 Epoch[17] Validation MSE: 4.32 Epoch[18] Validation MSE: 4.04 Epoch[19] Validation MSE: 3.79 Epoch[20] Validation MSE: 3.56 Epoch[21] Iteration[2000/97] Loss: 3.35 Epoch[21] Validation MSE: 3.35 Epoch[22] Validation MSE: 3.15 Epoch[23] Validation MSE: 2.97 Epoch[24] Validation MSE: 2.81 Epoch[25] Iteration[2400/97] Loss: 2.73 Epoch[25] Validation MSE: 2.66 Epoch[26] Validation MSE: 2.53 Epoch[27] Validation MSE: 2.40 Epoch[28] Validation MSE: 2.28 Epoch[29] Iteration[2800/97] Loss: 2.31 Epoch[29] Validation MSE: 2.17 Epoch[30] Validation MSE: 2.07 Epoch[31] Validation MSE: 1.97 Epoch[32] Validation MSE: 1.89 Epoch[33] Iteration[3200/97] Loss: 1.82 Epoch[33] Validation MSE: 1.81 Epoch[34] Validation MSE: 1.73 Epoch[35] Validation MSE: 1.66 Epoch[36] Validation MSE: 1.60 Epoch[37] Validation MSE: 1.54 Epoch[38] Iteration[3600/97] Loss: 1.60 Epoch[38] Validation MSE: 1.48 Epoch[39] Validation MSE: 1.43 Epoch[40] Validation MSE: 1.38 Epoch[41] Validation MSE: 1.34 Epoch[42] Iteration[4000/97] Loss: 1.27 Epoch[42] Validation MSE: 1.29 Epoch[43] Validation MSE: 1.25 Epoch[44] Validation MSE: 1.22 Epoch[45] Validation MSE: 1.19 Epoch[46] Iteration[4400/97] Loss: 1.11 Epoch[46] Validation MSE: 1.15 Epoch[47] Validation MSE: 1.13 Epoch[48] Validation MSE: 1.10 Epoch[49] Validation MSE: 1.07 Epoch[50] Iteration[4800/97] Loss: 1.11 Epoch[50] Validation MSE: 1.05 State: iteration: 4850 epoch: 50 epoch_length: 97 max_epochs: 50 output: 1.0818936824798584 batch: \u0026lt;class 'tuple'\u0026gt; metrics: \u0026lt;class 'dict'\u0026gt; dataloader: \u0026lt;class '__main__.Loader'\u0026gt; seed: \u0026lt;class 'NoneType'\u0026gt; times: \u0026lt;class 'dict'\u0026gt;  torch.save(model.state_dict(), \u0026quot;./pytorch_model\u0026quot;)   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/pytorch_inference/",
	"title": "Recommendation (Pytorch) Inference",
	"tags": [],
	"description": "",
	"content": "from surprise import Dataset import numpy as np import torch from torch import nn import pandas as pd class MF(nn.Module): itr = 0 def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0): super(MF, self).__init__() self.k = k self.n_user = n_user self.n_item = n_item self.c_bias = c_bias self.c_vector = c_vector self.user = nn.Embedding(n_user, k) self.item = nn.Embedding(n_item, k) # We've added new terms here: self.bias_user = nn.Embedding(n_user, 1) self.bias_item = nn.Embedding(n_item, 1) self.bias = nn.Parameter(torch.ones(1)) def __call__(self, train_x): user_id = train_x[:, 0] item_id = train_x[:, 1] vector_user = self.user(user_id) vector_item = self.item(item_id) # Pull out biases bias_user = self.bias_user(user_id).squeeze() bias_item = self.bias_item(item_id).squeeze() biases = (self.bias + bias_user + bias_item) ui_interaction = torch.sum(vector_user * vector_item, dim=1) # Add bias prediction to the interaction prediction prediction = ui_interaction + biases return prediction def loss(self, prediction, target): loss_mse = F.mse_loss(prediction, target.squeeze()) # Add new regularization to the biases prior_bias_user = l2_regularize(self.bias_user.weight) * self.c_bias prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias prior_user = l2_regularize(self.user.weight) * self.c_vector prior_item = l2_regularize(self.item.weight) * self.c_vector total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item return total def get_top_n(model,testset,trainset,uid_input,n=10): preds = [] try: uid_input = int(trainset.to_inner_uid(uid_input)) except KeyError: return preds # First map the predictions to each user. for uid, iid, _ in testset: #inefficient try: uid_internal = int(trainset.to_inner_uid(uid)) except KeyError: continue if uid_internal==uid_input: try: iid_internal = int(trainset.to_inner_iid(iid)) movie_name = df.loc[int(iid),'name'] preds.append((iid,movie_name,float(model(torch.tensor([[uid_input,iid_internal]]))))) except KeyError: pass # Then sort the predictions for each user and retrieve the k highest ones if preds is not None: preds.sort(key=lambda x: x[1], reverse=True) if len(preds) \u0026gt; n: preds = preds[:n] return preds  #Data df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) print(df.head()) data = Dataset.load_builtin('ml-100k') trainset = data.build_full_trainset() testset = trainset.build_anti_testset()   name genre iid 1 Toy Story (1995) Animation|Children's|Comedy 2 Jumanji (1995) Adventure|Children's|Fantasy 3 Grumpier Old Men (1995) Comedy|Romance 4 Waiting to Exhale (1995) Comedy|Drama 5 Father of the Bride Part II (1995) Comedy  #Parameters lr = 1e-2 k = 10 #latent dimension c_bias = 1e-6 c_vector = 1e-6 model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector) model.load_state_dict(torch.load('./pytorch_model')) model.eval()  MF( (user): Embedding(943, 10) (item): Embedding(1682, 10) (bias_user): Embedding(943, 1) (bias_item): Embedding(1682, 1) )  # Print the recommended items for each user limit = 0 for uid,_,_ in testset: print('\\nUser:',uid) seen = [df.loc[int(iid),'name'] for (iid, _) in trainset.ur[int(uid)]] if len(seen) \u0026gt; 10: seen = seen[:10] print('\\tSeen:',seen) print('\\tRecommendations:',get_top_n(model,testset,trainset,uid,n=10)) limit+=1 if limit\u0026gt;3: break  User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)'] User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)'] User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)'] User: 196 Seen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)'] Recommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \u0026quot;Pharaoh's Army (1995)\u0026quot;, 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)']  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/intro/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " TLDR Problem  Environmental compatibility is a headache, in addition to scaling, security, maintenance and upgrade of software and hardware.  For instance, in the serverless examples, the need for pickle5 instead of pickle was due to such incompatibility.  For hosted environments, we have to work hard on the devops to ensure the environments are the same. For serverless, we did this via the requirements file (Cloud Functions) and locally installing python packages (Lambda functions)  Solution  Containers\n they allow you to run arbitrary programs on new environments as long as they have a common piece of software (e.g., docker).  Their use cases include:\n Webapp deployments (real time dashboards) Model deployments (our objective) Packaging up projects (like a snapshot) and archiving so you can return to the project with the runtime/environment all set to go.   Containers  Containers are tools to produce reproducible environments by isolating and packaging everything that your program needs, independent of the target/host environment where you intended to run the program. Many tasks can be isolated and served in such a way, including model deployments. A Container is a process with features to keep it isolated from the host system. Because of their architecture, they are much lighter on resources compared to a full blown VM. You could think of them as stripped down virtual machines (VMs). Obviously this comes with some trade-offs as well. Containers with our code/program/model in it can be distributed to team members and can be publicly released with minimal environment incompatibility issues. One key variant of this solution is Docker. There are others, but we will stick with this one. In summary, they are useful because:  consistency: in app development as well as its delivery portable: local machine or cloud lightweight: multiple containers on single host   Elastic Container Service by AWS  A proprietary solution by AWS Elastic Container Service (ECS) similar to Lambda functions, and allows one to be agnostic to devops details (which server, server configs, security etc). Lifts certain restrictions of Lambda functions:  Not specific to the runtimes available in Lambda functions No memory limit (e.g., 256MB)  In ECS, we have to define the container explicitly, and then the rest (scaling, fault-tolerance) is behind the scenes.  Note: While exploring ECS, keep a tab on billing (check every so often)!\nWhat we will study  Docker local experimentation Run docker on EC2/AWS Use ECS (and Elastic Container Registry/ECR) on AWS The end goal is a functionality that is almost serverless, but has more work, potentially costing less and also allowing for more flexibility.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/",
	"title": "Lecture 3",
	"tags": [],
	"description": "",
	"content": " Serving ML Models Using Containers on AWS - Docker - AWS Elastic Container Service with Fargate "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/docker/",
	"title": "Docker",
	"tags": [],
	"description": "",
	"content": " We will first get learn a bit more about docker. From their website:\n Docker is an open platform (written in Go) for developing, shipping, and running applications.\nDocker enables you to separate your applications from your infrastructure so you can deliver software quickly.\nWith Docker, you can manage your infrastructure in the same ways you manage your applications.\nBy taking advantage of Docker’s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.\n Docker Overview  A loosely isolated environment (a container) can be created using Docker. We can run multiple containers ona host (like our operating system: Mac/Win/Linux) Containers are light on resources compared to VMs because they run directly with the host machine\u0026rsquo;s kernel.  Example, you may have heard of the linux OS kernel. A kernel is a computer program at the core of a computer\u0026rsquo;s operating system with complete control over everything in the system   Source: https://en.wikipedia.org/wiki/Kernel_(operating_system) \nDocker Engine  Is an app that is comprised of\n a server (dockerd) that handles creating containers an API to talk to the server a CLI (docker) that uses the API\n The client and the daemon they interact via the API to handle creation and deployment of containers.\n   Source: https://docs.docker.com/get-started/overview/ \nDocker Architecture Source: https://docs.docker.com/get-started/overview/ \n Docker registry: stores Docker images. E.g., Docker hub is a public registry. Docker objects:  Images Networks Containers Volumnes Plugins  Image:\n Read only template of instructions to create a container. It is often based on other images. Dockerfile creates images: slightly changing it does not mean rebuilding from scratch (lightweight)  When we run the following command:\n$ docker run -i -t ubuntu /bin/bash   Docker pulls the image from the registry Creates a container Allocates a private filesystem Creates a network inteface Executes /bin/bash because of the -i -t flag  Services: Docker (\u0026gt; 1.12) has the capability to make multiple daemons work together (as a swarm) with load balancing automatically handled.\n Container:\n A runnable instance of an image Several operations: create, start, stop, move, delete. When deleted, it does NOT store state! In contrast, a VM is a guest operating system that accesses host resources through a hypervisor.   Source: https://docs.docker.com/get-started/\nRunning a Container  Download Docker for Desktop from here. You will have to create a free account with them.\n Check version docker --version\n Run a hello world container: docker run hello-world\n   In the above, I changed to the appropriate conda env, although it is not necessary to do so. Its just a good practice.\n We can check the list of images we have locally via:\n(datasci-dev) ttmac:~ theja$ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest bf756fb1ae65 8 months ago 13.3kB  We can check the active/recently run containers via the following command:\n(datasci-dev) ttmac:~ theja$ docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 76cf92eaec76 hello-world \u0026quot;/hello\u0026quot; 4 minutes ago Exited (0) 4 minutes ago great_sinoussi  create a folder and copy the flask weather service app (weather.py) from before.\n(datasci-dev) ttmac:code theja$ mkdir docker-weather-service (datasci-dev) ttmac:code theja$ cd docker-weather-service/ (datasci-dev) ttmac:code theja$ cp ../lecture1/weather.py docker-weather-service/  Recall that running python weather.py and accessing it from the browser using the url http://localhost:5000/?msg=Chicago should give you a JSON response back.\n We will write a Dockerfile as below to build a custom image.\nFROM debian:buster-slim MAINTAINER Your Name RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y python3-pip python3-dev \\ \u0026amp;\u0026amp; cd /usr/local/bin \\ \u0026amp;\u0026amp; ln -s /usr/bin/python3 python \\ \u0026amp;\u0026amp; pip3 install flask geopy requests COPY weather.py weather.py ENTRYPOINT [\u0026quot;python3\u0026quot;,\u0026quot;weather.py\u0026quot;]  Building the custom image is achieved by the following commands:\ndocker image build -t \u0026quot;weather_service\u0026quot; . docker images #to check the recently built image  You should see the following output (truncated, only showing the last)\nStep 4/5 : COPY weather.py weather.py ---\u0026gt; 6ad721a3d5ef Step 5/5 : ENTRYPOINT [\u0026quot;python3\u0026quot;,\u0026quot;weather.py\u0026quot;] ---\u0026gt; Running in 70347eb72094 Removing intermediate container 70347eb72094 ---\u0026gt; ffd1e8b9172f Successfully built ffd1e8b9172f Successfully tagged weather_service:latest (datasci-dev) ttmac:docker-weather-service theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE weather_service latest ffd1e8b9172f 9 seconds ago 492MB debian buster-slim c7346dd7f20e 4 weeks ago 69.2MB hello-world latest bf756fb1ae65 8 months ago 13.3kB  We will run the newly created weather_service image as a container locally suing the docker run command (recall how we did the hello world example).\ndocker run -d -p 5000:5000 weather_service docker ps  Here, the -d -p flags are to make the container run as a daemon (i.e., not interactive with the user) and to do port forwarding.\n(datasci-dev) ttmac:docker-weather-service theja$ docker run -d -p 5000:5000 weather_service 50dbba727a24c9534f0a88165a0601f9447ec5e2e3a6c21fe8443c12cfac63e4 (datasci-dev) ttmac:docker-weather-service theja$ docker ps -all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 50dbba727a24 weather_service \u0026quot;python3 weather.py\u0026quot; 4 seconds ago Up 3 seconds 0.0.0.0:5000-\u0026gt;5000/tcp tender_shtern  Going to your browser and accessing http://localhost:5000/?msg=Chicago should give the appropriate weather response JSON object.\n The model serving python files (both pytorch and surprise based) can be deployed locally or on a VPS (EC2/Vultr/Digitalocean) similarly.\n Since hosting a container does not by itself give us additional features such as fault tolerance and scalability (being able to respond to many requests simultaneously), we will make use of ECS and GKE to address thee.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/ecr/",
	"title": "Orchestration using ECS and ECR - Part I",
	"tags": [],
	"description": "",
	"content": " Intro  Orchestration means managing container life cycle from building them to deploying (which requires provisioning of appropriate compute resources, storage resources, networking resources), scaling, load-balancing and other tasks, while accounting for failures throughout.\n While there are many orchestration solutions, we will focus on a couple of them: ECS by AWS and Kubernetes (local hosted solution and managed by GCP). While there is Elastic Kubernetes Service (EKS) by AWS as well, we will omit it here, as the ideas are the same.\nWhy should data science professions know such orchestration solutions?\n Pro: Get key devops features (fault tolerance, scalability etc) with low on-going effort. The deployed service will not likely break down. Con: There is a non-trivial setup cost/complexity.  Next, we will (a) deploy our model serving docker image to the AWS container registry ECR, (b) use ECS to deploy a container based on that image, and \u0026copy; set up a load balancer that mediates requests to the prediction model.\nElastic Container Registry (ECR)  Sending the image to a model registry such as ECR is needed to access it at other places to create containers. ECR allows for better integration with the AWS platform, and works with EKS as well. We will create a docker image locally (can also be done on EC2) and push it to an ECR repository that we will create.\n Navigate to the ECR link on the AWS console.\n   Click create a repository \u0026lsquo;Get Started\u0026rsquo; button. ECR can have multiple repositories and each repository can hold multiple images.   Give a name to the repository. It will contain multiple Docker images.   Review the current repository list.   Next we will allow the programmatic user we created for accessing S3 (we gave the user name model-user) to also manage the ECR repositories. Navigate to IAM as shown below.   Click on the user who we want to edit access controls for.   Currently this user had S3 access (not relevant for us).   Click on attaching existing policies and search for AmazonEC2ContainerRegistryFullAccess.   Review your policy choice and proceed.   You can see the summary of permissions that this programmatic user has.  Creating the Model Prediction Docker Image  We will use the following flask app that uses the pytorch model to serve recommendations:\nfrom surprise import Dataset import torch import pandas as pd import flask class MF(torch.nn.Module): def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0): super(MF, self).__init__() self.k = k self.n_user = n_user self.n_item = n_item self.c_bias = c_bias self.c_vector = c_vector self.user = torch.nn.Embedding(n_user, k) self.item = torch.nn.Embedding(n_item, k) # We've added new terms here: self.bias_user = torch.nn.Embedding(n_user, 1) self.bias_item = torch.nn.Embedding(n_item, 1) self.bias = torch.nn.Parameter(torch.ones(1)) def __call__(self, train_x): user_id = train_x[:, 0] item_id = train_x[:, 1] vector_user = self.user(user_id) vector_item = self.item(item_id) # Pull out biases bias_user = self.bias_user(user_id).squeeze() bias_item = self.bias_item(item_id).squeeze() biases = (self.bias + bias_user + bias_item) ui_interaction = torch.sum(vector_user * vector_item, dim=1) # Add bias prediction to the interaction prediction prediction = ui_interaction + biases return prediction def loss(self, prediction, target): loss_mse = F.mse_loss(prediction, target.squeeze()) # Add new regularization to the biases prior_bias_user = l2_regularize(self.bias_user.weight) * self.c_bias prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias prior_user = l2_regularize(self.user.weight) * self.c_vector prior_item = l2_regularize(self.item.weight) * self.c_vector total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item return total def get_top_n(model,testset,trainset,uid_input,n=10): preds = [] try: uid_input = int(trainset.to_inner_uid(uid_input)) except KeyError: return preds # First map the predictions to each user. for uid, iid, _ in testset: #inefficient try: uid_internal = int(trainset.to_inner_uid(uid)) except KeyError: continue if uid_internal==uid_input: try: iid_internal = int(trainset.to_inner_iid(iid)) movie_name = df.loc[int(iid),'name'] preds.append((iid,movie_name,float(model(torch.tensor([[uid_input,iid_internal]]))))) except KeyError: pass # Then sort the predictions for each user and retrieve the k highest ones if preds is not None: preds.sort(key=lambda x: x[1], reverse=True) if len(preds) \u0026gt; n: preds = preds[:n] return preds app = flask.Flask(__name__) #Data df = pd.read_csv('./movies.dat',sep=\u0026quot;::\u0026quot;,header=None,engine='python') df.columns = ['iid','name','genre'] df.set_index('iid',inplace=True) data = Dataset.load_builtin('ml-100k',prompt=False) ''' Exercise: remove the above dependency. Currently it downloads data from grouplens website and stores in .surprise folder in $HOME ''' trainset = data.build_full_trainset() testset = trainset.build_anti_testset() #Parameters that are needed to reload the model from disk k = 10 #latent dimension c_bias = 1e-6 c_vector = 1e-6 model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector) model.load_state_dict(torch.load('./pytorch_model')) model.eval() #no need for gradient computations in this setting # define a predict function as an endpoint @app.route(\u0026quot;/\u0026quot;, methods=[\u0026quot;GET\u0026quot;]) def predict(): data = {\u0026quot;success\u0026quot;: False} # check for passed in parameters params = flask.request.json if params is None: params = flask.request.args if \u0026quot;uid\u0026quot; in params.keys(): data[\u0026quot;response\u0026quot;] = get_top_n(model,testset,trainset,params['uid'],n=10) data[\u0026quot;success\u0026quot;] = True # return a response in json format return flask.jsonify(data) # start the flask app, allow remote connections app.run(host='0.0.0.0', port=80)  The corresponding Dockerfile is below. The key additional files in addition to recommend.py above are:\n movies.dat pytorch_model\nFROM continuumio/miniconda3:latest RUN conda install -y flask pandas \\ \u0026amp;\u0026amp; conda install -c conda-forge scikit-surprise \\ \u0026amp;\u0026amp; conda install pytorch torchvision cpuonly -c pytorch COPY recommend.py recommend.py COPY movies.dat movies.dat COPY pytorch_model pytorch_model ENTRYPOINT [\u0026quot;python\u0026quot;,\u0026quot;recommend.py\u0026quot;]   The miniconda image above is from https://hub.docker.com/r/continuumio/miniconda3.\n Building an image based on the above file and running our prediction locally can be done using the following commands:\ndocker image build -t \u0026quot;prediction_service\u0026quot; . docker run -d -p 5000:5000 prediction_service docker ps -a #check what all containers were/are running docker kill container_id #after checking that the service runs, we can safely stop and delete the container. docker rm container_id  If we run a container based on this image, the python file and others will be in the root (/) folder and will be run by the root user. While we will not improve this here, it is better to run services as non-root users.\n  Sending our Docker Image to ECR  We will follow the instruction here to push our image to the repository we just created.\n Assuming you have the aws CLI configured with the secret keys, run the following command:\naws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com  Substitute region with us-east-1 etc (check the URL on the ECR page) as well as aws_account_id with the actual account id. We should get a prompt saying \u0026lsquo;Login Succeeded\u0026rsquo;.\n Lets tag our image before sending it to ECR (replace account id and region below as well):\n(datasci-dev) ttmac:docker-prediction-service theja$ docker tag prediction_service aws_account_id.dkr.ecr.region.amazonaws.com/models:recommendations (datasci-dev) ttmac:docker-prediction-service theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE aws_account_id.dkr.ecr.region.amazonaws.com/recommendations pytorch_model 364179b27eb1 21 minutes ago 2.06GB weather_service latest 20d340f941c0 2 days ago 496MB debian buster-slim c7346dd7f20e 5 weeks ago 69.2MB continuumio/miniconda3 latest b4adc22212f1 6 months ago 429MB hello-world latest bf756fb1ae65 8 months ago 13.3kB  Pushing to ECR is achieved by the following:\ndocker push aws_account_id.dkr.region.amazonaws.com/recommendations:pytorch_model  You should see the update progress (this is a large upload!)\nThe push refers to repository [aws_account_id.dkr.ecr.us-east-2.amazonaws.com/recommendations] a5649bbe3e5f: Pushed 5c87fc4d582f: Pushed e1e8d92205bf: Pushed 5c6c81390816: Pushing [=========================\u0026gt; ] 848.8MB/1.635GB fcd8d39597dd: Pushed 875120aa853c: Pushed f2cb0ecef392: Pushed  And the push conclusion:\n(datasci-dev) ttmac:docker-prediction-service theja$ docker push aws_account_id.dkr.ecr.us-east-2.amazonaws.com/recommendations:pytorch_model The push refers to repository [aws_account_id.dkr.ecr.us-east-2.amazonaws.com/recommendations] a5649bbe3e5f: Pushed 5c87fc4d582f: Pushed e1e8d92205bf: Pushed 5c6c81390816: Pushed fcd8d39597dd: Pushed 875120aa853c: Pushed f2cb0ecef392: Pushed pytorch_model: digest: sha256:af5dfaf227cd96c4ca8ca952c511fb4274c59d76574726462137bc7c4230be07 size: 1793  On the ECR page, if we look at the images in the recommendations repository, it will contain our recently uploaded image.\n   There is a friendly help box that details specific (to your account) commands for pushing images to ECR on the above page as well. You could use that as a guiding reference, or the help page.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/ecs/",
	"title": "Orchestration using ECS and ECR - Part II",
	"tags": [],
	"description": "",
	"content": " Elastic Container Service (ECS)  This is a AWS propreitary solution for container orchestration. There are three key concepts to work with this solution:  Service: Manages containers and relates them to EC2 machines as needed Task: Is a specific container Cluster: Is the environment of EC2 machines where containers live  The below diagram illustrates these relationships.  Source: https://aws.com/ \n We will set up a cluster and run a task/container and use a service to manage it. We will use Fargate for this exercise, although there are other more detailed ways to achieve the same end goals. Using Fargate will hide a lot of complexity, especially with privisioning the underlying EC2 instances.\n Lets start by getting to ECS.\n   Hit the get started button above or navigate to the clusters link on the left and then hit the get started button below.   Choose the custom container definition and hit configure.   Name the container, and point it to the ECR image URI. Specify the port to be 80 (this is what we choose in recommend.py). Then hit update.   We will keep the task definition to the default values.   Similarly we will retain the defaults for the service definition on the next page as well.   Name the cluster and then review the settings. Note that a lot is happening under the hood.   Review the settings as shown below:   The cluster gets created and you can see the status for all tasks change to green in due time.   Click view service and then click the Tasks tab at the center/bottom part of the screen.   Clicking on the task (there should only be one task listed) will show the public IP through which we can seek model predicitons.   Navigating to our browser and accessing the IP and making a query such as http://18.220.91.58/?uid=20 gives the following response.  Setting Up a Load Balancer  Directly accessing the IP address (and thus the single container) is not scalable. A Load Balancer will give a static URL and route incoming HTTP(S) requests to ECS tasks managed by an ECS service.\n There are many types of load balancers on AWS, see here. We will use an application load balancer (ALB).\n Using the \u0026lsquo;Get Started\u0026rsquo; workflow on ECS is the easiest to set this up to work with the cluster.\n   Otherwise, we can instantiate the same from the EC2 page.   The load balancer uses a VPC (virtual private cloud, an AWS terminology and service), a security group (who can access our container) and a target group (who to route requests to).\n Aside: VPC essentially isolates your computing environment from the external world.\n   Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. You can use both IPv4 and IPv6 in your VPC for secure and easy access to resources and applications.\n  The ECS service should use the load balancer as shown below (if we did it using \u0026lsquo;Get Started\u0026rsquo; workflow on ECS).   We can also add it separately while creating the service (we will skip the details here).\n You can access the public static url by navigating to the load balancer page.\n First click on the load balancer on the service page shown above.\n Then click on the load balancer link to the top right of the page.\n   The public URL is the DNS name (the IP itself will be dynamic). You can bind it to your domain (using CNAME records).  Recap: Summary for ECS  Since there are a lot of steps involved (as well as quite a few moving parts), its good to revisit what our original goal is.\n Our goal was to set the model prediction/deployment up in such a way that it scales and has no issues with failures.\n The ECS cluster is scalable (we can add more tasks and services easily).\n Further, the ECS service manages these tasks such that even if the underlying EC2 instances that run these containers fail (could be any reason) the task can be restarted on other machines to keep everything running smoothly.\n Finally the load balancer, maps a static external IP to the internal container(s). So if there are multiple model prediction containers, the load balancer will use an algorithm (such as round robin) to distribute the incoming requests.\n While there is a lot more work to set this up unlike the serverless solution, there is more fine grained control and visibility into the components supporting your scalable model deployment effort.\n  Tear Down the ECS Cluster  Follow Step 7 onwards from this link: https://aws.amazon.com/getting-started/hands-on/deploy-docker-containers/  Essentially update the service to ensure that the number of tasks is 0. Then delete the service and stop the task and delete the task and task definition. Then delete the cluster itself. Check on the EC2, VPC and Load balancing pages if you are still consuming resources.   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": " Caveat: Unless we have a truly massive or complex system, we probably don’t need Kubernetes, and using it should be the result of a deliberate cost benefit analysis in comparison to other hosted solutions or managed solutions.\nIntroduction  Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. It was open-sourced by Google. Its predecessor was called borg internally. Kubernetes, or K8s for short, is a central orchestration system in large complex software systems. It coordinates a collection or cluster of computers with the purpose of working as a single unit. When we orchestrate containers with it, we are agnostic to which computer runs which container. K8s takes care of scheduling and distribution of containers to these unnamed computers behind the scenes.\n We will primarily be interested in ML model deployment.\n Lets start by going over some of the key concepts in the K8s container orchestration system below. For more information, have a look at the basics documented at https://kubernetes.io/docs/tutorials/kubernetes-basics/ and go from there.\n A K8s cluster has two resources:\n Master: coordinator of the cluster Nodes: workers are essentially VMs or computers that run containers.  Source: https://kubernetes.io/docs/tutorials/kubernetes-basics/create-cluster/cluster-intro/ \n Each node runs kubelet that manages it and communicated with the master. It also runs Docker daemon (or other technologies) to manage containers.\n The deploy sequence essentially involves the following:\n kubectl tells the master to start application containers. Master schedules containers to run.   Deployment  Once we have a cluster running, we can deploy containerized applications (one or many) using a Deployment configuration. We can think of this as a set of instructions to K8s to set up the application on the cluster. Essentially, the containers are mapped to individual nodes. After the mapping, a Deployment Controller keeps checking these instances allowing for self-healing.\nSource: https://kubernetes.io/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/ \n We create a deployment using a tool such as kubectl that uses an API to interact with the Master.\n When creating a deployment, we need to specify container images as well as the number of copies of each image we want to instantiate.\n  Nodes and Pods  When an app is deployed as a container, it is encapsulated in a pod on a specific node. A pod is a collection of containers that share a few things (see below) and are typically related to each other.  Storage Networking and IP address  These related containers can be a web server and a database for instance. We can also have a single container in a pod (this is what we will do here). Pods are the most basic unit in K8s. It is pods that are created and destroyed, not individual containers.  Source: https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/ \n A node is essentially a VM/machine and can have multiple pods (depending on how K8s schedules pods). A node runs:  Kubelet: a program that communicates between the master and the node and manages the pods on the node. Docker daemon (or equivalent) for pulling and running containers from images.   Source: https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/ \nServices  A Kubernetes Service is an abstraction layer which defines a logical set of Pods and enables external traffic exposure, load balancing and service discovery for those Pods. Source: https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/ \n  It enables coupling between pods (each of which have a unique IP). The pods with IPs cannot be typically accessed from the outside. Instead, a service can be used to allow external connections. For example, a service with a spec that says NodePort exposed pods on the same port of each selected node in the cluster using NAT (Network Address Translation). We will see an example while deploying our model.\nSource: https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/ \n The Service abstraction lets pods get deleted and replicated in the cluster with no influence on our app.\n A set of pods are matched to a service using labels and selectors. See https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/ for more information.\n  Scaling and Updating  See https://kubernetes.io/docs/tutorials/kubernetes-basics/scale/scale-intro/ for how an app can scale.\n See https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/ for information on how to perform a rolling update.\n  Installing kubectl, minikube, Virtualbox and Docker  We have already installed Docker for Desktop.  Virtualbox  Virtualbox is a software product for running virtual machines on a variety of host operating systems (MacOS/Windows/Linux).\n Go to https://www.virtualbox.org/ and download the installer for your OS. The installation is straightforward.\n Once installed, try to download a Linux distribution such as Debian or Fedora to try out how Virtualbox works.\n  Kubectl  kubectl is a client utility to talk to the the K8s server for container orchestration.\n Download kubectl from this page. Here is the example command for MacOS:\n(datasci-dev) ttmac:k8s theja$ curl -LO \u0026quot;https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl\u0026quot;  Change its permissions to make it executable and move it to a place where it can be on the $PATH. Check whether you can access it by querying its version information.\n(datasci-dev) ttmac:k8s theja$ ls -l total 96696 -rw-r--r-- 1 theja staff 49458208 Sep 15 21:18 kubectl (datasci-dev) ttmac:k8s theja$ chmod +x kubectl (datasci-dev) ttmac:k8s theja$ mv kubectl ~/Library/local/bin/ (datasci-dev) ttmac:k8s theja$ kubectl version --client Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;19\u0026quot;, GitVersion:\u0026quot;v1.19.1\u0026quot;, GitCommit:\u0026quot;206bcadf021e76c27513500ca24182692aabd17e\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2020-09-09T11:26:42Z\u0026quot;, GoVersion:\u0026quot;go1.15\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;darwin/amd64\u0026quot;}   Minikube  Minikube (minikube) is a tool that runs a single node K8s cluster in a virtual machine on our local computer. In particular, Minikube is a lightweight K8s implementation that creates a VM on your local machine and deploys a simple cluster containing only one node.\n Follow the instructions for your operating system at https://kubernetes.io/docs/tasks/tools/install-minikube/. For instance, to install it on MacOS, we do the following.\n We check if virtualization is supported. If we run the following command from the terminal, we expect the VMX acronym to be colored:\n(datasci-dev) ttmac:k8s theja$ sysctl -a | grep -E --color 'machdep.cpu.features|VMX' machdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX EST TM2 SSSE3 CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC POPCNT AES PCID XSAVE OSXSAVE TSCTMR AVX1.0 RDRAND F16C  We will download the stand-alone binary just as we did for kubectl and move it to the right path:\n(datasci-dev) ttmac:k8s theja$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 55.5M 100 55.5M 0 0 10.9M 0 0:00:05 0:00:05 --:--:-- 11.7M (datasci-dev) ttmac:k8s theja$ chmod +x minikube (datasci-dev) ttmac:k8s theja$ mv minikube ~/Library/local/bin/  Check that its on the path and working correctly by using the following status check:\n(datasci-dev) ttmac:k8s theja$ minikube status 🤷 There is no local cluster named \u0026quot;minikube\u0026quot; 👉 To fix this, run: \u0026quot;minikube start\u0026quot; (datasci-dev) ttmac:k8s theja$ minikube version minikube version: v1.13.0 commit: 0c5e9de4ca6f9c55147ae7f90af97eff5befef5f-dirty   Weather Service App  We will run the simple weather service application on K8s using Minikube. It will involve building a Docker image of the flask app, similar to before.\n The single node cluster can be started with the following command:\nminikube start   You will see the following blurb which ends with \u0026lsquo;Done! kubectl is now configured to use minikube by default\u0026rsquo; the first time you run it. You can stop minikube using minikube stop after the whole exercise is done.\n(datasci-dev) ttmac:k8s theja$ minikube start 😄 minikube v1.13.0 on Darwin 10.14.6 ✨ Automatically selected the hyperkit driver 💾 Downloading driver docker-machine-driver-hyperkit: \u0026gt; docker-machine-driver-hyperkit.sha256: 65 B / 65 B [---] 100.00% ? p/s 0s \u0026gt; docker-machine-driver-hyperkit: 10.90 MiB / 10.90 MiB 100.00% 7.97 MiB p 🔑 The 'hyperkit' driver requires elevated permissions. The following commands will be executed: $ sudo chown root:wheel /Users/theja/.minikube/bin/docker-machine-driver-hyperkit $ sudo chmod u+s /Users/theja/.minikube/bin/docker-machine-driver-hyperkit Password: 💿 Downloading VM boot image ... \u0026gt; minikube-v1.13.0.iso.sha256: 65 B / 65 B [-------------] 100.00% ? p/s 0s \u0026gt; minikube-v1.13.0.iso: 173.73 MiB / 173.73 MiB [ 100.00% 11.61 MiB p/s 15s 👍 Starting control plane node minikube in cluster minikube 💾 Downloading Kubernetes v1.19.0 preload ... \u0026gt; preloaded-images-k8s-v6-v1.19.0-docker-overlay2-amd64.tar.lz4: 486.28 MiB 🔥 Creating hyperkit VM (CPUs=2, Memory=4000MB, Disk=20000MB) ... 🐳 Preparing Kubernetes v1.19.0 on Docker 19.03.12 ... 🔎 Verifying Kubernetes components... 🌟 Enabled addons: default-storageclass, storage-provisioner 🏄 Done! kubectl is now configured to use \u0026quot;minikube\u0026quot; by default   If you run the status command, you should see the following:\n(datasci-dev) ttmac:k8s theja$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured  We can also query the nodes (we only have one) and more information about them as below:\n(datasci-dev) ttmac:k8s theja$ kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 131m v1.19.0 (datasci-dev) ttmac:k8s theja$ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 140m  Although we have the weather_service image (can be viewed by using the docker images command as long as the Docker daemon is running) in our local repository, we need to access and use the repository specific to Minikube. This can be done using the following two commands:\n(datasci-dev) ttmac:docker-weather-service theja$ minikube docker-env (datasci-dev) ttmac:docker-weather-service theja$ eval $(minikube -p minikube docker-env)  Now if we build the same weather service flask app (recall the corresponding Dockerfile and weather.py from before). This time, we will give a slightly different repository name.\n(datasci-dev) ttmac:docker-weather-service theja$ docker build -t weather-service-k8s/latest .  The newly created repository is now available in the minikube specific image repository (notice the other images for instance):\n(datasci-dev) ttmac:docker-weather-service theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE weather-service-k8s/latest latest ae4f44a22535 11 seconds ago 496MB debian buster-slim 052664ad4351 6 days ago 69.2MB gcr.io/k8s-minikube/storage-provisioner v3 bad58561c4be 2 weeks ago 29.7MB k8s.gcr.io/kube-proxy v1.19.0 bc9c328f379c 2 weeks ago 118MB k8s.gcr.io/kube-apiserver v1.19.0 1b74e93ece2f 2 weeks ago 119MB k8s.gcr.io/kube-controller-manager v1.19.0 09d665d529d0 2 weeks ago 111MB k8s.gcr.io/kube-scheduler v1.19.0 cbdc8369d8b1 2 weeks ago 45.7MB k8s.gcr.io/etcd 3.4.9-1 d4ca8726196c 2 months ago 253MB kubernetesui/dashboard v2.0.3 503bc4b7440b 2 months ago 225MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 3 months ago 45.2MB kubernetesui/metrics-scraper v1.0.4 86262685d9ab 5 months ago 36.9MB k8s.gcr.io/pause 3.2 80d28bedfe5d 7 months ago 683kB  Although there is a local image, we need to set a specific parameter to make kubectl not search for the image elsewhere. In order to do so, we create the following yaml (a data-serialization language thats quite popular to set config parameters and is very tab/space sensitive). See https://en.wikipedia.org/wiki/YAML for more info on yaml files.\n The yaml was generated automatically by running the following command and then modified by adding the line imagePullPolicy: Never:\n(datasci-dev) ttmac:docker-weather-service theja$ kubectl create deployment weather-minikube --image=weather-service-k8s:latest -o yaml --dry-run=client  apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: weather-minikube name: weather-minikube spec: replicas: 1 selector: matchLabels: app: weather-minikube strategy: {} template: metadata: creationTimestamp: null labels: app: weather-minikube spec: containers: - image: weather-service-k8s/latest:latest name: weather-service-k8s resources: {} imagePullPolicy: Never status: {}  Save the above in a file called. weather_minikube.yaml. Next we create a new deployment using the minikube cluster.\n(datasci-dev) ttmac:docker-weather-service theja$ kubectl apply -f weather_minikube.yaml deployment.apps/weather-minikube created (datasci-dev) ttmac:docker-weather-service theja$ kubectl get all NAME READY STATUS RESTARTS AGE pod/weather-minikube-56fd45dd59-zkmmg 1/1 Running 0 71s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 160m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/weather-minikube 1/1 1 1 71s NAME DESIRED CURRENT READY AGE replicaset.apps/weather-minikube-76cb958c4 1 1 1 71s  Next, we will expose this deployment. The option --type=NodePort specifies the type of the Service.\n(datasci-dev) ttmac:docker-weather-service theja$ kubectl expose deployment weather-minikube --type=NodePort --port=5000 service/weather-minikube exposed  We can check that the container is in fact running fine using the following command:\n(datasci-dev) ttmac:docker-weather-service theja$ kubectl get pod NAME READY STATUS RESTARTS AGE weather-minikube-56fd45dd59-zkmmg 1/1 Running 0 171m  Once we have verified that the container is running, we can figure out the URL that is being exposed by the system as follows:\n(datasci-dev) ttmac:docker-weather-service theja$ minikube service weather-minikube --url http://192.168.64.2:32233  We can then access the weather service flask app from the browser and make sure that it works with a few test examples:\n   We may have to debug our container for various reasons. For instance, it can throw up 500 errors, which correspond to internal server errors due to errors in the python function attached to a URL route in Flask. There are a couple of options:\n We can look at the logs (the last argument is the pod name and it has a single container in our example):\nkubectl logs weather-minikube-56fd45dd59-zkmmg  We can access the container shell (assuming it has bash at the appropriate path) by referencing the pod name and try to see which commands are not working:\nkubectl exec --stdin --tty weather-minikube-56fd45dd59-zkmmg -- /bin/bash  Finally, the command kubectl describe also helps in debugging by showing us detailed information associated with any resource.\n  Teardown involves the following commands:\n(datasci-dev) ttmac:docker-weather-service theja$ kubectl delete services weather-minikube service \u0026quot;weather-minikube\u0026quot; deleted (datasci-dev) ttmac:docker-weather-service theja$ kubectl delete deployment weather-minikube deployment.apps \u0026quot;weather-minikube\u0026quot; deleted  We can then stop and delete the minikube cluster using the minikube stop and minikube delete commands:\n(datasci-dev) ttmac:docker-weather-service theja$ minikube stop ✋ Stopping node \u0026quot;minikube\u0026quot; ... 🛑 1 nodes stopped.  Finally, you can delete the cluster using the command minikube delete.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/",
	"title": "Lecture 4",
	"tags": [],
	"description": "",
	"content": " Orchestration using Kubernetes - Kubernetes - Google Kubernetes Engine (GKE) "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/modelkube/",
	"title": "Model Serving using Kubernetes",
	"tags": [],
	"description": "",
	"content": "This time, instead of the weather app, we will deploy a container containing our recommendation model. Here are the steps.\n Lets start minicube\n(datasci-dev) ttmac:docker-prediction-service theja$ minikube start 😄 minikube v1.13.0 on Darwin 10.14.6 ▪ MINIKUBE_ACTIVE_DOCKERD=minikube ✨ Using the hyperkit driver based on existing profile 👍 Starting control plane node minikube in cluster minikube 🔄 Restarting existing hyperkit VM for \u0026quot;minikube\u0026quot; ... 🐳 Preparing Kubernetes v1.19.0 on Docker 19.03.12 ... 🔎 Verifying Kubernetes components... 🌟 Enabled addons: default-storageclass, storage-provisioner 🏄 Done! kubectl is now configured to use \u0026quot;minikube\u0026quot; by default  Ensure that docker can see minicube specific registry\n(datasci-dev) ttmac:docker-prediction-service theja$ minikube docker-env export DOCKER_TLS_VERIFY=\u0026quot;1\u0026quot; export DOCKER_HOST=\u0026quot;tcp://192.168.64.2:2376\u0026quot; export DOCKER_CERT_PATH=\u0026quot;/Users/theja/.minikube/certs\u0026quot; export MINIKUBE_ACTIVE_DOCKERD=\u0026quot;minikube\u0026quot; # To point your shell to minikube's docker-daemon, run: # eval $(minikube -p minikube docker-env) (datasci-dev) ttmac:docker-prediction-service theja$ eval $(minikube -p minikube docker-env)  Go to the directory containing the trained model that we created before. The directory should have\n a Dockerfile to build an image, the pytorch_model for recommendations, movies.dat metadata, and recommend.py flask app.  Once there, build an image. This image will be registered with minikube\u0026rsquo;s registry.\n(datasci-dev) ttmac:docker-prediction-service theja$ docker build -t prediction-service-k8s . Sending build context to Docker daemon 304.6kB Step 1/8 : FROM continuumio/miniconda3:latest latest: Pulling from continuumio/miniconda3 68ced04f60ab: Pull complete 9c388eb6d33c: Pull complete 96cf53b3a9dd: Pull complete Digest: sha256:456e3196bf3ffb13fee7c9216db4b18b5e6f4d37090b31df3e0309926e98cfe2 Status: Downloaded newer image for continuumio/miniconda3:latest ---\u0026gt; b4adc22212f1 Step 2/8 : MAINTAINER Theja Tulabandhula ---\u0026gt; Running in 9b4a3708a4f6 Removing intermediate container 9b4a3708a4f6 ---\u0026gt; 2ebbbd14e3d3 Step 3/8 : RUN conda install -y flask pandas \u0026amp;\u0026amp; conda install -c conda-forge scikit-surprise \u0026amp;\u0026amp; conda install pytorch torchvision cpuonly -c pytorch ---\u0026gt; Running in 0ce30dc6b5a6 Collecting package metadata (current_repodata.json): ...working... done Solving environment: ...working... done ## Package Plan ## environment location: /opt/conda added / updated specs: - flask - pandas The following packages will be downloaded: package | build ---------------------------|----------------- blas-1.0 | mkl 6 KB ca-certificates-2020.7.22 | 0 125 KB certifi-2020.6.20 | py37_0 156 KB click-7.1.2 | py_0 71 KB conda-4.8.4 | py37_0 2.9 MB flask-1.1.2 | py_0 78 KB intel-openmp-2020.2 | 254 786 KB itsdangerous-1.1.0 | py37_0 28 KB jinja2-2.11.2 | py_0 103 KB markupsafe-1.1.1 | py37h14c3975_1 26 KB mkl-2020.2 | 256 138.3 MB mkl-service-2.3.0 | py37he904b0f_0 218 KB mkl_fft-1.1.0 | py37h23d657b_0 143 KB mkl_random-1.1.1 | py37h0573a6f_0 322 KB numpy-1.19.1 | py37hbc911f0_0 21 KB numpy-base-1.19.1 | py37hfa32c7d_0 4.1 MB openssl-1.1.1g | h7b6447c_0 2.5 MB pandas-1.1.1 | py37he6710b0_0 8.2 MB python-dateutil-2.8.1 | py_0 215 KB pytz-2020.1 | py_0 184 KB werkzeug-1.0.1 | py_0 240 KB ------------------------------------------------------------ Total: 158.6 MB The following NEW packages will be INSTALLED: blas pkgs/main/linux-64::blas-1.0-mkl click pkgs/main/noarch::click-7.1.2-py_0 flask pkgs/main/noarch::flask-1.1.2-py_0 intel-openmp pkgs/main/linux-64::intel-openmp-2020.2-254 itsdangerous pkgs/main/linux-64::itsdangerous-1.1.0-py37_0 jinja2 pkgs/main/noarch::jinja2-2.11.2-py_0 markupsafe pkgs/main/linux-64::markupsafe-1.1.1-py37h14c3975_1 mkl pkgs/main/linux-64::mkl-2020.2-256 mkl-service pkgs/main/linux-64::mkl-service-2.3.0-py37he904b0f_0 mkl_fft pkgs/main/linux-64::mkl_fft-1.1.0-py37h23d657b_0 mkl_random pkgs/main/linux-64::mkl_random-1.1.1-py37h0573a6f_0 numpy pkgs/main/linux-64::numpy-1.19.1-py37hbc911f0_0 numpy-base pkgs/main/linux-64::numpy-base-1.19.1-py37hfa32c7d_0 pandas pkgs/main/linux-64::pandas-1.1.1-py37he6710b0_0 python-dateutil pkgs/main/noarch::python-dateutil-2.8.1-py_0 pytz pkgs/main/noarch::pytz-2020.1-py_0 werkzeug pkgs/main/noarch::werkzeug-1.0.1-py_0 The following packages will be UPDATED: ca-certificates 2020.1.1-0 --\u0026gt; 2020.7.22-0 certifi 2019.11.28-py37_0 --\u0026gt; 2020.6.20-py37_0 conda 4.8.2-py37_0 --\u0026gt; 4.8.4-py37_0 openssl 1.1.1d-h7b6447c_4 --\u0026gt; 1.1.1g-h7b6447c_0 Downloading and Extracting Packages blas-1.0 | 6 KB | ########## | 100% flask-1.1.2 | 78 KB | ########## | 100% certifi-2020.6.20 | 156 KB | ########## | 100% markupsafe-1.1.1 | 26 KB | ########## | 100% numpy-base-1.19.1 | 4.1 MB | ########## | 100% pytz-2020.1 | 184 KB | ########## | 100% python-dateutil-2.8. | 215 KB | ########## | 100% itsdangerous-1.1.0 | 28 KB | ########## | 100% openssl-1.1.1g | 2.5 MB | ########## | 100% click-7.1.2 | 71 KB | ########## | 100% conda-4.8.4 | 2.9 MB | ########## | 100% mkl-service-2.3.0 | 218 KB | ########## | 100% werkzeug-1.0.1 | 240 KB | ########## | 100% pandas-1.1.1 | 8.2 MB | ########## | 100% mkl_fft-1.1.0 | 143 KB | ########## | 100% mkl-2020.2 | 138.3 MB | ########## | 100% mkl_random-1.1.1 | 322 KB | ########## | 100% jinja2-2.11.2 | 103 KB | ########## | 100% intel-openmp-2020.2 | 786 KB | ########## | 100% ca-certificates-2020 | 125 KB | ########## | 100% numpy-1.19.1 | 21 KB | ########## | 100% Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done Collecting package metadata (current_repodata.json): ...working... done Solving environment: ...working... done ## Package Plan ## environment location: /opt/conda added / updated specs: - scikit-surprise The following packages will be downloaded: package | build ---------------------------|----------------- ca-certificates-2020.6.20 | hecda079_0 145 KB conda-forge certifi-2020.6.20 | py37hc8dfbb8_0 151 KB conda-forge conda-4.8.5 | py37hc8dfbb8_1 3.0 MB conda-forge joblib-0.16.0 | py_0 203 KB conda-forge openssl-1.1.1g | h516909a_1 2.1 MB conda-forge python_abi-3.7 | 1_cp37m 4 KB conda-forge scikit-surprise-1.1.1 | py37h03ebfcd_0 591 KB conda-forge ------------------------------------------------------------ Total: 6.2 MB The following NEW packages will be INSTALLED: joblib conda-forge/noarch::joblib-0.16.0-py_0 python_abi conda-forge/linux-64::python_abi-3.7-1_cp37m scikit-surprise conda-forge/linux-64::scikit-surprise-1.1.1-py37h03ebfcd_0 The following packages will be UPDATED: conda pkgs/main::conda-4.8.4-py37_0 --\u0026gt; conda-forge::conda-4.8.5-py37hc8dfbb8_1 openssl pkgs/main::openssl-1.1.1g-h7b6447c_0 --\u0026gt; conda-forge::openssl-1.1.1g-h516909a_1 The following packages will be SUPERSEDED by a higher-priority channel: ca-certificates pkgs/main::ca-certificates-2020.7.22-0 --\u0026gt; conda-forge::ca-certificates-2020.6.20-hecda079_0 certifi pkgs/main::certifi-2020.6.20-py37_0 --\u0026gt; conda-forge::certifi-2020.6.20-py37hc8dfbb8_0 Proceed ([y]/n)? Downloading and Extracting Packages ca-certificates-2020 | 145 KB | ########## | 100% conda-4.8.5 | 3.0 MB | ########## | 100% python_abi-3.7 | 4 KB | ########## | 100% certifi-2020.6.20 | 151 KB | ########## | 100% openssl-1.1.1g | 2.1 MB | ########## | 100% scikit-surprise-1.1. | 591 KB | ########## | 100% joblib-0.16.0 | 203 KB | ########## | 100% Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done Collecting package metadata (current_repodata.json): ...working... done Solving environment: ...working... ## Package Plan ## environment location: /opt/conda added / updated specs: - cpuonly - pytorch - torchvision The following packages will be downloaded: package | build ---------------------------|----------------- cpuonly-1.0 | 0 2 KB pytorch freetype-2.10.2 | h5ab3b9f_0 608 KB jpeg-9b | h024ee3a_2 214 KB lcms2-2.11 | h396b838_0 307 KB libpng-1.6.37 | hbc83047_0 278 KB libtiff-4.1.0 | h2733197_0 447 KB ninja-1.10.1 | py37hfd86e86_0 1.4 MB olefile-0.46 | py37_0 50 KB pillow-7.2.0 | py37hb39fc2d_0 617 KB pytorch-1.6.0 | py3.7_cpu_0 59.4 MB pytorch tk-8.6.10 | hbc83047_0 3.0 MB torchvision-0.7.0 | py37_cpu 10.3 MB pytorch zstd-1.3.7 | h0b5b093_0 401 KB ------------------------------------------------------------ Total: 76.9 MB The following NEW packages will be INSTALLED: cpuonly pytorch/noarch::cpuonly-1.0-0 freetype pkgs/main/linux-64::freetype-2.10.2-h5ab3b9f_0 jpeg pkgs/main/linux-64::jpeg-9b-h024ee3a_2 lcms2 pkgs/main/linux-64::lcms2-2.11-h396b838_0 libpng pkgs/main/linux-64::libpng-1.6.37-hbc83047_0 libtiff pkgs/main/linux-64::libtiff-4.1.0-h2733197_0 ninja pkgs/main/linux-64::ninja-1.10.1-py37hfd86e86_0 olefile pkgs/main/linux-64::olefile-0.46-py37_0 pillow pkgs/main/linux-64::pillow-7.2.0-py37hb39fc2d_0 pytorch pytorch/linux-64::pytorch-1.6.0-py3.7_cpu_0 torchvision pytorch/linux-64::torchvision-0.7.0-py37_cpu zstd pkgs/main/linux-64::zstd-1.3.7-h0b5b093_0 The following packages will be UPDATED: ca-certificates conda-forge::ca-certificates-2020.6.2~ --\u0026gt; pkgs/main::ca-certificates-2020.7.22-0 tk 8.6.8-hbc83047_0 --\u0026gt; 8.6.10-hbc83047_0 The following packages will be SUPERSEDED by a higher-priority channel: certifi conda-forge::certifi-2020.6.20-py37hc~ --\u0026gt; pkgs/main::certifi-2020.6.20-py37_0 Proceed ([y]/n)? Downloading and Extracting Packages ninja-1.10.1 | 1.4 MB | ########## | 100% torchvision-0.7.0 | 10.3 MB | ########## | 100% jpeg-9b | 214 KB | ########## | 100% olefile-0.46 | 50 KB | ########## | 100% pillow-7.2.0 | 617 KB | ########## | 100% pytorch-1.6.0 | 59.4 MB | ########## | 100% lcms2-2.11 | 307 KB | ########## | 100% cpuonly-1.0 | 2 KB | ########## | 100% freetype-2.10.2 | 608 KB | ########## | 100% libtiff-4.1.0 | 447 KB | ########## | 100% zstd-1.3.7 | 401 KB | ########## | 100% libpng-1.6.37 | 278 KB | ########## | 100% tk-8.6.10 | 3.0 MB | ########## | 100% Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done Removing intermediate container 0ce30dc6b5a6 ---\u0026gt; 4fafe65a6cf2 Step 4/8 : USER root ---\u0026gt; Running in ad07655303d4 Removing intermediate container ad07655303d4 ---\u0026gt; 66a0f0bbdffd Step 5/8 : WORKDIR /app ---\u0026gt; Running in 15c4304a5210 Removing intermediate container 15c4304a5210 ---\u0026gt; 9e5ade99225b Step 6/8 : ADD . /app ---\u0026gt; 3af47f01d23f Step 7/8 : EXPOSE 80 ---\u0026gt; Running in a9ecfc2e60e9 Removing intermediate container a9ecfc2e60e9 ---\u0026gt; 967dfa99debb Step 8/8 : CMD [\u0026quot;python\u0026quot;, \u0026quot;recommend.py\u0026quot;] ---\u0026gt; Running in 9d93ff09dfef Removing intermediate container 9d93ff09dfef ---\u0026gt; 0821856015d5 Successfully built 0821856015d5 Successfully tagged prediction-service-k8s:latest  Check that the image is present.\n(datasci-dev) ttmac:docker-prediction-service theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE prediction-service-k8s latest 0821856015d5 43 seconds ago 2.06GB weather-service-k8s/latest latest ae4f44a22535 22 hours ago 496MB debian buster-slim 052664ad4351 7 days ago 69.2MB gcr.io/k8s-minikube/storage-provisioner v3 bad58561c4be 2 weeks ago 29.7MB k8s.gcr.io/kube-proxy v1.19.0 bc9c328f379c 3 weeks ago 118MB k8s.gcr.io/kube-controller-manager v1.19.0 09d665d529d0 3 weeks ago 111MB k8s.gcr.io/kube-apiserver v1.19.0 1b74e93ece2f 3 weeks ago 119MB k8s.gcr.io/kube-scheduler v1.19.0 cbdc8369d8b1 3 weeks ago 45.7MB k8s.gcr.io/etcd 3.4.9-1 d4ca8726196c 2 months ago 253MB kubernetesui/dashboard v2.0.3 503bc4b7440b 2 months ago 225MB k8s.gcr.io/coredns 1.7.0 bfe3a36ebd25 3 months ago 45.2MB kubernetesui/metrics-scraper v1.0.4 86262685d9ab 5 months ago 36.9MB continuumio/miniconda3 latest b4adc22212f1 6 months ago 429MB k8s.gcr.io/pause 3.2 80d28bedfe5d 7 months ago 683kB  Next we will create a yaml file using the --dry-run=client command line argument. This will help us avoid trying to search for the image elsewhere.\nkubectl create deployment recommend-minikube --image=prediction-service-k8s:latest -o yaml --dry-run=client  Save the output of that command into a file called recommend-minikube.yaml. Add the additional specification imagePullPolicy: Never to the containers section. Take note of indentation using spaces/tabs as this can lead to errors.\napiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: recommend-minikube name: recommend-minikube spec: replicas: 1 selector: matchLabels: app: recommend-minikube strategy: {} template: metadata: creationTimestamp: null labels: app: recommend-minikube spec: containers: - image: prediction-service-k8s:latest name: prediction-service-k8s resources: {} imagePullPolicy: Never status: {}  Before the deploy the app, we can look at the cluster status using the following:\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h  Run the following command to deploy the app:\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl apply -f recommend-minikube.yaml deployment.apps/recommend-minikube created  Check the state of the cluster:\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl get pod NAME READY STATUS RESTARTS AGE recommend-minikube-5887d99b57-mqjfx 1/1 Running 0 2s (datasci-dev) ttmac:docker-prediction-service theja$ kubectl get all NAME READY STATUS RESTARTS AGE pod/recommend-minikube-5887d99b57-mqjfx 1/1 Running 0 28s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/recommend-minikube 1/1 1 1 28s NAME DESIRED CURRENT READY AGE replicaset.apps/recommend-minikube-5887d99b57 1 1 1 28s  Expose port 80 through a service.\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl expose deployment recommend-minikube --type=NodePort --port=80 service/recommend-minikube exposed  Get the URL to the app.\n(datasci-dev) ttmac:docker-prediction-service theja$ minikube service recommend-minikube --url http://192.168.64.2:32683  Check the URL in the browser. When no uid is passed, we get a json back with a single key.\n   When we do pass a valid uid, we get the recommendations (pytorch inference happened on the server when the request was made) as shown below.   Next we can teardown the cluster using the following command:\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl delete -f recommend-minikube.yaml deployment.apps \u0026quot;recommend-minikube\u0026quot; deleted  If you run the following command quickly, you can see the container getting terminated.\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl get pod NAME READY STATUS RESTARTS AGE recommend-minikube-5887d99b57-mqjfx 1/1 Terminating 0 9m40s (datasci-dev) ttmac:docker-prediction-service theja$ kubectl get pod No resources found in default namespace.  We also need to delete the NodePort service we had created.\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl delete services recommend-minikube service \u0026quot;recommend-minikube\u0026quot; deleted  Next you can stop and even delete the minikube cluster. Below, we are just stopping it.\n(datasci-dev) ttmac:docker-prediction-service theja$ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h (datasci-dev) ttmac:docker-prediction-service theja$ minikube stop ✋ Stopping node \u0026quot;minikube\u0026quot; ... 🛑 1 nodes stopped. (datasci-dev) ttmac:docker-prediction-service theja$ kubectl get all The connection to the server localhost:8080 was refused - did you specify the right host or port?  Congrats, you have deployed a container containing your model on a kubernetes cluster.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/gke/",
	"title": "Orchestration using GKE",
	"tags": [],
	"description": "",
	"content": " Note: While exploring GKE, keep a tab on billing (check every so often)!\nIntroduction to Google Kubernetes Engine by GCP  Google Kubernetes Engine (GKE) by GCP a managed service for running K8s, with key features such as security, scaling and multi-cluster support taken care of as part of K8s on their infrastructure.\n GKE\u0026rsquo;s operation is very similar to ECS.\n Our goal will be to use GKE for deploying our recommendation system (the ML model we have been using).\n We will first save our docker image to a Docker registry on GCP (this is called the Container Registry). Next we will use that image while setting up a K8s cluster.   Google Container Registry  We will use the docker login command with the previously created service account with the JSON based credentials we had saved.\n(base) ttmac:~ theja$ cat model-user.json | docker login -u _json_key --password-stdin https://us.gcr.io Login Succeeded  Tag the docker image with the Google container registry specific tag as follows:\n(base) ttmac:~ theja$ docker tag prediction_service us.gcr.io/authentic-realm-276822/prediction_service (base) ttmac:~ theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE prediction_service latest dd408a931e14 7 days ago 2.06GB us.gcr.io/authentic-realm-276822/prediction_service latest dd408a931e14 7 days ago 2.06GB weather_service latest 20d340f941c0 9 days ago 496MB debian buster-slim c7346dd7f20e 6 weeks ago 69.2MB continuumio/miniconda3 latest b4adc22212f1 6 months ago 429MB hello-world latest bf756fb1ae65 8 months ago 13.3kB  Next, we push the local image to GCR. The upload status will keep getting updated.\n(base) ttmac:~ theja$ docker push us.gcr.io/authentic-realm-276822/prediction_service The push refers to repository [us.gcr.io/authentic-realm-276822/prediction_service] d4bf100b2f89: Pushed 6719394c8842: Pushed a432b6ec80f7: Pushing [\u0026gt; ] 20.81MB/1.635GB fcd8d39597dd: Pushing [========\u0026gt; ] 24.11MB/149.1MB 875120aa853c: Pushing [=====\u0026gt; ] 23.17MB/210.4MB f2cb0ecef392: Layer already exists  When its done, you will see the following:\n(base) ttmac:~ theja$ docker push us.gcr.io/authentic-realm-276822/prediction_service The push refers to repository [us.gcr.io/authentic-realm-276822/prediction_service] d4bf100b2f89: Pushed 6719394c8842: Pushed a432b6ec80f7: Pushed fcd8d39597dd: Pushed 875120aa853c: Pushed f2cb0ecef392: Layer already exists latest: digest: sha256:f5b19d0e4510194ab8bdbed22f915fec8a07d1a465725ccfa6196782a480172c size: 1582  We can verify that the prediction_service image is present in the GCR page.\n  Google Kubernetes Engine  We will now set up the K8s cluster. Lets start by accessing the GKE page.   Click on Deply container next to the Create cluster button.   Pick the existing cluster option and choose select.   Choose the recently uploaded prediction_service image.   Hit continue.   On the next page, we will leave everything to default except for the name and then hit Deploy button.   It may take some time for the cluster to get fully set up.   Recall the system level diagram.\nSource: https://www.gstatic.com/pantheon/images/container/kubernetes_cluster.svg \n Once the cluster is set up, we can investigate its properties.\n   Just as we did in the local deployment, we will expose the cluster to be able to trigger prediction requests. We can do that by going to the Workload tab and clicking on the cluster.   We will next click on the expose button to the far right.   We will specify the container port as 80 (if you look at recommend.py we have specified port 80 where the flask app listens to requests).   Once the service is running, we can obtain the external IP.   As expected, if we query without a payload we get a default response.   With an example payload, we are able to retrieve the recommendations from real-time execution of the pytorch model.   To tear down the cluster, we first delete the service.   Finally, we can delete the workload itself.   As a next exercise, try to create an explicit K8s cluster and deploy the prediction model.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture5/workflow/",
	"title": "Data Science Workflows",
	"tags": [],
	"description": "",
	"content": " Introduction  In data science work streams, batch pipelines involve touching varied data sources (databases, warehouses, data lakes), generating features, imputing, exploration and many other tasks all the way to generating trained model artifacts.\n While doing so, we think of the process from the start to end as blocks that can be chained in a sequence (or more generally as a directed acyclic graph or DAG).\n Some desirable properties we want from model pipelines are:\n ability to manage multiple pipelines ability to run blocks in a schedule (nightly, hourly) ability to detect if a previous block has not finished its part and quickly fix it! (reliability is important)  Ultimately we would like to manage pipelines without much manual work.\n Workflow tools address these gaps. What they enable the user to do is\n allow easy specification of the DAG ensure the dependencies for each block are met schedule blocks to be run automatically  Further these tools also control resources (compute, storage etc) and perform monitoring to achieve these goals.\n Example tools:\n Airflow MLflow Luigi Metaflow Kubeflow Argo   TLDR  These tools are essential for production machine learning lifecyle management with multiple team members.  Our Goals  Build a batch pipeline running in a container Use cron and Kubernetes for scheduling Explore Airflow  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture5/",
	"title": "Lecture 5",
	"tags": [],
	"description": "",
	"content": " TBD "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture5/simple_pipeline/",
	"title": "Training Workflows",
	"tags": [],
	"description": "",
	"content": "  What are some common task blocks?\n Extract data Train a model Predict on a test set Save results in a database  The data must first be prepared (via ETL or extract/transform/load jobs).\n Training and making predictions requires appropriate compute resources.\n Data read and write imply access to an external service (such as a database) or storage (such as AWS S3).\n When you do data science work on a local machine, you will likely use some simple ways to read data (likely from disk or from databases) as well as write your results to disk. But this is not sufficient in a production setting.  And these jobs may need to run periodically so as to get the latest data from logging systems/data lakes.\n Some example of batch pipelines are:\n persistent model pipelines: model update is de-coupled from updating predictions. transient model pipelines: model update is tightly coupled with predictions. This helps with ensuring that the model is not losing prediction accuracy due to changing data distributions (e.g., time varying). \u0026hellip;   Transient Pipeline  We will build a pipeline such that it is built from scratch every-time to generate predictions.\n This will need compute resources (GPUs if its a neural network), which may impact cost benefit analysis.  Our sub-tasks are as follows:\n get the training data train a model use the model for predictions save the results in an external resource. In particular, we will try out BigQuery.  We will be training a recommendation engine for movie recommendation using the lightfm package. In particular, we will follow the example here.\n   Lets clone the repository and start a local jupyter notebook server to have a look at the notebook. For cloning we use the following:\n(datasci-dev) ttmac:pipelines theja$ git clone https://github.com/lyst/lightfm.git    Alternatively, we can load the notebook on Google colab. For this, navigate to colab.research.google.com and logging in using your google account.   You can choose the Github tab and paste the url to the repository.   Colab will open the notebook for you.   The colab notebook above is not stored on your google drive yet. You will need to explicitly choose so (use the file-\u0026gt;save a copy in drive). You can also turn on GPU option (not needed for this notebook) by navigating to runtime-\u0026gt; change runtime type as shown below.   Lets come back to our local jupyter notebook. We can actually execute the notebook by running the following command. See https://github.com/jupyter/nbconvert/issues/515 for more information on the command line arguments used here.\n(datasci-dev) ttmac:pipelines theja$ cd lightfm/examples/quickstart/ (datasci-dev) ttmac:quickstart theja$ jupyter nbconvert --to notebook --ExecutePreprocessor.kernel_name=python3 --inplace --execute quickstart.ipynb [NbConvertApp] Converting notebook quickstart.ipynb to notebook [NbConvertApp] Executing notebook with kernel: python3 [NbConvertApp] Writing 8387 bytes to quickstart.ipynb  We will slightly modify the quickstart notebook to delineate the four steps and add the BigQuery export code.\n Download notebook locally Open with Colab bu using the URL https://github.com/ChicagoDataScience/MLOps/.  We will install one additional package called pandas_gbq from https://pandas-gbq.readthedocs.io/en/latest/ to upload our predictions to Google\u0026rsquo;s BigQuery managed service (can act like an application database).\n(datasci-dev) ttmac:quickstart theja$ conda install pandas-gbq --channel conda-forge Collecting package metadata (current_repodata.json): done Solving environment: done . . (truncated) . . oauthlib-3.0.1 | 82 KB | ############################################################################################################ | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done  (Aside) To do a quick check if you are authenticated, execute the following commands in the terminal (don\u0026rsquo;t forget to set the environment variable using export GOOGLE_APPLICATION_CREDENTIALS=/Users/theja/model-user.json beforehand):\n(datasci-dev) ttmac:pipelines theja$ gcloud auth list Credentialed Accounts ACTIVE ACCOUNT * *****@gmail.com To set the active account, run: $ gcloud config set account `ACCOUNT` (datasci-dev) ttmac:pipelines theja$ gcloud config list project [core] project = authentic-realm-276822 Your active configuration is: [default]  (Aside) You may need to do a downgrade of a package using the command conda install google-cloud-core==1.3.0 in case you are seeing errors such as\nAttributeError: 'ClientOptions' object has no attribute 'scopes'  Once we run all cells of the notebook, we have essentially pushed a pandas dataframe of predictions to Google BigQuery. The dataframe itself looks like the following:\n   we should be able to see these predictions on the Google cloud console. So lets open up the Google Console homepage.   From the console homepage, navigating to BigQuery lands us the following page.   We are not interested in the SQL editor at the moment. At the bottom right, we can see our project.   Expand the project on the left column to get to the movie_recommendation_service database and then to the predicted_movies table. The default information is the schema.   Changing from the scema tab to the preview tab shows that the upload was successful.   Lets rerun the notebook from the commandline. I am assuming that the model-user.json is in the current directory for simplicity. This way, we don\u0026rsquo;t have to set the environment variable GOOGLE_APPLICATION_CREDENTIALS.\n(datasci-dev) ttmac:pipelines theja$ jupyter nbconvert --to notebook --ExecutePreprocessor.kernel_name=python3 --inplace --execute recommend_lightfm.ipynb [NbConvertApp] Converting notebook recommend_lightfm.ipynb to notebook [NbConvertApp] Executing notebook with kernel: python3 [NbConvertApp] Writing 11592 bytes to recommend_lightfm.ipynb  Going back to the BigQuery interface, the only thing that has changed is the timestamp when the predictions were generated (previewed results may not retrieve the same user-ids).\n   Querying from this table can also be done from a notebook:  Download notebook locally    You will notice that the format of the returned recommendations is not easy to parse. So a good exercise challenge is to use regular expressions on the output or modify the way predictions are generated (see Exercises).  Remark  While we did all four blocks in a single script, it makes sense to break it into 4 blocks (fetch data, train, predict, send predictions to database service). This, way a block can retry its execution if the previous block fails and is manually handled by a team member. In particular, this retry can be automated using scheduling tools. One such tool is cron, which is the subject of the next section.\n For simplicity we will containerize the four blocks of this transient model pipeline into a single container and script (i.e., retain the above). Our next tool will allow us to run it automatically in a periodic manner.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture5/cron/",
	"title": "Cron Jobs",
	"tags": [],
	"description": "",
	"content": " Cron expressions will be useful while looking at Apache Airflow scheduling system.\nDocker Image of the Transient Pipeline  First, we will convert our notebook to a script (reduces dependency on Jupyter, try to find other packages you can get away with not installing). Running the py file locally updates the predictions on BigQuery as expected.\n(datasci-dev) ttmac:lec05 theja$ jupyter nbconvert --to script recommend_lightfm.ipynb [NbConvertApp] Converting notebook recommend_lightfm.ipynb to script [NbConvertApp] Writing 4718 bytes to recommend_lightfm.py (datasci-dev) ttmac:lec05 theja$ python recommend_lightfm.py /Users/theja/miniconda3/envs/datasci-dev/lib/python3.7/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used. warnings.warn('LightFM was compiled without OpenMP support. ' \u0026lt;943x1682 sparse matrix of type '\u0026lt;class 'numpy.int32'\u0026gt;' with 19048 stored elements in COOrdinate format\u0026gt; \u0026lt;943x1682 sparse matrix of type '\u0026lt;class 'numpy.int32'\u0026gt;' with 2153 stored elements in COOrdinate format\u0026gt; 1it [00:03, 3.86s/it]  Second, we will create a simple bash script called run_transient_pipeline.sh which has the following content for ease of use:\n Download locally\n#!/bin/bash export GOOGLE_APPLICATION_CREDENTIALS=/model-user.json python recommend_lightfm.py   Next, we will repeat the steps covered in a previous section to quickly build a docker image. For this, we will need a Dockerfile that installs the appropriate dependencies beforehand.\n Download locally\nFROM debian:buster-slim MAINTAINER Theja Tulabandhula RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y python3-pip python3-dev \\ \u0026amp;\u0026amp; cd /usr/local/bin \\ \u0026amp;\u0026amp; ln -s /usr/bin/python3 python RUN pip3 install pandas-gbq lightfm==1.15 numpy RUN pip3 install google-cloud-core==1.3.0 google-cloud-bigquery==1.20.0 COPY recommend_lightfm.py recommend_lightfm.py COPY model-user.json model-user.json COPY run_transient_pipeline.sh run_transient_pipeline.sh RUN chmod +x run_transient_pipeline.sh CMD ./run_transient_pipeline.sh   We can build the image and then run a container based on this image locally.\n(datasci-dev) ttmac:lec05 theja$ docker image build -t \u0026quot;recommend_pipeline\u0026quot; . . . (truncated output) . . (datasci-dev) ttmac:lec05 theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE recommend_pipeline latest 12a25accc725 36 seconds ago 758MB (datasci-dev) ttmac:lec05 theja$ docker run recommend_pipeline \u0026lt;943x1682 sparse matrix of type '\u0026lt;class 'numpy.int32'\u0026gt;' with 19048 stored elements in COOrdinate format\u0026gt; \u0026lt;943x1682 sparse matrix of type '\u0026lt;class 'numpy.int32'\u0026gt;' with 2153 stored elements in COOrdinate format\u0026gt; (datasci-dev) ttmac:lec05 theja$   Verify through the browser based BigQuery page that the predictions are indeed updated.   Cron (1975-)  As described on wikipedia, cron is a command line service/daemon/program for time based scheduling of programs (python scripts in our case). Its present in many UNIX-like OSs including Ubuntu/CentOS etc.\n It is a very useful tool to automate system maintenance (e.g., taking backups) as well as for administration.\n Its name originates from the greek word χρόνος (chronos) meaning time.\n One of the greatest benefits of using cron is that:\n it has been widely used for several decades, and it is easy to use with limited ways to go wrong.  The key limitations of using a cron daemon for scheduling include:\n can run tasks on a single machine OS dependent need glue code (python, bash) to work with it  While cron has its issues with respect to how flexible it is in large scale production environments. But it is a great time-tested utility for individual level automation (e.g., scraping, training models repeatedly as we are planning to do etc).\n crontab is the command line utility to set up the task schedule (as a user or by root). The task (our python script) is run on the system where cron itself is present, so it is good to ensure all dependencies and logging are in place.\n The tasks are run according to the privileges of the user who created the schedule.\n We can edit/create a schedule by using the command crontab -e on the commandline (linux/macOS).\n Task schedules are expressed in terms of corn expressions.\n This is a sequence of 5 numbers that tell the scheduler to run your program periodically. Instead of numbers, characters such as *,-,\\ and , are also used. Lets look at an example expression and understand what it does:\n1 2 3 4 5 /path/to/mycommand argument1 argument2  In the above, the first entry is the minute index (varying between 0-59), 2nd is for hour (0-23), 3rd for day of the month (1-31), 4th for month (1-12) and 5th for day of week (0 or 7 is for Sunday).\n If we need to run a job/task as a system service, then the username also features into the task expression. So the above becomes:\n1 2 3 4 5 username /path/to/mycommand argument1 argument2  Example 1: To run a task everyday at 12 minutes past 2pm, our specification would be:\n12 14 * * * /path/to/mycommand argument1 argument2  Example 2: If you want a task to be run every month at a certain date (say the 5th and 7th days), then the specification would be:\n12 14 5,7 * * /path/to/mycommand argument1 argument2  Example 3: If a task needs to be run every two hours after midnight, then:\n12 0-23/2 * * * /path/to/mycommand argument1 argument2  In the above, * specifies all possible values for that field. and - spcifies a range. Similarly, / specifies the jump and , specifies a list.\n By default cron will try to email you the outcome of running the command. We can just redirect it to the commandline. For example:\n1 2 3 4 5 /path/to/mycommand argument1 argument2 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1  Additional useful commands are:\n crontab -l to list the current jobs, and crontab -r to remove all current job specifications.  System jobs are typically listed in the file /etc/crontab or /etc/cron.d/* folders. For example, if you open a terminal into the recommend_pipeline container and install cron, you can see the following output:\nroot@27cf8d2b2681:/# cat /etc/crontab # /etc/crontab: system-wide crontab # Unlike any other crontab you don't have to run the `crontab' # command to install the new version when you edit this file # and files in /etc/cron.d. These files also have username fields, # that none of the other crontabs do. SHELL=/bin/sh PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin # Example of job definition: # .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * user-name command to be executed 17 *\t* * *\troot cd / \u0026amp;\u0026amp; run-parts --report /etc/cron.hourly 25 6\t* * *\troot\ttest -x /usr/sbin/anacron || ( cd / \u0026amp;\u0026amp; run-parts --report /etc/cron.daily ) 47 6\t* * 7\troot\ttest -x /usr/sbin/anacron || ( cd / \u0026amp;\u0026amp; run-parts --report /etc/cron.weekly ) 52 6\t1 * *\troot\ttest -x /usr/sbin/anacron || ( cd / \u0026amp;\u0026amp; run-parts --report /etc/cron.monthly ) #  Here are a couple of useful websites to interpret the expressions: https://crontab.guru/ and http://cron.schlitt.info/. And wikipedia does a good job laying out the details of expression design.\n To get our hands dirty, we can run a simple cron job that appends a word to a file every minute with the specification:\n* * * * * echo \u0026quot;hello\u0026quot; \u0026gt;\u0026gt; /Users/theja/cds/mlops/crontest.log  And check that the log file is indeed being written to. (Don\u0026rsquo;t forget to delete the job after checking this.)\n We can run our transient pipeline container (although it is not fetching new data, it is easy to see how it can be integrated with something like BigQuery to read updated training data) every hour using the following entry:\n# run every hour on the dot 0 * * * * docker run recommend_pipeline  Note that an application started via cron has no connected terminal, so printing to terminal does not work. So to check that your job is indeed working, you can write something to disk (such as to a log file).\n  Running Cron on the Cloud  Cron is provided as a managed option by AWS, Google Cloud and others. We can look at GKE for an example of this.\n Note that the key difference between previous docker images and the image we created above is that the former was a web server (a flask app) that used a pretrained model/predictions, whereas the latter is only focused on training and generating predictions. Both are complimentary to each other. In fact, the previous container can be triggered to update its predictions/model for serving after the transient training pipeline is completed. And this can be done in a periodic manner.\n Lets come back to scheduling the transient training pipeline as a job on GCP.\n To do so, lets repeat the steps to upload our docker image to the container registry. This involves the following steps.\n First we login via docker using our credentials so that docker can push to the Google container registry.\n(datasci-dev) ttmac:lec05 theja$ cat model-user.json | docker login -u _json_key --password-stdin https://us.gcr.io Login Succeeded  Next we tag our image appropriately to reflect the registry URL and repository name.\n(datasci-dev) ttmac:lec05 theja$ docker tag recommend_pipeline us.gcr.io/authentic-realm-276822/recommend_pipeline (datasci-dev) ttmac:lec05 theja$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE recommend_pipeline latest 12a25accc725 13 hours ago 758MB us.gcr.io/authentic-realm-276822/recommend_pipeline latest 12a25accc725 13 hours ago 758MB  Finally, lets push the image to the container registry. Here is a snapshot when its still uploading.\n(datasci-dev) ttmac:lec05 theja$ docker push us.gcr.io/authentic-realm-276822/recommend_pipeline The push refers to repository [us.gcr.io/authentic-realm-276822/recommend_pipeline] 4e9c256172e5: Pushed 8f8a170d6a46: Pushed d84f1396f776: Pushed ae965efd348c: Pushed 0978afbb1068: Pushed 3c1ec82efa99: Pushing [\u0026gt; ] 3.315MB/269.1MB 21b00e3fe1bb: Pushing [\u0026gt; ] 8.079MB/418.4MB d0f104dc0a1f: Layer already exists  And here is the final result.\n(datasci-dev) ttmac:lec05 theja$ docker push us.gcr.io/authentic-realm-276822/recommend_pipeline The push refers to repository [us.gcr.io/authentic-realm-276822/recommend_pipeline] 4e9c256172e5: Pushed 8f8a170d6a46: Pushed d84f1396f776: Pushed ae965efd348c: Pushed 0978afbb1068: Pushed 3c1ec82efa99: Pushed 21b00e3fe1bb: Pushed d0f104dc0a1f: Layer already exists latest: digest: sha256:9df0d99623e6408d187736c2b82a748ed30eeb773dffb70dfffe03aaa7113173 size: 1995  We can verify that the container is uploaded by checking the container registry page in the browser.\n   We will need the image URL shown below.   More details about this image are available once you click on the image id. We will not be needing this information further, but its a good practice to check it (you can also see how to pull or deploy the image directly from this page).   We will now use K8s (GKE) to schedule the image to run periodically.\n Note that we are not triggering the python script to run periodically, but creating a new container every time.\n This container is downloading the same data in our running example (but it is conceivable how it can be modified to use updated training data).\n Hint: there is no need to modify the image in our example. We only need to have updated data at the URL from which the data is being read.  Lets set up a K8s cluster next. Navigate to the GKE page.\n   Click \u0026lsquo;create a cluster\u0026rsquo;.   We will choose the \u0026lsquo;cluster setup guide\u0026rsquo; option on the right and choose \u0026lsquo;my first cluster\u0026rsquo; (this uses lower end underlying machines). We can see the spec below.   Once the cluster is created, we can view it in the GKE homepage.   We will use kubectl to manage this cluster (just like we managed the minikube single node cluster).\n In particular, we will create a YAML file which will have cron scheduling instructions.\n Lets connect to the cluster on the homepage. We can get information of how to connect bu clicking the connect link.\n   Lets choose the \u0026lsquo;Run in cloud shell\u0026rsquo; option.   If its the first time, it will instantiate a machine to run the shell.   Once instantiated, it will show the command we wanted to run to be able to start using the kubectl utility to deploy our recommend_pipeline image.   You may run into an authorization issue (gcloud utility does not have the credentials on this machine to access any of the cloud services in our account).  theja4gcp@cloudshell:~ (authentic-realm-276822)$ gcloud container clusters get-credentials my-first-cluster-1 --zone us-central1-c - -project authentic-realm-276822 ERROR: (gcloud.container.clusters.get-credentials) You do not currently have an active account selected. Please run: $ gcloud auth login to obtain new credentials. If you have already logged in with a different account: $ gcloud config set account ACCOUNT to select an already authenticated account to use.   The cloud shell may then complain about the following when you run gcloud auth login:\ntheja4gcp@cloudshell:~ (authentic-realm-276822)$ gcloud auth login Go to the following link in your browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com\u0026amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth% 3A2.0%3Aoob\u0026amp;scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-pl atform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww .googleapis.com%2Fauth%2Faccounts.reauth\u0026amp;code_challenge=Vb_NeTDzREqrM34Aiv-64YwhY44vnz9ahbu0idZ0gQM\u0026amp;code_challenge_method=S256\u0026amp;acces s_type=offline\u0026amp;response_type=code\u0026amp;prompt=select_account Enter verification code:  You will use the code generated as below into the cloud shell.\n   Once entered, you will see the following success message:\nYou are now logged in as [theja4gcp@gmail.com]. Your current project is [authentic-realm-276822]. You can change this setting by running: $ gcloud config set project PROJECT_ID  Lets run the original gcloud command that was trying to get the credentials for my-first-cluster again:\ntheja4gcp@cloudshell:~ (authentic-realm-276822)$ gcloud container clusters get-credentials my-first-cluster-1 --zone us-central1-c - -project authentic-realm-276822 Fetching cluster endpoint and auth data. kubeconfig entry generated for my-first-cluster-1.  We can check the kubectl version:\ntheja4gcp@cloudshell:~ (authentic-realm-276822)$ kubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;19\u0026quot;, GitVersion:\u0026quot;v1.19.2\u0026quot;, GitCommit:\u0026quot;f5743093fd1c663cb0cbc89748f730662345d44d\u0026quot;, GitT reeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2020-09-16T13:41:02Z\u0026quot;, GoVersion:\u0026quot;go1.15\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;18+\u0026quot;, GitVersion:\u0026quot;v1.18.6-gke.3504\u0026quot;, GitCommit:\u0026quot;ebdafa7ed3984f94e1ab914221bf04b62a5cd 1b8\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2020-09-09T02:49:12Z\u0026quot;, GoVersion:\u0026quot;go1.13.9b4\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}  Next we will create a yaml file in the current directory (using the vim text editor) in the cloud shell. The file contents are shown below.\napiVersion: batch/v1beta1 kind: CronJob metadata: name: recommend-pipeline-deployment spec: schedule: \u0026quot;* * * * *\u0026quot; jobTemplate: spec: template: spec: containers: - name: recommend-pipeline image: us.gcr.io/authentic-realm-276822/recommend_pipeline restartPolicy: OnFailure  Vim specific keys to use are (i for insert, esc to escape from insert mode, \u0026lsquo;wq\u0026rsquo; to write and quit, and \u0026lsquo;q!\u0026rsquo; to quit without any modifications if needed).\n Once the yaml file is created as shown below, we can deploy the image:\ntheja4gcp@cloudshell:~ (authentic-realm-276822)$ vim recommend_pipeline.yaml   theja4gcp@cloudshell:~ (authentic-realm-276822)$ ls README-cloudshell.txt recommend_pipeline.yaml theja4gcp@cloudshell:~ (authentic-realm-276822)$ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.108.0.1 \u0026lt;none\u0026gt; 443/TCP 56m theja4gcp@cloudshell:~ (authentic-realm-276822)$ kubectl apply -f recommend_pipeline.yaml cronjob.batch/recommend-pipeline-deployment created   We can now go to the workloads page of GKE and view the deployment.   Clicking on it gives more information.   You can see at the bottom that it is a 3 node cluster.   Switching to the events tab shows how the cronjob is being run every minute:   We can also inspect the BigQuery table details page. Here are two screenshots about 1 minute apart from each other:   Great, so we have run a cron job on the cloud. A key benefit of doing this on the cloud is fault tolerance.\n To wrap up things, we can delete the deployment using the following command:\ntheja4gcp@cloudshell:~ (authentic-realm-276822)$ kubectl delete -f recommend_pipeline.yaml cronjob.batch \u0026quot;recommend-pipeline-deployment\u0026quot; deleted theja4gcp@cloudshell:~ (authentic-realm-276822)$ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.108.0.1 \u0026lt;none\u0026gt; 443/TCP 73m  You can see the deployment getting terminated in the browser as well.\n   Finally, we can also terminate the cluster.   We have only scratched the surface of scheduled jobs. Both cron on a self-hosted instance as well as K8s scheduling are very powerful for automation. It is worthwhile delving deeper into these using the resources linked throughout this section.  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture5/airflow/",
	"title": "Apache Airflow",
	"tags": [],
	"description": "",
	"content": "  While cron and cron based scheduling is great, it becomes harder to manage if certain jobs fail and other scheduled jobs depend on their outputs.\n Workflow tools help with resolving these types of dependencies.\n They also allow for version control of objects beyond code.\n These tools have additional capabilities such as alerting team members if a block/task/job failed so that someone can fix and even manually run it.\n It is beneficial for the whole organization if they can use a similar tool:\n data engineers doing ETL jobs, data scientists doing model trainng jobs, analysts doing reporting jobs etc.  We will go through Apache Airflow as an example workflow tool. There are many others, as we mentioned before.\n  Source: https://airflow.apache.org/ \n As listed above, a key benefit with airflow is that it allows us to describe a ML pipeline in code (and in python!).  Airflow Basics  Airflow works with graphs (spcifically, directed acyclic graphs or DAGs) that relate tasks to each other and describe their ordering.\n Each node in the DAG is a task, with incoming arrows from other tasks implying that they are upstream dependencies.\n Lets install the airflow package and get a server running. From the quickstart page\n# airflow needs a home, ~/airflow is the default, # but you can lay foundation somewhere else if you prefer # (optional) export AIRFLOW_HOME=~/airflow # install from pypi using pip pip install apache-airflow # initialize the database airflow initdb # start the web server, default port is 8080 airflow webserver -p 8080 # start the scheduler airflow scheduler # visit localhost:8080 in the browser and enable the example dag in the home page  For instance, when you start the webserver, you should seen an output similar to below:\n(datasci-dev) ttmac:lec05 theja$ airflow webserver -p 8080 ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \\_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \\____/____/|__/ [2020-09-24 12:55:50,012] {__init__.py:50} INFO - Using executor SequentialExecutor [2020-09-24 12:55:50,012] {dagbag.py:417} INFO - Filling up the DagBag from /Users/theja/airflow/dags /Users/theja/miniconda3/envs/datasci-dev/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception. category=PendingDeprecationWarning) Running the Gunicorn Server with: Workers: 4 sync Host: 0.0.0.0:8080 Timeout: 120 Logfiles: - - . . (truncated) . .  Similarly when the scheduler is started, you should see:\n(datasci-dev) ttmac:lec05 theja$ airflow scheduler ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \\_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \\____/____/|__/ [2020-09-24 12:57:27,736] {__init__.py:50} INFO - Using executor SequentialExecutor [2020-09-24 12:57:27,774] {scheduler_job.py:1367} INFO - Starting the scheduler [2020-09-24 12:57:27,775] {scheduler_job.py:1375} INFO - Running execute loop for -1 seconds [2020-09-24 12:57:27,775] {scheduler_job.py:1376} INFO - Processing each file at most -1 times [2020-09-24 12:57:27,775] {scheduler_job.py:1379} INFO - Searching for files in /Users/theja/airflow/dags [2020-09-24 12:57:27,785] {scheduler_job.py:1381} INFO - There are 25 files in /Users/theja/airflow/dags [2020-09-24 12:57:27,785] {scheduler_job.py:1438} INFO - Resetting orphaned tasks for active dag runs [2020-09-24 12:57:27,802] {dag_processing.py:562} INFO - Launched DagFileProcessorManager with pid: 5109 [2020-09-24 12:57:27,812] {settings.py:55} INFO - Configured default timezone \u0026lt;Timezone [UTC]\u0026gt; [2020-09-24 12:57:27,829] {dag_processing.py:776} WARNING - Because we cannot use more than 1 thread (max_threads = 2) when using sqlite. So we set parallelism to 1.  Following this, we can go to localhost:8080 to see the follwoing:\n   When the above sequence of commands was ran, airflow created a config file in ~/airflow folder. This config file has about 1000 lines.\n(datasci-dev) ttmac:~ theja$ cd airflow/ (datasci-dev) ttmac:airflow theja$ less airflow.cfg [core] # The folder where your airflow pipelines live, most likely a # subfolder in a code repository. This path must be absolute. dags_folder = /Users/theja/airflow/dags # The folder where airflow should store its log files # This path must be absolute base_log_folder = /Users/theja/airflow/logs . . (truncated) . . (datasci-dev) ttmac:airflow theja$ wc -l airflow.cfg 1073 airflow.cfg  Airflow manages information about pipelines through a database. By default is it sqlite (we could change this to something else if needed). This is initialized via the initdb argument.\n The scheduler executes out tasks on workers (machines).\n The webserver allows us to interact with the task scheduler and the database.\n  Anatomy of a Workflow Specification  The key idea is that We need to create a python file to define the workflow DAG.\n A key module that we will import is called the BashOperator, which allows us to run arbitrary commands (e.g., docker run image) as long as the dependencies are there (e.g., the docker daemon, the local image registry, and command line utility).\n There are a set of parameters that one should set for any workflow. For instance, who is the owner of this workflow, and if they need to be alerted by email.\n We next create an instance of the DAG class.\n We can define our command line task using the BashOperator. There are various kinds of operators available. We will also make it a node in our DAG.\n If there are additional tasks, we related them to each other.\n  A very quick start using the tutorial workflow  Lets start by going through the tutorial in their documentation. After that we will run our transient pipeline as a workflow through airflow. Below is a gist/anatomy of a workflow specification.\n We can execute the tutorial workflow by using the following command:\n(datasci-dev) ttmac:dags theja$ airflow backfill tutorial -s 2020-09-20 -e 2020-09-22  We can watch the progress in the browser by going to Browse -\u0026gt; Task Instances. You can see the progress snapshots below.\n   Here it shows the successful completion of all tasks.  Workflow Specification for Transient Training Pipeline  We can specify our workflow in the ~/airflow/dags folder as recommend.py, which will get picked up automatically by the scheduler.\n Download locally  If it is not automatically added, try running the following command:\n(datasci-dev) ttmac:dags theja$ airflow list_dags [2020-09-24 15:02:44,915] {__init__.py:50} INFO - Using executor SequentialExecutor [2020-09-24 15:02:44,915] {dagbag.py:417} INFO - Filling up the DagBag from /Users/theja/airflow/dags /Users/theja/miniconda3/envs/datasci-dev/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception. category=PendingDeprecationWarning) ------------------------------------------------------------------- DAGS ------------------------------------------------------------------- example_bash_operator example_branch_dop_operator_v3 example_branch_operator example_complex example_external_task_marker_child example_external_task_marker_parent example_http_operator example_kubernetes_executor_config example_nested_branch_dag example_passing_params_via_test_command example_pig_operator example_python_operator example_short_circuit_operator example_skip_dag example_subdag_operator example_subdag_operator.section-1 example_subdag_operator.section-2 example_trigger_controller_dag example_trigger_target_dag example_xcom latest_only latest_only_with_trigger recommend-pipeline test_utils tutorial  Our python script\u0026rsquo;s contents are reproduced below (to check for syntax issues just run the py file on the commandline):\n# [START import_module] from datetime import timedelta # The DAG object; we'll need this to instantiate a DAG from airflow import DAG # Operators; we need this to operate! from airflow.operators.bash_operator import BashOperator from airflow.utils.dates import days_ago # [END import_module] # [START default_args] # These args will get passed on to each operator # You can override them on a per-task basis during operator initialization default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': days_ago(2), 'email': ['myself@theja.org'], 'email_on_failure': False, 'email_on_retry': False, 'retries': 1, 'retry_delay': timedelta(minutes=5), # 'queue': 'bash_queue', # 'pool': 'backfill', # 'priority_weight': 10, # 'end_date': datetime(2016, 1, 1), # 'wait_for_downstream': False, # 'dag': dag, # 'sla': timedelta(hours=2), # 'execution_timeout': timedelta(seconds=300), # 'on_failure_callback': some_function, # 'on_success_callback': some_other_function, # 'on_retry_callback': another_function, # 'sla_miss_callback': yet_another_function, # 'trigger_rule': 'all_success' } # [END default_args] # [START instantiate_dag] dag = DAG( 'recommend-pipeline', default_args=default_args, description='Run the transient training pipeline', schedule_interval=timedelta(days=1), ) # [END instantiate_dag] t1 = BashOperator( task_id='docker-pipeline-run', bash_command='docker run recommend_pipeline', dag=dag, ) # [START documentation] dag.doc_md = __doc__ t1.doc_md = \u0026quot;\u0026quot;\u0026quot;\\ #### Transient Pipeline Downloads movielens-100k, trains a recommendation model and saves top 10 recommendations to Google BigQuery. \u0026quot;\u0026quot;\u0026quot; # [END documentation] t1 # [END tutorial]  The task can be seen from the browser UI as well:\n   We can run this workflow by triggering it through the UI or by using the backfill argument.\n(datasci-dev) ttmac:dags theja$ airflow backfill recommend-pipeline -s 2020-09-01 -e 2020-09-01  We can verify that the task ran successfully in the browser.\n   We can also check that the timestamp of update in BigQuery reflects the successful completion of the transient training pipeline.  Remarks  If there were other tasks, they can be specified similarly and can be related to each other in the script using .set_upstream() function (there are other ways, we already saw one in the tutorial).\n Instead of the BashOperator, we can also use DockerOperator (we haven\u0026rsquo;t done this here).\n Next, we will see how to use a managed solution (K8s) to run airflow and our pipelines.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture6/",
	"title": "Lecture 6",
	"tags": [],
	"description": "",
	"content": " TBD "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture7/",
	"title": "Lecture 7",
	"tags": [],
	"description": "",
	"content": " TBD "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture8/",
	"title": "Lecture 8",
	"tags": [],
	"description": "",
	"content": " Online Experimentation - A/B testing: sample size considerations - Tackling bandit feedback "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture1/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Deploy model on Heroku.\n Set up your custom domain to point to your VPS.\n Repeat the setup on AWS, GCP, DigitalOcean or any other VPS of your choice.\n Read the documentation for flask, mlflow, pytorch, surprise, pandas.\n Replace Flask with Django and Starlette.\n Read up about function decorators in Python (see here and here for instance). Function decorators add functionality to an existing function, and are an example of metaprogramming.\n Try to set up HTTPS with Lets Encrypt for the flask based model deployed on a single VPS such as EC2.\n Add a WSGI server such as gunicorn. A Web Server Gateway Interface (WSGI) server implements the web server side of the WSGI interface for running Python web applications. See here.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture2/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Find out how serverless technologies work behind the scene.\n Connect your custom domain to the GCP Cloudn Function and the API Gateway/Lambda function in AWS.\n Learn command line tools for GCP and the difference between programmatic access and manual access.\n Learn about identities, roles and access aspects in GCP and AWS.\n Try deploying a different recommendation model.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture3/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Deploy your image to Docker Hub container registry (needs an account, has free tier limits).\n Run a container using the python images from Docker Hub.\n Try to minimize the size of the docker images produced.\n Add checks for out of bound queries in your recommendation function (e.g., http://localhost/?uid=2000 will give a value error on the server and the browser will show that an internal server error occured).\n Add a load balancer to the ECS deployment and study what it does.\n Replicate the linked tutorial: AWS and Docker\n Use cookie cutter data science repository to learn the best practices while training a machine learning moodel in a container.\n Experiment with a different container technology such as https://linuxcontainers.org/.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture4/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Launch a kubernetes cluster with a single pod/container that loads and serves Jupyter notebooks, and which can be accessed via the browser. The images from https://hub.docker.com/u/jupyter/#! such as https://hub.docker.com/r/jupyter/datascience-notebook can help.\n Go through the introductory examples from https://k3s.io and from https://microk8s.io/. Both of these allow you to try Kubernetes locally.\n Try switching to different images such as https://hub.docker.com/_/python/  with minikube.\n Go through the documentation for Kubernetes and Docker.\n Try to access the shell of a deployed container using these instructions.\n Read container logs to debug issues (e.g., python bugs or errors that were not caught) using these instructions.\n Try creating a mini-cloud using VMs and not containers using https://multipass.run/.\n Go through the Redis powered Guestbook app tutorial on GKE.\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/lecture5/exercises/",
	"title": "Exercises",
	"tags": [],
	"description": "",
	"content": " Generalize the data fetching in the recommendation workflow from a external URL that changes the data each day.\n Change the package pandas_gbq to google-cloud-bigquery to accomplish saving the predictions to google cloud. See https://cloud.google.com/bigquery/docs/pandas-gbq-migration for more information.\n Improve the formatting of the recommended movies in Section; Recommendation Workflow.\n Go through the CronJob documentation on https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/ and run the example cronjob on minikube.\n Go through the tutorial on cron by Digitalocean.\n Follow the bigquery + python module from https://codelabs.developers.google.com/codelabs/cloud-bigquery-python/index.html#0\n  "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/",
	"title": "MLOps: Operationalizing Machine Learning",
	"tags": [],
	"description": "",
	"content": " Operationalizing Machine Learning (IDS594) Note: Also known as ML Deployment in the course catalog.\nThis practice-oriented course surveys modern best practices around getting machine learning (ML) models into production. It continues where IDS 572 and IDS 575 left off, which is to learn multiple ways of operationalizing machine learning work flows and models in the context of the larger business end-goals. The course is complementary to IDS 561. We will gain a better understanding of strategies for model management, monitoring and deployment. We will also intertwine these topics with online experimentation techniques (A/B testing) and software engineering ideas such as version control, containerization, and continuous integration/continuous deployment.\nA tentative list of topics is as follows:\n Deploying ML models using web servers (Flask) Containers for machine learning: the Docker ecosystem and Kubernetes Git, CI/CD and their modifications for ML workflows A/B testing of KPIs and data considerations Model management: model tracking and logging  Including case studies such as Databricks\u0026rsquo; MLFlow, Google\u0026rsquo;s TFX/Kubeflow, Uber’s Michelangelo, Facebook\u0026rsquo;s FBLearner Flow.   "
},
{
	"uri": "https://chicagodatascience.github.io/MLOps/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]