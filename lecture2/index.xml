<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lecture 2 on MLOps: Operationalizing Machine Learning</title>
    <link>https://chicagodatascience.github.io/MLOps/lecture2/</link>
    <description>Recent content in Lecture 2 on MLOps: Operationalizing Machine Learning</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    
	<atom:link href="https://chicagodatascience.github.io/MLOps/lecture2/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Serverless Deployments</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture2/serverless/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture2/serverless/</guid>
      <description>A. TLDR  Models do not need to be complex, but it can be complex to deploy models. - Ben Weber (2020)
 Problem  We have to take care of provisioning and server maintenance while deploying our models. We have to worry about scale: would 1 server be enough? How to minimize the time to deploy (at an acceptable increase in cost)? How can a single developer or data science/analytics professional manage a complex service?</description>
    </item>
    
    <item>
      <title>Cloud Functions</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions/</guid>
      <description>Intro  Cloud Functions (CFs) are a solution from GCP for serverless deployments. Very little boilerplate beyond what we will write for simple offline model inference. In any such deployment, we need to be concerned about:  where the model is stored (recall pickle and mlflow), and what python packages are available.   Empty Deployment  We will set up triggers that will trigger our serving function (in particular, a HTTP request).</description>
    </item>
    
    <item>
      <title>GCP Serverless Model Serving</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions_model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions_model/</guid>
      <description>We modify the flask app that we had before, by again specifying the requirements.txt and the main python file appropriately. We will also increase the memory to 2GB and the timeout to 180 seconds. You will see that the following deployment has a lot of inefficiencies (can you spot the redundacy in loading the model and the predictions below?).
The requirements file will have the following entries:
numpy flask pandas google-cloud-storage scikit-surprise pickle5  The main file is also modified accordingly.</description>
    </item>
    
    <item>
      <title>Lambda Functions</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions/</guid>
      <description>Lambda by Amazon Web Services (AWS) is an analogous serverless solution. Lambda can be used internall as well as for model deployments (we are focusing on the latter). We will repeat setting up the weather app and the recommender model, using the CLI (command line interface tools)  Aside: Setting up an IAM user  TBD  Hello World in Lambda  Select the lambda service.   Pick the python 3.</description>
    </item>
    
    <item>
      <title>AWS Serverless Model Serving</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions_model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions_model/</guid>
      <description> Storing the Model on S3 (Simple Storage Service)  S3 is a very popular service used by many large and small companies and individuals. For example, a typical data science work flow may involve ETL work on databases (such as Amazon Redshift) and then storing outputs on S3. To set up S3 for model serving, we have to perform a number of steps. We start with the s3 page.  </description>
    </item>
    
    <item>
      <title>Exercises</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture2/exercises/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture2/exercises/</guid>
      <description> Find out how serverless technologies work behind the scene.
 Connect your custom domain to the GCP Cloudn Function and the API Gateway/Lambda function in AWS.
 Learn command line tools for GCP and the difference between programmatic access and manual access.
 Learn about identities, roles and access aspects in GCP and AWS.
  </description>
    </item>
    
  </channel>
</rss>