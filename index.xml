<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MLOps: Operationalizing Machine Learning</title>
    <link>https://chicagodatascience.github.io/MLOps/</link>
    <description>Recent content on MLOps: Operationalizing Machine Learning</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    
	<atom:link href="https://chicagodatascience.github.io/MLOps/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Online Learning Details</title>
      <link>https://chicagodatascience.github.io/MLOps/logistics/online_technology_requirements/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/logistics/online_technology_requirements/</guid>
      <description>Online Learning Details To maximize the learning experience, it will be good if students can meet the following basic technology requirements:
 At a minimum, students should have a device and an internet connection. A microphone, and a webcam would be highly recommended. See the Basic Technology Requirements link for more details.
 Laptop, Chromebook or Desktop Computer: Note that Chromebooks are used to perform a variety of browser-based tasks with most data and applications, such as Blackboard Learn, Blackboard Collaborate, Google Docs, and Office 365, residing in the cloud rather than on the machine itself.</description>
    </item>
    
    <item>
      <title>Schedule</title>
      <link>https://chicagodatascience.github.io/MLOps/logistics/schedule/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/logistics/schedule/</guid>
      <description>Textbook  Data Science in Production by Ben Weber (2020, $5 for the ebook/pdf). A sample of the first three chapters is available at the publishers page linked here.  Lecture Schedule Lecture 1: Serving ML Models Using Web Servers Reference: Chapter 2  Learning Goals:  Be able to set up a Python environment Be able to set up a jupyter session with SSH tunneling Be able to secure a web server Be able to use Flask to serve a ML model   Lecture 2: Serving ML Models Using Serverless Infrastructure Reference: Chapter 3  Learning Goals:  Be able to differentiate hosted vs managed solutions Assess deops effort for web server vs serverless deployments Be able to deploy a ML model using Google Cloud Functions and AWS Lambda Functions   Lecture 3: Serving ML Models Using Docker Reference: Chapter 4, upto 4.</description>
    </item>
    
    <item>
      <title>Project</title>
      <link>https://chicagodatascience.github.io/MLOps/logistics/project_instructions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/logistics/project_instructions/</guid>
      <description>Aim  The aim of the project is to simulate the real-world process of deploying machine learning models. More specifically, the project component of this course allows you to explore a technology that assists in model deployment, either directly or indirectly, and asks you to report your experience working with that technology (or multiple technologies) to achieve some overall deployment goal.  Group  You should form groups of 4 students for this project component (this is a strict requirement).</description>
    </item>
    
    <item>
      <title>Basics</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/intro/</guid>
      <description>Python  We will be predominantly concerned with the Python ecosystem A big advanage is that local system development can be easily moved to cloud and or a scalable on-prem solution. Many companies use python to start data science projects in-house (via fresh recruits, interns etc) Python has some relatively easy ways to access databases Big data platforms such as Spark have great python bindings  E.g., Pandas dataframe and Spark dataframe  Latest models (deep learning, pre-trained) are built in the python ecosystem Many many useful libraries: pandas, matplotlib, flask,&amp;hellip;  Our Objective  Learn the patterns, not the specific tools  Deployment Targets  Local machines On-prem or self-hosted machines (needs DevOps skills) Managed cloud  Heroku (PAAS) Azure GCP AWS (IAAS)  The decision to deply on one versus the other depends on  skills business need internal vs external scale, reliability, security costs ease of deployment   Local Deployments are Hard  Need to learn linux security Need to learn how to manage access Need for learn backups Need to learn hot switching / reliability  Cloud Deployments are not Easy  Also need to learn a complex ecosystem Vendor lock-in (for successful businesses, this is not an issue)  Aside: Software Tools Python development can happen:</description>
    </item>
    
    <item>
      <title>SSH and Firewall</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/ssh_and_firewall/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/ssh_and_firewall/</guid>
      <description>It is important to secure your connection to the machine. In order to do so, we will configure the ssh access pattern as well as set up a firewall that blocks all incoming requests except ssh port and web server ports.
We will assume that we have a non-root account that is in the sudoers group.
SSH  When you first create the server instance, you may or may not have the ssh server running.</description>
    </item>
    
    <item>
      <title>Setting up Python</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/conda/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/conda/</guid>
      <description>Here are a few notes on installing a user specific python distribution:
Get Miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh conda install pip #better to use the pip in the base conda env than system pip   The difference between conda and pip: pip is a package manager specifically for python, whereas conda is a package manager for multiple languages as well as is an environment manager. Python module venv is python specific environment manager.</description>
    </item>
    
    <item>
      <title>Remote Jupyter Server</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/jupyter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/jupyter/</guid>
      <description>The following sets a simple password based login, which is handy:
jupyter notebook --generate-config jupyter notebook password  Unfortuantely, hashed password is sent unencrypted by your browser here. So read up here to do this in a better way.
Starting jupyter on the server can be done inside a screen session:
screen -S jupyter-session #can also use nohup or tmux here jupyter notebook --no-browser --port=8888  SSH tunnel can be setup by running the following on your local machine, and then opening the browser (http://localhost:8889)</description>
    </item>
    
    <item>
      <title>Recommendation (SVD) Training</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/surprise_training/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/surprise_training/</guid>
      <description># https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.accuracy import rmse from surprise.dump import dump  # Load the movielens-100k dataset (download it if needed). data = Dataset.load_builtin(&#39;ml-100k&#39;) trainset = data.build_full_trainset() # Use an example algorithm: SVD. algo = SVD() algo.fit(trainset) # predict ratings for all pairs (u, i) that are in the training set. testset = trainset.build_testset() predictions = algo.test(testset) rmse(predictions) #actual predictions as thse items have not been seen by the users.</description>
    </item>
    
    <item>
      <title>Serving ML Models Using Web Servers</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/deploy_webserver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/deploy_webserver/</guid>
      <description>Model Serving  Sharing results with others (humans, web services, applications) Batch approach: dump predictions to a database (quite popular) Real-time approach: send a test feature vector, get back the prediction instantly and the computation happens now  How to consume from prediction services?  Using web requests (e.g., using a JSON payload)  How to output predictions?  We will plan to set up a server to serve predictions  It will respond to web requests (GET, POST) We pass some inputs (image, text, vector of numbers), and get some outputs (just like a function) The environment from which we pass inputs may be very different from the environment where the prediction happens (e.</description>
    </item>
    
    <item>
      <title>Recommendation (SVD) Inference</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/surprise_inference/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/surprise_inference/</guid>
      <description># https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.dump import load from collections import defaultdict import pandas as pd def get_top_n(predictions, n=10): &amp;quot;&amp;quot;&amp;quot;Return the top-N recommendation for each user from a set of predictions. Args: predictions(list of Prediction objects): The list of predictions, as returned by the test method of an algorithm. n(int): The number of recommendation to output for each user. Default is 10. Returns: A dict where keys are user (raw) ids and values are lists of tuples: [(raw item id, rating estimation), .</description>
    </item>
    
    <item>
      <title>Flask App</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/flask/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/flask/</guid>
      <description>Flask is a micro web framework written in Python. We first show how a simple service works, and then show how to load a model (e.g., based on pytorch) and serve it as well.
Weather Reporting Service The key thing to see here are that the HTTP route / is mapped directly to a function weather. For instance, when someone hits localhost:5000 (5000 is the default unless specified in app.</description>
    </item>
    
    <item>
      <title>Serverless Deployments</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture2/serverless/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture2/serverless/</guid>
      <description>A. TLDR  Models do not need to be complex, but it can be complex to deploy models. - Ben Weber (2020)
 Problem  We have to take care of provisioning and server maintenance while deploying our models. We have to worry about scale: would 1 server be enough? How to minimize the time to deploy (at an acceptable increase in cost)? How can a single developer or data science/analytics professional manage a complex service?</description>
    </item>
    
    <item>
      <title>Cloud Functions</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions/</guid>
      <description>Intro  Cloud Functions (CFs) are a solution from GCP for serverless deployments. Very little boilerplate beyond what we will write for simple offline model inference. In any such deployment, we need to be concerned about:  where the model is stored (recall pickle and mlflow), and what python packages are available.   Empty Deployment  We will set up triggers that will trigger our serving function (in particular, a HTTP request).</description>
    </item>
    
    <item>
      <title>GCP Serverless Model Serving</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions_model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture2/cloud_functions_model/</guid>
      <description>We modify the flask app that we had before, by again specifying the requirements.txt and the main python file appropriately. We will also increase the memory to 2GB and the timeout to 180 seconds. You will see that the following deployment has a lot of inefficiencies (can you spot the redundacy in loading the model and the predictions below?).
The requirements file will have the following entries:
numpy flask pandas google-cloud-storage scikit-surprise pickle5  The main file is also modified accordingly.</description>
    </item>
    
    <item>
      <title>Lambda Functions</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions/</guid>
      <description>Lambda by Amazon Web Services (AWS) is an analogous serverless solution. Lambda can be used internall as well as for model deployments (we are focusing on the latter). We will repeat setting up the weather app and the recommender model, using the CLI (command line interface tools)  Aside: Setting up an IAM user  TBD  Hello World in Lambda  Select the lambda service.   Pick the python 3.</description>
    </item>
    
    <item>
      <title>AWS Serverless Model Serving</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions_model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture2/lambda_functions_model/</guid>
      <description>Storing the Model on S3  To set up S3 for model serving, we have to perform a number of steps. We start with the s3 page.  - Create a bucket with an informative name.
 We don&amp;rsquo;t have to touch any of these for now.   Here the summary to review.   And we can see the bucket in the list of buckets.  Zip of Local Environment  We need a zip of local environment that includes all dependent libraries.</description>
    </item>
    
    <item>
      <title>Recommendation (Pytorch) Training</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/pytorch_training/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/pytorch_training/</guid>
      <description>Please install the package using the command conda install -c conda-forge scikit-surprise in the ight environment.
# https://github.com/NicolasHug/Surprise from surprise import SVD, Dataset from surprise.accuracy import rmse from surprise.dump import dump import numpy as np import torch from torch import nn import torch.nn.functional as F from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator from ignite.metrics import Loss, MeanSquaredError from datetime import datetime from sklearn.utils import shuffle class Loader(): current = 0 def __init__(self, x, y, batchsize=1024, do_shuffle=True): self.</description>
    </item>
    
    <item>
      <title>Recommendation (Pytorch) Inference</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/pytorch_inference/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/recommendation_model/pytorch_inference/</guid>
      <description>from surprise import Dataset import numpy as np import torch from torch import nn import pandas as pd class MF(nn.Module): itr = 0 def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0): super(MF, self).__init__() self.k = k self.n_user = n_user self.n_item = n_item self.c_bias = c_bias self.c_vector = c_vector self.user = nn.Embedding(n_user, k) self.item = nn.Embedding(n_item, k) # We&#39;ve added new terms here: self.bias_user = nn.Embedding(n_user, 1) self.bias_item = nn.Embedding(n_item, 1) self.bias = nn.</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture3/intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture3/intro/</guid>
      <description>TLDR Problem  Environmental compatibility is a headache, in addition to scaling, security, maintenance and upgrade of software and hardware.  For instance, in the serverless examples, the need for pickle5 instead of pickle was due to such incompatibility.  For hosted environments, we have to work hard on the devops to ensure the environments are the same. For serverless, we did this via the requirements file (Cloud Functions) and locally installing python packages (Lambda functions)  Solution  Containers</description>
    </item>
    
    <item>
      <title>Docker</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture3/docker/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture3/docker/</guid>
      <description>We will first get learn a bit more about docker. From their website:
 Docker is an open platform (written in Go) for developing, shipping, and running applications.
Docker enables you to separate your applications from your infrastructure so you can deliver software quickly.
With Docker, you can manage your infrastructure in the same ways you manage your applications.
By taking advantage of Docker‚Äôs methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.</description>
    </item>
    
    <item>
      <title>Orchestration using ECS and ECR - Part I</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture3/ecr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture3/ecr/</guid>
      <description>Intro  Orchestration means managing container life cycle from building them to deploying (which requires provisioning of appropriate compute resources, storage resources, networking resources), scaling, load-balancing and other tasks, while accounting for failures throughout.
 While there are many orchestration solutions, we will focus on a couple of them: ECS by AWS and Kubernetes (local hosted solution and managed by GCP). While there is Elastic Kubernetes Service (EKS) by AWS as well, we will omit it here, as the ideas are the same.</description>
    </item>
    
    <item>
      <title>Orchestration using ECS and ECR - Part II</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture3/ecs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture3/ecs/</guid>
      <description>Elastic Container Service (ECS)  This is a AWS propreitary solution for container orchestration. There are three key concepts to work with this solution:  Service: Manages containers and relates them to EC2 machines as needed Task: Is a specific container Cluster: Is the environment of EC2 machines where containers live  The below diagram illustrates these relationships.  Source: https://aws.com/ 
 We will set up a cluster and run a task/container and use a service to manage it.</description>
    </item>
    
    <item>
      <title>Kubernetes</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture4/kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture4/kubernetes/</guid>
      <description>Caveat: Unless we have a truly massive or complex system, we probably don‚Äôt need Kubernetes, and using it should be the result of a deliberate cost benefit analysis in comparison to other hosted solutions or managed solutions.
Introduction  Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. It was open-sourced by Google. Its predecessor was called borg internally. Kubernetes, or K8s for short, is a central orchestration system in large complex software systems.</description>
    </item>
    
    <item>
      <title>Model Serving using Kubernetes</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture4/modelkube/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture4/modelkube/</guid>
      <description>This time, instead of the weather app, we will deploy a container containing our recommendation model. Here are the steps.
 Lets start minicube
(datasci-dev) ttmac:docker-prediction-service theja$ minikube start üòÑ minikube v1.13.0 on Darwin 10.14.6 ‚ñ™ MINIKUBE_ACTIVE_DOCKERD=minikube ‚ú® Using the hyperkit driver based on existing profile üëç Starting control plane node minikube in cluster minikube üîÑ Restarting existing hyperkit VM for &amp;quot;minikube&amp;quot; ... üê≥ Preparing Kubernetes v1.19.0 on Docker 19.03.12 .</description>
    </item>
    
    <item>
      <title>Orchestration using GKE</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture4/gke/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture4/gke/</guid>
      <description>Note: While exploring GKE, keep a tab on billing (check every so often)!
Introduction to Google Kubernetes Engine by GCP  Google Kubernetes Engine (GKE) by GCP a managed service for running K8s, with key features such as security, scaling and multi-cluster support taken care of as part of K8s on their infrastructure.
 GKE&amp;rsquo;s operation is very similar to ECS.
 Our goal will be to use GKE for deploying our recommendation system (the ML model we have been using).</description>
    </item>
    
    <item>
      <title>Data Science Workflows</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture5/workflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture5/workflow/</guid>
      <description>Introduction  In data science work streams, batch pipelines involve touching varied data sources (databases, warehouses, data lakes), generating features, imputing, exploration and many other tasks all the way to generating trained model artifacts.
 While doing so, we think of the process from the start to end as blocks that can be chained in a sequence (or more generally as a directed acyclic graph or DAG).
 Some desirable properties we want from model pipelines are:</description>
    </item>
    
    <item>
      <title>Training Workflows</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture5/simple_pipeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture5/simple_pipeline/</guid>
      <description>What are some common task blocks?
 Extract data Train a model Predict on a test set Save results in a database  The data must first be prepared (via ETL or extract/transform/load jobs).
 Training and making predictions requires appropriate compute resources.
 Data read and write imply access to an external service (such as a database) or storage (such as AWS S3).
 When you do data science work on a local machine, you will likely use some simple ways to read data (likely from disk or from databases) as well as write your results to disk.</description>
    </item>
    
    <item>
      <title>Cron Jobs</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture5/cron/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture5/cron/</guid>
      <description>Cron expressions will be useful while looking at Apache Airflow scheduling system.
Docker Image of the Transient Pipeline  First, we will convert our notebook to a script (reduces dependency on Jupyter, try to find other packages you can get away with not installing). Running the py file locally updates the predictions on BigQuery as expected.
(datasci-dev) ttmac:lec05 theja$ jupyter nbconvert --to script recommend_lightfm.ipynb [NbConvertApp] Converting notebook recommend_lightfm.ipynb to script [NbConvertApp] Writing 4718 bytes to recommend_lightfm.</description>
    </item>
    
    <item>
      <title>Apache Airflow</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture5/airflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture5/airflow/</guid>
      <description>While cron and cron based scheduling is great, it becomes harder to manage if certain jobs fail and other scheduled jobs depend on their outputs.
 Workflow tools help with resolving these types of dependencies.
 They also allow for version control of objects beyond code.
 These tools have additional capabilities such as alerting team members if a block/task/job failed so that someone can fix and even manually run it.</description>
    </item>
    
    <item>
      <title>Spark based Pipelines</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture6/spark/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture6/spark/</guid>
      <description>Introduction Spark  Spark lets you run data tasks (preprocessing, feature engineering, training) on multiple machines. A core idea behind spark is the notion of resilient distributed datasets (RDDs). Using this core idea, spark is able to manage fault tolerance and scale. Spark also has a abstract data type called dataframe, similar to pandas and R. This dataframe interface sits on top of RDDs and allows for more approachable specification of our tasks.</description>
    </item>
    
    <item>
      <title>Spark Clusters</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture6/spark_envs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture6/spark_envs/</guid>
      <description>The execution environment is typically distributed across several machines (i.e., a cluster). We submit jobs to clusters for execution. Spark itself is written in Scala/Java. But the Python interface makes it amenable for data science professionals to benefit from it.  Types of Deployments There are three types: - Self hosted cluster deployments - Direct cloud solutions (e.g., Cloud Dataproc by Google Cloud and EMR by AWS) - Vendor based deployments (e.</description>
    </item>
    
    <item>
      <title>Spark on Databricks</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture6/databricks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture6/databricks/</guid>
      <description>Databricks allows organizations to run spark jobs and integrates well with AWS/Azure/GCP.
 We will use the community edition to learn more about pyspark and spark based task and pipeline development.
 It is hosted on AWS    With the Databricks Community Edition, the users will have access to 15GB clusters, a cluster manager and the notebook environment to prototype simple applications, and JDBC / ODBC integrations for BI analysis.</description>
    </item>
    
    <item>
      <title>PySpark</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture6/pyspark/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture6/pyspark/</guid>
      <description>Intro  PySpark is a great pythonic ways of accessing spark dataframes (written in Scala) and manipulating them. It makes it easy to switch back to familiar python tools such as matplotlib and pandas when all the heavy lifting (working with really large data) is done.
 The spark dataframe is the core object of interest in pyspark, and is similar to a pandas dataframe in some aspects. A key difference, as pointed out earlier is that all operations on the dataframe are not executed until it is really needed.</description>
    </item>
    
    <item>
      <title>MLlib - ML Library for Spark</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture6/mllib/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture6/mllib/</guid>
      <description>MLlib is a ML library that works really well with Spark and especially with distributed training (note: not all models can do distributed training).
 It has algorithms for classification, regression, clustering and collaborative filtering.
 Along with the scalable manipulation of data using spark dataframes, it can be a great entry point into scalable machine learning without a dedicated team of ML engineers and data scientists.
 Have a look at the documentation for learning some of these capabilities.</description>
    </item>
    
    <item>
      <title>Exercises</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture1/exercises/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture1/exercises/</guid>
      <description>Deploy model on Heroku.
 Set up your custom domain to point to your VPS.
 Repeat the setup on AWS, GCP, DigitalOcean or any other VPS of your choice.
 Read the documentation for flask, mlflow, pytorch, surprise, pandas.
 Replace Flask with Django and Starlette.
 Read up about function decorators in Python (see here and here for instance). Function decorators add functionality to an existing function, and are an example of metaprogramming.</description>
    </item>
    
    <item>
      <title>Exercises</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture2/exercises/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture2/exercises/</guid>
      <description> Find out how serverless technologies work behind the scene.
 Connect your custom domain to the GCP Cloudn Function and the API Gateway/Lambda function in AWS.
 Learn command line tools for GCP and the difference between programmatic access and manual access.
 Learn about identities, roles and access aspects in GCP and AWS.
 Try deploying a different recommendation model.
  </description>
    </item>
    
    <item>
      <title>Exercises</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture3/exercises/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture3/exercises/</guid>
      <description>Deploy your image to Docker Hub container registry (needs an account, has free tier limits).
 Run a container using the python images from Docker Hub.
 Try to minimize the size of the docker images produced.
 Add checks for out of bound queries in your recommendation function (e.g., http://localhost/?uid=2000 will give a value error on the server and the browser will show that an internal server error occured).</description>
    </item>
    
    <item>
      <title>Exercises</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture4/exercises/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture4/exercises/</guid>
      <description>Launch a kubernetes cluster with a single pod/container that loads and serves Jupyter notebooks, and which can be accessed via the browser. The images from https://hub.docker.com/u/jupyter/#! such as https://hub.docker.com/r/jupyter/datascience-notebook can help.
 Go through the introductory examples from https://k3s.io and from https://microk8s.io/. Both of these allow you to try Kubernetes locally.
 Try switching to different images such as https://hub.docker.com/_/python/  with minikube.
 Go through the documentation for Kubernetes and Docker.</description>
    </item>
    
    <item>
      <title>Exercises</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture5/exercises/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture5/exercises/</guid>
      <description>Generalize the data fetching in the recommendation workflow from a external URL that changes the data each day.
 Change the package pandas_gbq to google-cloud-bigquery to accomplish saving the predictions to google cloud. See https://cloud.google.com/bigquery/docs/pandas-gbq-migration for more information.
 Improve the formatting of the recommended movies in Section; Recommendation Workflow.
 Go through the CronJob documentation on https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/ and run the example cronjob on minikube.
 Go through the tutorial on cron by Digitalocean.</description>
    </item>
    
    <item>
      <title>Exercises</title>
      <link>https://chicagodatascience.github.io/MLOps/lecture6/exercises/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture6/exercises/</guid>
      <description>Go through the examples in https://sparkbyexamples.com/ with notebooks using your Databricks community edition account.
 Go through this article to learn more about the spark architecture.
 Learn the difference between a data lake and a data warehouse here.
 Try to write the non-sql version of the code shown to obtain the top 10 movies by average rating.
 (hard) Try to use the idea of Pandas UDFs to generate a new dataframe of users and a derived attribute that captures whether their average rating over time has increased or decreased.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chicagodatascience.github.io/MLOps/lecture6/pyspark/basics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture6/pyspark/basics/</guid>
      <description>basics - Databricks window.settings = {&#34;enableUsageDeliveryConfiguration&#34;:false,&#34;enablePasswordAclsForTier&#34;:false,&#34;enableNotebookNotifications&#34;:true,&#34;enableDbfsFileBrowser&#34;:false,&#34;enableSshKeyUI&#34;:false,&#34;defaultInteractivePricePerDBU&#34;:0.4,&#34;enableDynamicAutoCompleteResourceLoading&#34;:false,&#34;enableClusterMetricsUI&#34;:true,&#34;enableNewClusterReduxFormUI&#34;:false,&#34;allowWhitelistedIframeDomains&#34;:true,&#34;enableOnDemandClusterType&#34;:true,&#34;enableAutoCompleteAsYouType&#34;:[],&#34;devTierName&#34;:&#34;Community Edition&#34;,&#34;enableJobsPrefetching&#34;:true,&#34;enableNotebookTemplateImport&#34;:true,&#34;shardName&#34;:&#34;devtierprod1&#34;,&#34;enableReservoirTableUI&#34;:true,&#34;enablePartnerIntegrations&#34;:true,&#34;enableClearStateFeature&#34;:true,&#34;dbcForumURL&#34;:&#34;http://forums.databricks.com/&#34;,&#34;enableProtoClusterInfoDeltaPublisher&#34;:true,&#34;enableAttachExistingCluster&#34;:true,&#34;enable-X-Frame-Options&#34;:true,&#34;enableLakehousesHistoryUi&#34;:false,&#34;defaultPythonVersion&#34;:3,&#34;sandboxForSandboxFrame&#34;:&#34;allow-scripts allow-popups allow-popups-to-escape-sandbox allow-forms allow-same-origin allow-downloads&#34;,&#34;requireSpecialCharacterInDefaultPassword&#34;:true,&#34;resetJobListOnConnect&#34;:true,&#34;serverlessDefaultSparkVersion&#34;:&#34;&#34;,&#34;maxCustomTags&#34;:45,&#34;defaultAutomatedLightPricePerDBU&#34;:0.07,&#34;serverlessDefaultMaxWorkers&#34;:8,&#34;dbfsCommandResultServiceWriteResultTimeoutMillis&#34;:15000,&#34;enableInstanceProfilesUIInJobs&#34;:true,&#34;nodeInfo&#34;:{&#34;node_types&#34;:[{&#34;instance_type_id&#34;:&#34;i3.large&#34;,&#34;spark_core_oversubscription_factor&#34;:4.0,&#34;node_type_id&#34;:&#34;dev-tier-node&#34;,&#34;description&#34;:&#34;Community Optimized&#34;,&#34;support_cluster_tags&#34;:false,&#34;container_memory_mb&#34;:10347,&#34;node_instance_type&#34;:{&#34;instance_type_id&#34;:&#34;i3.large&#34;,&#34;provider&#34;:&#34;AWS&#34;,&#34;instance_family&#34;:&#34;EC2 i3 Family vCPUs&#34;,&#34;local_disk_size_gb&#34;:475,&#34;swap_size&#34;:&#34;10g&#34;,&#34;compute_units&#34;:7.0,&#34;number_of_ips&#34;:9,&#34;local_disks&#34;:1,&#34;reserved_compute_units&#34;:3.64,&#34;gpus&#34;:0,&#34;memory_mb&#34;:15616,&#34;num_cores&#34;:2,&#34;max_attachable_disks&#34;:1,&#34;supported_disk_types&#34;:[{&#34;ebs_volume_type&#34;:&#34;GENERAL_PURPOSE_SSD&#34;},{&#34;ebs_volume_type&#34;:&#34;THROUGHPUT_OPTIMIZED_HDD&#34;}],&#34;reserved_memory_mb&#34;:4800,&#34;disk_naming&#34;:&#34;LOCAL_NVME&#34;},&#34;memory_mb&#34;:15616,&#34;is_hidden&#34;:false,&#34;category&#34;:&#34;Community Edition&#34;,&#34;num_cores&#34;:2.0,&#34;is_io_cache_enabled&#34;:false,&#34;spark_mem_info&#34;:{&#34;spark_heap_memory&#34;:8278,&#34;spark_executor_offheap_memory&#34;:0},&#34;support_port_forwarding&#34;:true,&#34;support_ebs_volumes&#34;:false,&#34;is_deprecated&#34;:false}],&#34;default_node_type_id&#34;:&#34;dev-tier-node&#34;},&#34;enableClusterAcls&#34;:true,&#34;notebookRevisionVisibilityHorizon&#34;:999999,&#34;serverlessClusterProductName&#34;:&#34;High Concurrency&#34;,&#34;showS3TableImportOption&#34;:true,&#34;enableContentsApiInProjectsMenu&#34;:false,&#34;redirectBrowserOnWorkspaceSelection&#34;:false,&#34;maxEbsVolumesPerInstance&#34;:10,&#34;enableLakehousesAclsUi&#34;:false,&#34;isDbAdmin&#34;:false,&#34;enableRStudioUI&#34;:true,&#34;isAdmin&#34;:true,&#34;deltaProcessingBatchSize&#34;:1000,&#34;timerUpdateQueueLength&#34;:100,&#34;sqlAclsEnabledMap&#34;:{&#34;spark.databricks.acl.enabled&#34;:&#34;true&#34;,&#34;spark.databricks.acl.sqlOnly&#34;:&#34;true&#34;},&#34;enableLargeResultDownload&#34;:true,&#34;useGraphqlClusterStore&#34;:true,&#34;serverlessDefaultMinWorkers&#34;:2,&#34;enableSparkAdvisor&#34;:true,&#34;zoneInfos&#34;:[{&#34;id&#34;:&#34;us-west-2c&#34;,&#34;isDefault&#34;:true},{&#34;id&#34;:&#34;us-west-2b&#34;,&#34;isDefault&#34;:false},{&#34;id&#34;:&#34;us-west-2a&#34;,&#34;isDefault&#34;:false}],&#34;enableLakehousesUi&#34;:false,&#34;enableCustomSpotPricingUIByTier&#34;:false,&#34;serverlessClustersEnabled&#34;:false,&#34;enableCustomCodeMirrorPreview&#34;:true,&#34;enableWorkspaceBrowserSorting&#34;:true,&#34;enableSentryLogging&#34;:true,&#34;enableFindAndReplace&#34;:true,&#34;enableProjectTypeInWorkspace&#34;:false,&#34;enableWebTerminalUI&#34;:true,&#34;disallowUrlImportExceptFromDocs&#34;:false,&#34;enableGlobalInitScriptsUi&#34;:true,&#34;enableEBSVolumesUIForJobs&#34;:true,&#34;dbcUnifiedSupportPortalURL&#34;:&#34;#sso/support&#34;,&#34;userCanUseSqlService&#34;:true,&#34;enableDeprecatedGlobalInitScripts&#34;:false,&#34;enablePublishNotebooks&#34;:true,&#34;enableUploadDataUis&#34;:true,&#34;browserLogSamplingRateModulo&#34;:10,&#34;createTableInNotebookS3Link&#34;:{&#34;url&#34;:&#34;https://docs.databricks.com/_static/notebooks/data-import/s3.html.template&#34;,&#34;displayName&#34;:&#34;S3&#34;,&#34;workspaceFileName&#34;:&#34;S3 Example&#34;},&#34;sanitizeHtmlResult&#34;:true,&#34;enableNewChartSelectionMenu&#34;:true,&#34;enableClusterPinningUI&#34;:false,&#34;enableJobAclsConfig&#34;:false,&#34;enableFullTextSearch&#34;:false,&#34;enableElasticSparkUI&#34;:false,&#34;enableHeapAnalytics&#34;:true,&#34;aclChecksEnabledForModelRegistryInCurrentWorkspace&#34;:true,&#34;clusters&#34;:true,&#34;allowRunOnPendingClusters&#34;:true,&#34;enableNewClusterLibraryUITab&#34;:true,&#34;useAutoscalingByDefault&#34;:false,&#34;enableWorkspaceExperiments&#34;:true,&#34;enableAzureToolbar&#34;:false,&#34;enableRequireClusterSettingsUI&#34;:true,&#34;fileStoreBase&#34;:&#34;FileStore&#34;,&#34;enableEmailInAzure&#34;:false,&#34;enableJobPauseScheduleFeature&#34;:true,&#34;enableRLibraries&#34;:true,&#34;enableTableAclsConfig&#34;:false,&#34;enableSshKeyUIInJobs&#34;:true,&#34;disableFeedback&#34;:false,&#34;graphqlLatestJobRunPollInterval&#34;:3000,&#34;enableDetachAndAttachSubMenu&#34;:true,&#34;displayFileTypeInWorkspace&#34;:false,&#34;enableReactNotebookComments&#34;:true,&#34;enableAdminPasswordReset&#34;:false,&#34;checkBeforeAddingAadUser&#34;:false,&#34;enableResetPassword&#34;:true,&#34;directoryRefetchMinInterval&#34;:2500,&#34;disableClusterIamRoleOptionality&#34;:false,&#34;maxClusterTagValueLength&#34;:255,&#34;enableJobsSparkUpgrade&#34;:true,&#34;createTableInNotebookDBFSLink&#34;:{&#34;url&#34;:&#34;https://docs.databricks.com/_static/notebooks/data-import/dbfs.html.template&#34;,&#34;displayName&#34;:&#34;DBFS&#34;,&#34;workspaceFileName&#34;:&#34;DBFS Example&#34;},&#34;perClusterAutoterminationEnabled&#34;:false,&#34;enableNotebookCommandNumbers&#34;:true,&#34;measureRoundTripTimes&#34;:true,&#34;maxGraphqlClusterListQueryTries&#34;:6,&#34;singleNodeClustersEnabled&#34;:true,&#34;enableIntercomAdminUI&#34;:false,&#34;maxGraphqlJobListQueryTries&#34;:3,&#34;enableInstancePoolPermissionsUi&#34;:true,&#34;allowStyleInSanitizedHtml&#34;:true,&#34;sparkVersions&#34;:[{&#34;key&#34;:&#34;1.6.3-db2-hadoop2-scala2.10&#34;,&#34;displayName&#34;:&#34;Spark 1.6.3-db2 (Hadoop 2, Scala 2.10)&#34;,&#34;packageLabel&#34;:&#34;spark-image-720f9dfc8f7eced16a107571505379bc791e683beb8402a9883cc718e6e77e84&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:true,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[],&#34;sparkVersion&#34;:null,&#34;scalaVersion&#34;:&#34;2.10&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;5.5.x-hls-scala2.11&#34;,&#34;displayName&#34;:&#34;5.5 Genomics Beta (includes Apache Spark 2.4.3, Scala 2.11)&#34;,&#34;packageLabel&#34;:&#34;snapshot__5.5.x-snapshot-hls-scala2.11__databricks-universe__dbr-branch-5.5__af36814__b77e19f__jenkins__ea6d766__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:true,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_TABLE_ACLS&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;SUPPORTS_ADLS_PASSTHROUGH&#34;,&#34;SUPPORTS_ADLS_GEN2_PASSTHROUGH&#34;,&#34;SUPPORTS_DATA_PLANE_EVENT_TRACKING&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_PYTHON_SELECT_2_3&#34;],&#34;sparkVersion&#34;:&#34;2.4.3&#34;,&#34;scalaVersion&#34;:&#34;2.11&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:&#34;hls&#34;},{&#34;key&#34;:&#34;5.5.x-snapshot-gpu-scala2.11&#34;,&#34;displayName&#34;:&#34;5.5 Snapshot (5.5 snapshot, GPU, Scala 2.11)&#34;,&#34;packageLabel&#34;:&#34;snapshot__5.5.x-snapshot-gpu-scala2.11__databricks-universe__head__194f792__15d4c90__jenkins__b7eccae__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:false,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_PYTHON_SELECT_2_3&#34;],&#34;sparkVersion&#34;:&#34;2.4.3&#34;,&#34;scalaVersion&#34;:&#34;2.11&#34;,&#34;gpuSupport&#34;:true,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;6.2.x-cpu-ml-scala2.11&#34;,&#34;displayName&#34;:&#34;6.2 ML (includes Apache Spark 2.4.4, Scala 2.11)&#34;,&#34;packageLabel&#34;:&#34;release__6.2.x-snapshot-cpu-ml-scala2.11__databricks-universe__head__d72d1ae__dcba513__jenkins__f2d4e07__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:true,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_TABLE_ACLS&#34;,&#34;SUPPORTS_ADLS_PASSTHROUGH&#34;,&#34;SUPPORTS_ADLS_GEN2_PASSTHROUGH&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_CONDA&#34;,&#34;SUPPORTS_IAM_PASSTHROUGH&#34;],&#34;sparkVersion&#34;:&#34;2.4.4&#34;,&#34;scalaVersion&#34;:&#34;2.11&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;7.2.x-hls-scala2.12&#34;,&#34;displayName&#34;:&#34;7.2 Genomics (includes Apache Spark 3.0.0, Scala 2.12)&#34;,&#34;packageLabel&#34;:&#34;release__7.2.x-snapshot-hls-scala2.12__databricks-universe__head__90a279d__faec3ee__jenkins__62b9f08__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:false,&#34;customerVisible&#34;:true,&#34;capabilities&#34;:[&#34;SUPPORTS_CONDA&#34;,&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_TABLE_ACLS&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;SUPPORTS_ADLS_PASSTHROUGH&#34;,&#34;SUPPORTS_ADLS_GEN2_PASSTHROUGH&#34;,&#34;SUPPORTS_IAM_PASSTHROUGH&#34;,&#34;SUPPORTS_DATA_PLANE_EVENT_TRACKING&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_PYTHON3_ONLY&#34;,&#34;SUPPORTS_OPTIMIZED_AUTOSCALING&#34;,&#34;SUPPORTS_WEB_TERMINAL&#34;],&#34;sparkVersion&#34;:&#34;3.0.0&#34;,&#34;scalaVersion&#34;:&#34;2.12&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:&#34;hls&#34;},{&#34;key&#34;:&#34;2.1.1-db6-rc-scala2.10&#34;,&#34;displayName&#34;:&#34;Spark 2.1.1-db6 RC (Scala 2.10)&#34;,&#34;packageLabel&#34;:&#34;spark-image-ce0563427d7def265598896c278cdb788e0ce79e85f34a952aa5b341b310a038&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:true,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;],&#34;sparkVersion&#34;:null,&#34;scalaVersion&#34;:&#34;2.10&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;6.5.x-snapshot-scala2.11&#34;,&#34;displayName&#34;:&#34;6.5 Snapshot (6.5 snapshot, Scala 2.11)&#34;,&#34;packageLabel&#34;:&#34;snapshot__6.5.x-snapshot-scala2.11__databricks-universe__head__c98cb85__5b3dffd__jenkins__cc98344__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:false,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_BYOC&#34;,&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_TABLE_ACLS&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;SUPPORTS_ADLS_PASSTHROUGH&#34;,&#34;SUPPORTS_ADLS_GEN2_PASSTHROUGH&#34;,&#34;SUPPORTS_IAM_PASSTHROUGH&#34;,&#34;SUPPORTS_DATA_PLANE_EVENT_TRACKING&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_PYTHON3_ONLY&#34;,&#34;SUPPORTS_OPTIMIZED_AUTOSCALING&#34;],&#34;sparkVersion&#34;:&#34;2.4.5&#34;,&#34;scalaVersion&#34;:&#34;2.11&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;5.4.x-cpu-ml-scala2.11&#34;,&#34;displayName&#34;:&#34;5.4 ML (includes Apache Spark 2.4.3, Scala 2.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chicagodatascience.github.io/MLOps/lecture6/pyspark/recommendation_system_mllib/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture6/pyspark/recommendation_system_mllib/</guid>
      <description>recommendation_system_MLLib - Databricks window.settings = {&#34;enableUsageDeliveryConfiguration&#34;:false,&#34;enablePasswordAclsForTier&#34;:false,&#34;enableNotebookNotifications&#34;:true,&#34;enableDbfsFileBrowser&#34;:false,&#34;enableSshKeyUI&#34;:false,&#34;defaultInteractivePricePerDBU&#34;:0.4,&#34;enableDynamicAutoCompleteResourceLoading&#34;:false,&#34;enableClusterMetricsUI&#34;:true,&#34;enableNewClusterReduxFormUI&#34;:false,&#34;allowWhitelistedIframeDomains&#34;:true,&#34;enableOnDemandClusterType&#34;:true,&#34;enableAutoCompleteAsYouType&#34;:[],&#34;devTierName&#34;:&#34;Community Edition&#34;,&#34;enableJobsPrefetching&#34;:true,&#34;enableNotebookTemplateImport&#34;:true,&#34;shardName&#34;:&#34;devtierprod1&#34;,&#34;enableReservoirTableUI&#34;:true,&#34;enablePartnerIntegrations&#34;:true,&#34;enableClearStateFeature&#34;:true,&#34;dbcForumURL&#34;:&#34;http://forums.databricks.com/&#34;,&#34;enableProtoClusterInfoDeltaPublisher&#34;:true,&#34;enableAttachExistingCluster&#34;:true,&#34;enable-X-Frame-Options&#34;:true,&#34;enableLakehousesHistoryUi&#34;:false,&#34;defaultPythonVersion&#34;:3,&#34;sandboxForSandboxFrame&#34;:&#34;allow-scripts allow-popups allow-popups-to-escape-sandbox allow-forms allow-same-origin allow-downloads&#34;,&#34;requireSpecialCharacterInDefaultPassword&#34;:true,&#34;resetJobListOnConnect&#34;:true,&#34;serverlessDefaultSparkVersion&#34;:&#34;&#34;,&#34;maxCustomTags&#34;:45,&#34;defaultAutomatedLightPricePerDBU&#34;:0.07,&#34;serverlessDefaultMaxWorkers&#34;:8,&#34;dbfsCommandResultServiceWriteResultTimeoutMillis&#34;:15000,&#34;enableInstanceProfilesUIInJobs&#34;:true,&#34;nodeInfo&#34;:{&#34;node_types&#34;:[{&#34;instance_type_id&#34;:&#34;i3.large&#34;,&#34;spark_core_oversubscription_factor&#34;:4.0,&#34;node_type_id&#34;:&#34;dev-tier-node&#34;,&#34;description&#34;:&#34;Community Optimized&#34;,&#34;support_cluster_tags&#34;:false,&#34;container_memory_mb&#34;:10347,&#34;node_instance_type&#34;:{&#34;instance_type_id&#34;:&#34;i3.large&#34;,&#34;provider&#34;:&#34;AWS&#34;,&#34;instance_family&#34;:&#34;EC2 i3 Family vCPUs&#34;,&#34;local_disk_size_gb&#34;:475,&#34;swap_size&#34;:&#34;10g&#34;,&#34;compute_units&#34;:7.0,&#34;number_of_ips&#34;:9,&#34;local_disks&#34;:1,&#34;reserved_compute_units&#34;:3.64,&#34;gpus&#34;:0,&#34;memory_mb&#34;:15616,&#34;num_cores&#34;:2,&#34;max_attachable_disks&#34;:1,&#34;supported_disk_types&#34;:[{&#34;ebs_volume_type&#34;:&#34;GENERAL_PURPOSE_SSD&#34;},{&#34;ebs_volume_type&#34;:&#34;THROUGHPUT_OPTIMIZED_HDD&#34;}],&#34;reserved_memory_mb&#34;:4800,&#34;disk_naming&#34;:&#34;LOCAL_NVME&#34;},&#34;memory_mb&#34;:15616,&#34;is_hidden&#34;:false,&#34;category&#34;:&#34;Community Edition&#34;,&#34;num_cores&#34;:2.0,&#34;is_io_cache_enabled&#34;:false,&#34;spark_mem_info&#34;:{&#34;spark_heap_memory&#34;:8278,&#34;spark_executor_offheap_memory&#34;:0},&#34;support_port_forwarding&#34;:true,&#34;support_ebs_volumes&#34;:false,&#34;is_deprecated&#34;:false}],&#34;default_node_type_id&#34;:&#34;dev-tier-node&#34;},&#34;enableClusterAcls&#34;:true,&#34;notebookRevisionVisibilityHorizon&#34;:999999,&#34;serverlessClusterProductName&#34;:&#34;High Concurrency&#34;,&#34;showS3TableImportOption&#34;:true,&#34;enableContentsApiInProjectsMenu&#34;:false,&#34;redirectBrowserOnWorkspaceSelection&#34;:false,&#34;maxEbsVolumesPerInstance&#34;:10,&#34;enableLakehousesAclsUi&#34;:false,&#34;isDbAdmin&#34;:false,&#34;enableRStudioUI&#34;:true,&#34;isAdmin&#34;:true,&#34;deltaProcessingBatchSize&#34;:1000,&#34;timerUpdateQueueLength&#34;:100,&#34;sqlAclsEnabledMap&#34;:{&#34;spark.databricks.acl.enabled&#34;:&#34;true&#34;,&#34;spark.databricks.acl.sqlOnly&#34;:&#34;true&#34;},&#34;enableLargeResultDownload&#34;:true,&#34;useGraphqlClusterStore&#34;:true,&#34;serverlessDefaultMinWorkers&#34;:2,&#34;enableSparkAdvisor&#34;:true,&#34;zoneInfos&#34;:[{&#34;id&#34;:&#34;us-west-2c&#34;,&#34;isDefault&#34;:true},{&#34;id&#34;:&#34;us-west-2b&#34;,&#34;isDefault&#34;:false},{&#34;id&#34;:&#34;us-west-2a&#34;,&#34;isDefault&#34;:false}],&#34;enableLakehousesUi&#34;:false,&#34;enableCustomSpotPricingUIByTier&#34;:false,&#34;serverlessClustersEnabled&#34;:false,&#34;enableCustomCodeMirrorPreview&#34;:true,&#34;enableWorkspaceBrowserSorting&#34;:true,&#34;enableSentryLogging&#34;:true,&#34;enableFindAndReplace&#34;:true,&#34;enableProjectTypeInWorkspace&#34;:false,&#34;enableWebTerminalUI&#34;:true,&#34;disallowUrlImportExceptFromDocs&#34;:false,&#34;enableGlobalInitScriptsUi&#34;:true,&#34;enableEBSVolumesUIForJobs&#34;:true,&#34;dbcUnifiedSupportPortalURL&#34;:&#34;#sso/support&#34;,&#34;userCanUseSqlService&#34;:true,&#34;enableDeprecatedGlobalInitScripts&#34;:false,&#34;enablePublishNotebooks&#34;:true,&#34;enableUploadDataUis&#34;:true,&#34;browserLogSamplingRateModulo&#34;:10,&#34;createTableInNotebookS3Link&#34;:{&#34;url&#34;:&#34;https://docs.databricks.com/_static/notebooks/data-import/s3.html.template&#34;,&#34;displayName&#34;:&#34;S3&#34;,&#34;workspaceFileName&#34;:&#34;S3 Example&#34;},&#34;sanitizeHtmlResult&#34;:true,&#34;enableNewChartSelectionMenu&#34;:true,&#34;enableClusterPinningUI&#34;:false,&#34;enableJobAclsConfig&#34;:false,&#34;enableFullTextSearch&#34;:false,&#34;enableElasticSparkUI&#34;:false,&#34;enableHeapAnalytics&#34;:true,&#34;aclChecksEnabledForModelRegistryInCurrentWorkspace&#34;:true,&#34;clusters&#34;:true,&#34;allowRunOnPendingClusters&#34;:true,&#34;enableNewClusterLibraryUITab&#34;:true,&#34;useAutoscalingByDefault&#34;:false,&#34;enableWorkspaceExperiments&#34;:true,&#34;enableAzureToolbar&#34;:false,&#34;enableRequireClusterSettingsUI&#34;:true,&#34;fileStoreBase&#34;:&#34;FileStore&#34;,&#34;enableEmailInAzure&#34;:false,&#34;enableJobPauseScheduleFeature&#34;:true,&#34;enableRLibraries&#34;:true,&#34;enableTableAclsConfig&#34;:false,&#34;enableSshKeyUIInJobs&#34;:true,&#34;disableFeedback&#34;:false,&#34;graphqlLatestJobRunPollInterval&#34;:3000,&#34;enableDetachAndAttachSubMenu&#34;:true,&#34;displayFileTypeInWorkspace&#34;:false,&#34;enableReactNotebookComments&#34;:true,&#34;enableAdminPasswordReset&#34;:false,&#34;checkBeforeAddingAadUser&#34;:false,&#34;enableResetPassword&#34;:true,&#34;directoryRefetchMinInterval&#34;:2500,&#34;disableClusterIamRoleOptionality&#34;:false,&#34;maxClusterTagValueLength&#34;:255,&#34;enableJobsSparkUpgrade&#34;:true,&#34;createTableInNotebookDBFSLink&#34;:{&#34;url&#34;:&#34;https://docs.databricks.com/_static/notebooks/data-import/dbfs.html.template&#34;,&#34;displayName&#34;:&#34;DBFS&#34;,&#34;workspaceFileName&#34;:&#34;DBFS Example&#34;},&#34;perClusterAutoterminationEnabled&#34;:false,&#34;enableNotebookCommandNumbers&#34;:true,&#34;measureRoundTripTimes&#34;:true,&#34;maxGraphqlClusterListQueryTries&#34;:6,&#34;singleNodeClustersEnabled&#34;:true,&#34;enableIntercomAdminUI&#34;:false,&#34;maxGraphqlJobListQueryTries&#34;:3,&#34;enableInstancePoolPermissionsUi&#34;:true,&#34;allowStyleInSanitizedHtml&#34;:true,&#34;sparkVersions&#34;:[{&#34;key&#34;:&#34;1.6.3-db2-hadoop2-scala2.10&#34;,&#34;displayName&#34;:&#34;Spark 1.6.3-db2 (Hadoop 2, Scala 2.10)&#34;,&#34;packageLabel&#34;:&#34;spark-image-720f9dfc8f7eced16a107571505379bc791e683beb8402a9883cc718e6e77e84&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:true,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[],&#34;sparkVersion&#34;:null,&#34;scalaVersion&#34;:&#34;2.10&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;5.5.x-hls-scala2.11&#34;,&#34;displayName&#34;:&#34;5.5 Genomics Beta (includes Apache Spark 2.4.3, Scala 2.11)&#34;,&#34;packageLabel&#34;:&#34;snapshot__5.5.x-snapshot-hls-scala2.11__databricks-universe__dbr-branch-5.5__af36814__b77e19f__jenkins__ea6d766__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:true,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_TABLE_ACLS&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;SUPPORTS_ADLS_PASSTHROUGH&#34;,&#34;SUPPORTS_ADLS_GEN2_PASSTHROUGH&#34;,&#34;SUPPORTS_DATA_PLANE_EVENT_TRACKING&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_PYTHON_SELECT_2_3&#34;],&#34;sparkVersion&#34;:&#34;2.4.3&#34;,&#34;scalaVersion&#34;:&#34;2.11&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:&#34;hls&#34;},{&#34;key&#34;:&#34;5.5.x-snapshot-gpu-scala2.11&#34;,&#34;displayName&#34;:&#34;5.5 Snapshot (5.5 snapshot, GPU, Scala 2.11)&#34;,&#34;packageLabel&#34;:&#34;snapshot__5.5.x-snapshot-gpu-scala2.11__databricks-universe__head__194f792__15d4c90__jenkins__b7eccae__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:false,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_PYTHON_SELECT_2_3&#34;],&#34;sparkVersion&#34;:&#34;2.4.3&#34;,&#34;scalaVersion&#34;:&#34;2.11&#34;,&#34;gpuSupport&#34;:true,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;6.2.x-cpu-ml-scala2.11&#34;,&#34;displayName&#34;:&#34;6.2 ML (includes Apache Spark 2.4.4, Scala 2.11)&#34;,&#34;packageLabel&#34;:&#34;release__6.2.x-snapshot-cpu-ml-scala2.11__databricks-universe__head__d72d1ae__dcba513__jenkins__f2d4e07__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:true,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_TABLE_ACLS&#34;,&#34;SUPPORTS_ADLS_PASSTHROUGH&#34;,&#34;SUPPORTS_ADLS_GEN2_PASSTHROUGH&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_CONDA&#34;,&#34;SUPPORTS_IAM_PASSTHROUGH&#34;],&#34;sparkVersion&#34;:&#34;2.4.4&#34;,&#34;scalaVersion&#34;:&#34;2.11&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;7.2.x-hls-scala2.12&#34;,&#34;displayName&#34;:&#34;7.2 Genomics (includes Apache Spark 3.0.0, Scala 2.12)&#34;,&#34;packageLabel&#34;:&#34;release__7.2.x-snapshot-hls-scala2.12__databricks-universe__head__90a279d__faec3ee__jenkins__62b9f08__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:false,&#34;customerVisible&#34;:true,&#34;capabilities&#34;:[&#34;SUPPORTS_CONDA&#34;,&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_TABLE_ACLS&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;SUPPORTS_ADLS_PASSTHROUGH&#34;,&#34;SUPPORTS_ADLS_GEN2_PASSTHROUGH&#34;,&#34;SUPPORTS_IAM_PASSTHROUGH&#34;,&#34;SUPPORTS_DATA_PLANE_EVENT_TRACKING&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_PYTHON3_ONLY&#34;,&#34;SUPPORTS_OPTIMIZED_AUTOSCALING&#34;,&#34;SUPPORTS_WEB_TERMINAL&#34;],&#34;sparkVersion&#34;:&#34;3.0.0&#34;,&#34;scalaVersion&#34;:&#34;2.12&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:&#34;hls&#34;},{&#34;key&#34;:&#34;2.1.1-db6-rc-scala2.10&#34;,&#34;displayName&#34;:&#34;Spark 2.1.1-db6 RC (Scala 2.10)&#34;,&#34;packageLabel&#34;:&#34;spark-image-ce0563427d7def265598896c278cdb788e0ce79e85f34a952aa5b341b310a038&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:true,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;],&#34;sparkVersion&#34;:null,&#34;scalaVersion&#34;:&#34;2.10&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;6.5.x-snapshot-scala2.11&#34;,&#34;displayName&#34;:&#34;6.5 Snapshot (6.5 snapshot, Scala 2.11)&#34;,&#34;packageLabel&#34;:&#34;snapshot__6.5.x-snapshot-scala2.11__databricks-universe__head__c98cb85__5b3dffd__jenkins__cc98344__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:false,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_BYOC&#34;,&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_TABLE_ACLS&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;SUPPORTS_ADLS_PASSTHROUGH&#34;,&#34;SUPPORTS_ADLS_GEN2_PASSTHROUGH&#34;,&#34;SUPPORTS_IAM_PASSTHROUGH&#34;,&#34;SUPPORTS_DATA_PLANE_EVENT_TRACKING&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_PYTHON3_ONLY&#34;,&#34;SUPPORTS_OPTIMIZED_AUTOSCALING&#34;],&#34;sparkVersion&#34;:&#34;2.4.5&#34;,&#34;scalaVersion&#34;:&#34;2.11&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;5.4.x-cpu-ml-scala2.11&#34;,&#34;displayName&#34;:&#34;5.4 ML (includes Apache Spark 2.4.3, Scala 2.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chicagodatascience.github.io/MLOps/lecture6/pyspark/recommendation_system_surpriselib/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chicagodatascience.github.io/MLOps/lecture6/pyspark/recommendation_system_surpriselib/</guid>
      <description>recommendation_system_surpriselib - Databricks window.settings = {&#34;enableUsageDeliveryConfiguration&#34;:false,&#34;enablePasswordAclsForTier&#34;:false,&#34;enableNotebookNotifications&#34;:true,&#34;enableDbfsFileBrowser&#34;:false,&#34;enableSshKeyUI&#34;:false,&#34;defaultInteractivePricePerDBU&#34;:0.4,&#34;enableDynamicAutoCompleteResourceLoading&#34;:false,&#34;enableClusterMetricsUI&#34;:true,&#34;enableNewClusterReduxFormUI&#34;:false,&#34;allowWhitelistedIframeDomains&#34;:true,&#34;enableOnDemandClusterType&#34;:true,&#34;enableAutoCompleteAsYouType&#34;:[],&#34;devTierName&#34;:&#34;Community Edition&#34;,&#34;enableJobsPrefetching&#34;:true,&#34;enableNotebookTemplateImport&#34;:true,&#34;shardName&#34;:&#34;devtierprod1&#34;,&#34;enableReservoirTableUI&#34;:true,&#34;enablePartnerIntegrations&#34;:true,&#34;enableClearStateFeature&#34;:true,&#34;dbcForumURL&#34;:&#34;http://forums.databricks.com/&#34;,&#34;enableProtoClusterInfoDeltaPublisher&#34;:true,&#34;enableAttachExistingCluster&#34;:true,&#34;enable-X-Frame-Options&#34;:true,&#34;enableLakehousesHistoryUi&#34;:false,&#34;defaultPythonVersion&#34;:3,&#34;sandboxForSandboxFrame&#34;:&#34;allow-scripts allow-popups allow-popups-to-escape-sandbox allow-forms allow-same-origin allow-downloads&#34;,&#34;requireSpecialCharacterInDefaultPassword&#34;:true,&#34;resetJobListOnConnect&#34;:true,&#34;serverlessDefaultSparkVersion&#34;:&#34;&#34;,&#34;maxCustomTags&#34;:45,&#34;defaultAutomatedLightPricePerDBU&#34;:0.07,&#34;serverlessDefaultMaxWorkers&#34;:8,&#34;dbfsCommandResultServiceWriteResultTimeoutMillis&#34;:15000,&#34;enableInstanceProfilesUIInJobs&#34;:true,&#34;nodeInfo&#34;:{&#34;node_types&#34;:[{&#34;instance_type_id&#34;:&#34;i3.large&#34;,&#34;spark_core_oversubscription_factor&#34;:4.0,&#34;node_type_id&#34;:&#34;dev-tier-node&#34;,&#34;description&#34;:&#34;Community Optimized&#34;,&#34;support_cluster_tags&#34;:false,&#34;container_memory_mb&#34;:10347,&#34;node_instance_type&#34;:{&#34;instance_type_id&#34;:&#34;i3.large&#34;,&#34;provider&#34;:&#34;AWS&#34;,&#34;instance_family&#34;:&#34;EC2 i3 Family vCPUs&#34;,&#34;local_disk_size_gb&#34;:475,&#34;swap_size&#34;:&#34;10g&#34;,&#34;compute_units&#34;:7.0,&#34;number_of_ips&#34;:9,&#34;local_disks&#34;:1,&#34;reserved_compute_units&#34;:3.64,&#34;gpus&#34;:0,&#34;memory_mb&#34;:15616,&#34;num_cores&#34;:2,&#34;max_attachable_disks&#34;:1,&#34;supported_disk_types&#34;:[{&#34;ebs_volume_type&#34;:&#34;GENERAL_PURPOSE_SSD&#34;},{&#34;ebs_volume_type&#34;:&#34;THROUGHPUT_OPTIMIZED_HDD&#34;}],&#34;reserved_memory_mb&#34;:4800,&#34;disk_naming&#34;:&#34;LOCAL_NVME&#34;},&#34;memory_mb&#34;:15616,&#34;is_hidden&#34;:false,&#34;category&#34;:&#34;Community Edition&#34;,&#34;num_cores&#34;:2.0,&#34;is_io_cache_enabled&#34;:false,&#34;spark_mem_info&#34;:{&#34;spark_heap_memory&#34;:8278,&#34;spark_executor_offheap_memory&#34;:0},&#34;support_port_forwarding&#34;:true,&#34;support_ebs_volumes&#34;:false,&#34;is_deprecated&#34;:false}],&#34;default_node_type_id&#34;:&#34;dev-tier-node&#34;},&#34;enableClusterAcls&#34;:true,&#34;notebookRevisionVisibilityHorizon&#34;:999999,&#34;serverlessClusterProductName&#34;:&#34;High Concurrency&#34;,&#34;showS3TableImportOption&#34;:true,&#34;enableContentsApiInProjectsMenu&#34;:false,&#34;redirectBrowserOnWorkspaceSelection&#34;:false,&#34;maxEbsVolumesPerInstance&#34;:10,&#34;enableLakehousesAclsUi&#34;:false,&#34;isDbAdmin&#34;:false,&#34;enableRStudioUI&#34;:true,&#34;isAdmin&#34;:true,&#34;deltaProcessingBatchSize&#34;:1000,&#34;timerUpdateQueueLength&#34;:100,&#34;sqlAclsEnabledMap&#34;:{&#34;spark.databricks.acl.enabled&#34;:&#34;true&#34;,&#34;spark.databricks.acl.sqlOnly&#34;:&#34;true&#34;},&#34;enableLargeResultDownload&#34;:true,&#34;useGraphqlClusterStore&#34;:true,&#34;serverlessDefaultMinWorkers&#34;:2,&#34;enableSparkAdvisor&#34;:true,&#34;zoneInfos&#34;:[{&#34;id&#34;:&#34;us-west-2c&#34;,&#34;isDefault&#34;:true},{&#34;id&#34;:&#34;us-west-2b&#34;,&#34;isDefault&#34;:false},{&#34;id&#34;:&#34;us-west-2a&#34;,&#34;isDefault&#34;:false}],&#34;enableLakehousesUi&#34;:false,&#34;enableCustomSpotPricingUIByTier&#34;:false,&#34;serverlessClustersEnabled&#34;:false,&#34;enableCustomCodeMirrorPreview&#34;:true,&#34;enableWorkspaceBrowserSorting&#34;:true,&#34;enableSentryLogging&#34;:true,&#34;enableFindAndReplace&#34;:true,&#34;enableProjectTypeInWorkspace&#34;:false,&#34;enableWebTerminalUI&#34;:true,&#34;disallowUrlImportExceptFromDocs&#34;:false,&#34;enableGlobalInitScriptsUi&#34;:true,&#34;enableEBSVolumesUIForJobs&#34;:true,&#34;dbcUnifiedSupportPortalURL&#34;:&#34;#sso/support&#34;,&#34;userCanUseSqlService&#34;:true,&#34;enableDeprecatedGlobalInitScripts&#34;:false,&#34;enablePublishNotebooks&#34;:true,&#34;enableUploadDataUis&#34;:true,&#34;browserLogSamplingRateModulo&#34;:10,&#34;createTableInNotebookS3Link&#34;:{&#34;url&#34;:&#34;https://docs.databricks.com/_static/notebooks/data-import/s3.html.template&#34;,&#34;displayName&#34;:&#34;S3&#34;,&#34;workspaceFileName&#34;:&#34;S3 Example&#34;},&#34;sanitizeHtmlResult&#34;:true,&#34;enableNewChartSelectionMenu&#34;:true,&#34;enableClusterPinningUI&#34;:false,&#34;enableJobAclsConfig&#34;:false,&#34;enableFullTextSearch&#34;:false,&#34;enableElasticSparkUI&#34;:false,&#34;enableHeapAnalytics&#34;:true,&#34;aclChecksEnabledForModelRegistryInCurrentWorkspace&#34;:true,&#34;clusters&#34;:true,&#34;allowRunOnPendingClusters&#34;:true,&#34;enableNewClusterLibraryUITab&#34;:true,&#34;useAutoscalingByDefault&#34;:false,&#34;enableWorkspaceExperiments&#34;:true,&#34;enableAzureToolbar&#34;:false,&#34;enableRequireClusterSettingsUI&#34;:true,&#34;fileStoreBase&#34;:&#34;FileStore&#34;,&#34;enableEmailInAzure&#34;:false,&#34;enableJobPauseScheduleFeature&#34;:true,&#34;enableRLibraries&#34;:true,&#34;enableTableAclsConfig&#34;:false,&#34;enableSshKeyUIInJobs&#34;:true,&#34;disableFeedback&#34;:false,&#34;graphqlLatestJobRunPollInterval&#34;:3000,&#34;enableDetachAndAttachSubMenu&#34;:true,&#34;displayFileTypeInWorkspace&#34;:false,&#34;enableReactNotebookComments&#34;:true,&#34;enableAdminPasswordReset&#34;:false,&#34;checkBeforeAddingAadUser&#34;:false,&#34;enableResetPassword&#34;:true,&#34;directoryRefetchMinInterval&#34;:2500,&#34;disableClusterIamRoleOptionality&#34;:false,&#34;maxClusterTagValueLength&#34;:255,&#34;enableJobsSparkUpgrade&#34;:true,&#34;createTableInNotebookDBFSLink&#34;:{&#34;url&#34;:&#34;https://docs.databricks.com/_static/notebooks/data-import/dbfs.html.template&#34;,&#34;displayName&#34;:&#34;DBFS&#34;,&#34;workspaceFileName&#34;:&#34;DBFS Example&#34;},&#34;perClusterAutoterminationEnabled&#34;:false,&#34;enableNotebookCommandNumbers&#34;:true,&#34;measureRoundTripTimes&#34;:true,&#34;maxGraphqlClusterListQueryTries&#34;:6,&#34;singleNodeClustersEnabled&#34;:true,&#34;enableIntercomAdminUI&#34;:false,&#34;maxGraphqlJobListQueryTries&#34;:3,&#34;enableInstancePoolPermissionsUi&#34;:true,&#34;allowStyleInSanitizedHtml&#34;:true,&#34;sparkVersions&#34;:[{&#34;key&#34;:&#34;1.6.3-db2-hadoop2-scala2.10&#34;,&#34;displayName&#34;:&#34;Spark 1.6.3-db2 (Hadoop 2, Scala 2.10)&#34;,&#34;packageLabel&#34;:&#34;spark-image-720f9dfc8f7eced16a107571505379bc791e683beb8402a9883cc718e6e77e84&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:true,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[],&#34;sparkVersion&#34;:null,&#34;scalaVersion&#34;:&#34;2.10&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;5.5.x-hls-scala2.11&#34;,&#34;displayName&#34;:&#34;5.5 Genomics Beta (includes Apache Spark 2.4.3, Scala 2.11)&#34;,&#34;packageLabel&#34;:&#34;snapshot__5.5.x-snapshot-hls-scala2.11__databricks-universe__dbr-branch-5.5__af36814__b77e19f__jenkins__ea6d766__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:true,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_TABLE_ACLS&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;SUPPORTS_ADLS_PASSTHROUGH&#34;,&#34;SUPPORTS_ADLS_GEN2_PASSTHROUGH&#34;,&#34;SUPPORTS_DATA_PLANE_EVENT_TRACKING&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_PYTHON_SELECT_2_3&#34;],&#34;sparkVersion&#34;:&#34;2.4.3&#34;,&#34;scalaVersion&#34;:&#34;2.11&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:&#34;hls&#34;},{&#34;key&#34;:&#34;5.5.x-snapshot-gpu-scala2.11&#34;,&#34;displayName&#34;:&#34;5.5 Snapshot (5.5 snapshot, GPU, Scala 2.11)&#34;,&#34;packageLabel&#34;:&#34;snapshot__5.5.x-snapshot-gpu-scala2.11__databricks-universe__head__194f792__15d4c90__jenkins__b7eccae__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:false,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_PYTHON_SELECT_2_3&#34;],&#34;sparkVersion&#34;:&#34;2.4.3&#34;,&#34;scalaVersion&#34;:&#34;2.11&#34;,&#34;gpuSupport&#34;:true,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;6.2.x-cpu-ml-scala2.11&#34;,&#34;displayName&#34;:&#34;6.2 ML (includes Apache Spark 2.4.4, Scala 2.11)&#34;,&#34;packageLabel&#34;:&#34;release__6.2.x-snapshot-cpu-ml-scala2.11__databricks-universe__head__d72d1ae__dcba513__jenkins__f2d4e07__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:true,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_TABLE_ACLS&#34;,&#34;SUPPORTS_ADLS_PASSTHROUGH&#34;,&#34;SUPPORTS_ADLS_GEN2_PASSTHROUGH&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_CONDA&#34;,&#34;SUPPORTS_IAM_PASSTHROUGH&#34;],&#34;sparkVersion&#34;:&#34;2.4.4&#34;,&#34;scalaVersion&#34;:&#34;2.11&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;7.2.x-hls-scala2.12&#34;,&#34;displayName&#34;:&#34;7.2 Genomics (includes Apache Spark 3.0.0, Scala 2.12)&#34;,&#34;packageLabel&#34;:&#34;release__7.2.x-snapshot-hls-scala2.12__databricks-universe__head__90a279d__faec3ee__jenkins__62b9f08__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:false,&#34;customerVisible&#34;:true,&#34;capabilities&#34;:[&#34;SUPPORTS_CONDA&#34;,&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_TABLE_ACLS&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;SUPPORTS_ADLS_PASSTHROUGH&#34;,&#34;SUPPORTS_ADLS_GEN2_PASSTHROUGH&#34;,&#34;SUPPORTS_IAM_PASSTHROUGH&#34;,&#34;SUPPORTS_DATA_PLANE_EVENT_TRACKING&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_PYTHON3_ONLY&#34;,&#34;SUPPORTS_OPTIMIZED_AUTOSCALING&#34;,&#34;SUPPORTS_WEB_TERMINAL&#34;],&#34;sparkVersion&#34;:&#34;3.0.0&#34;,&#34;scalaVersion&#34;:&#34;2.12&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:&#34;hls&#34;},{&#34;key&#34;:&#34;2.1.1-db6-rc-scala2.10&#34;,&#34;displayName&#34;:&#34;Spark 2.1.1-db6 RC (Scala 2.10)&#34;,&#34;packageLabel&#34;:&#34;spark-image-ce0563427d7def265598896c278cdb788e0ce79e85f34a952aa5b341b310a038&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:true,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;],&#34;sparkVersion&#34;:null,&#34;scalaVersion&#34;:&#34;2.10&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;6.5.x-snapshot-scala2.11&#34;,&#34;displayName&#34;:&#34;6.5 Snapshot (6.5 snapshot, Scala 2.11)&#34;,&#34;packageLabel&#34;:&#34;snapshot__6.5.x-snapshot-scala2.11__databricks-universe__head__c98cb85__5b3dffd__jenkins__cc98344__format-2&#34;,&#34;upgradable&#34;:true,&#34;deprecated&#34;:false,&#34;customerVisible&#34;:false,&#34;capabilities&#34;:[&#34;SUPPORTS_BYOC&#34;,&#34;SUPPORTS_END_TO_END_ENCRYPTION&#34;,&#34;SUPPORTS_TABLE_ACLS&#34;,&#34;SUPPORTS_RSTUDIO&#34;,&#34;SUPPORTS_DATABRICKS_DELTA&#34;,&#34;SUPPORTS_ADLS_PASSTHROUGH&#34;,&#34;SUPPORTS_ADLS_GEN2_PASSTHROUGH&#34;,&#34;SUPPORTS_IAM_PASSTHROUGH&#34;,&#34;SUPPORTS_DATA_PLANE_EVENT_TRACKING&#34;,&#34;STANDARD_SKU&#34;,&#34;SUPPORTS_PYTHON3_ONLY&#34;,&#34;SUPPORTS_OPTIMIZED_AUTOSCALING&#34;],&#34;sparkVersion&#34;:&#34;2.4.5&#34;,&#34;scalaVersion&#34;:&#34;2.11&#34;,&#34;gpuSupport&#34;:false,&#34;requiredVisibilityTag&#34;:null},{&#34;key&#34;:&#34;5.4.x-cpu-ml-scala2.11&#34;,&#34;displayName&#34;:&#34;5.4 ML (includes Apache Spark 2.4.3, Scala 2.</description>
    </item>
    
  </channel>
</rss>